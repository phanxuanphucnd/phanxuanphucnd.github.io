<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>mBART</title>
  <meta name="title" content="mBART">
  <meta name="description" content="mBART stands for “Multilingual Bidirectional Auto-regressive
Transformer” which is a multilingual NMT model proposed by FacebookAI in
2020 and published in their paper: “Multilingual Denoising Pre-training
for Neural Machine Translation”.
The official code for this paper can be found in the fairseq GitHub
repository:
mbart.
mBART is the first method for pre-training a complete
sequence-to-sequence model by denoising full texts in multiple
monolingual data, while previous approaches have focused only on the
encoder/decoder.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="mBART">
  <meta itemprop="description" content="mBART stands for “Multilingual Bidirectional Auto-regressive
Transformer” which is a multilingual NMT model proposed by FacebookAI in
2020 and published in their paper: “Multilingual Denoising Pre-training
for Neural Machine Translation”.
The official code for this paper can be found in the fairseq GitHub
repository:
mbart.
mBART is the first method for pre-training a complete
sequence-to-sequence model by denoising full texts in multiple
monolingual data, while previous approaches have focused only on the
encoder/decoder.

">
  <meta itemprop="image" content="/multilingual-nmt/media/mBART/image0.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="mBART">
  <meta property="og:description" content="mBART stands for “Multilingual Bidirectional Auto-regressive
Transformer” which is a multilingual NMT model proposed by FacebookAI in
2020 and published in their paper: “Multilingual Denoising Pre-training
for Neural Machine Translation”.
The official code for this paper can be found in the fairseq GitHub
repository:
mbart.
mBART is the first method for pre-training a complete
sequence-to-sequence model by denoising full texts in multiple
monolingual data, while previous approaches have focused only on the
encoder/decoder.

">
  <meta property="og:image" content="/multilingual-nmt/media/mBART/image0.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="mBART">
  <meta name="twitter:description" content="mBART stands for “Multilingual Bidirectional Auto-regressive
Transformer” which is a multilingual NMT model proposed by FacebookAI in
2020 and published in their paper: “Multilingual Denoising Pre-training
for Neural Machine Translation”.
The official code for this paper can be found in the fairseq GitHub
repository:
mbart.
mBART is the first method for pre-training a complete
sequence-to-sequence model by denoising full texts in multiple
monolingual data, while previous approaches have focused only on the
encoder/decoder.

">
  
  <meta name="twitter:image" content="/multilingual-nmt/media/mBART/image0.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/multilingual-nmt/mBART">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          4 mins read
        </span>
      </p>
      <time datetime="2020-01-22 00:00" class="post-meta__body date">Published on arXiv on: 22 Jan 2020</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#FAIR">FAIR</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=mBART> mBART</h1>
    <p>mBART stands for “Multilingual Bidirectional Auto-regressive
Transformer” which is a multilingual NMT model proposed by FacebookAI in
2020 and published in their paper: “<a href="https://arxiv.org/pdf/2001.08210.pdf">Multilingual Denoising Pre-training
for Neural Machine Translation</a>”.
The official code for this paper can be found in the fairseq GitHub
repository:
<a href="https://github.com/pytorch/fairseq/tree/master/examples/mbart">mbart</a>.
mBART is the first method for pre-training a complete
sequence-to-sequence model by denoising full texts in multiple
monolingual data, while previous approaches have focused only on the
encoder/decoder.</p>

<p>The whole idea behind mBART is to apply the BART architecture to
large-scale monolingual corpora across many languages where the input
texts are noised by masking phrases and permuting sentences. This will
create a universal language model that is able to denoise this input
text which makes translation from one language to another achievable.</p>

<div align="center">
    <img src="media/mBART/image1.png" width="750" />
</div>

<h2 id="pre-training">Pre-training</h2>

<p>The data used for pre-training mBART is monolingual data of 25 different
languages collected by <a href="https://commoncrawl.org/">common crawl</a>. The table below
shows the size of the collected data.</p>

<div align="center">
    <img src="media/mBART/image2.png" width="350" />
</div>

<p>The model used in this paper is a standard sequence-to-sequence
Transformer architecture from <a href="https://github.com/pytorch/fairseq/">fairseq
repository</a> with:</p>

<ul>
  <li>
    <p>12 layers of encoder and 12 layers of decoder. The model’s dimension
is 1024 on 16 heads (∼ 680M parameters).</p>
  </li>
  <li>
    <p>They included an additional layer-normalization layer on top of both
the encoder and decoder, which they found stabilized the training.</p>
  </li>
  <li>
    <p>The model was trained for 500 steps using Adam optimizer with
0.000001 learning rate and 0.98 beta2 and linear learning rate
decay. The training took around 2.4 weeks despite using 256 Nvidia
V100 GPUs.</p>
  </li>
  <li>
    <p>They started the training with dropout 0.1 and reduced it to 0.05 at
250K steps and 0 at 400K steps.</p>

    <p>Regarding the encoder:</p>
  </li>
  <li>
    <p>For each instance, they packed as many consecutive sentences as
possible sampled from the corresponding corpus of a certain
language &lt;LID&gt;, until either it hits the document boundary or
reaches the 512 max token length.</p>
  </li>
  <li>
    <p>Sentences in the instance are separated by the end of sentence
(&lt;/S&gt;) token.</p>
  </li>
  <li>
    <p>The language ID token is appended at the end of the instance.</p>
  </li>
</ul>

<div align="center">
    <img src="media/mBART/image3.png" width="750" />
</div>

<p>Regarding the decoder:</p>

<ul>
  <li>The decoder input is the original text with one position offset
which is the language id symbol &lt;LID&gt; as it’s used as the
initial token to predict the sentence.</li>
</ul>

<div align="center">
    <img src="media/mBART/image4.png" width="750" />
</div>

<p>Following the BART paper, they uses two types of noise:</p>

<ul>
  <li>
    <p><strong>Text Infilling</strong>: They masked around 35% of the words in each
instance by random sampling a span length according to a Poisson
distribution (λ = 3.5).</p>
  </li>
  <li>
    <p><strong>Sentence Permutation</strong>: They also permuted the order of sentences
within each instance.</p>
  </li>
</ul>

<h2 id="fine-tuning">Fine-tuning</h2>

<p>We fine-tune our multilingual pre-trained models on a single pair of
parallel data, feeding the source language into the encoder and decoding
the target language. Note that they used monolingual data for
pre-training and parallel corpus for fine-tuning. In the paper, they
used different pre-trained models to compare between while fine-tuning:</p>

<p>a.  <strong>BART-En/Ro</strong>: Baseline model for just one pair (English -
    &gt; Romanian).</p>

<p>b.  <strong>mBART25</strong>: Uses all 25 languages.</p>

<p>c.  <strong>mBART06</strong>: Uses just 6 European languages [Ro (Romanian), It
    (Italian), Cs (Czech), Fr (French), Es (Spanish) and En(English)].</p>

<p>d.  <strong>mBART02</strong>: They used 4 different versions of this model; one for
    -&gt; each pair of the following pairs: ( English-German,
    -&gt; English-Romanian, English-Italian).</p>

<p>e.  <strong>Random</strong>: don’t know exactly :D.</p>

<p>For all these models, they trained using 0.3 dropout, 0.2 label
smoothing, 2500 warm-up steps, 3e−5 maximum learning rate with a
maximum of 40K training updates for all low and medium resource
pairs and 100K for high resource pairs. The final models are
selected based on validation likelihood. For decoding, we use
beam-search with beam size 5 for all directions.</p>

<p>All models use the same vocabulary. Not all tokens will frequently
occur in all pre-training corpora, but later experiments show that
this large vocabulary can improve generalization in multilingual
settings even for unseen languages.</p>

<p>The following table shows a comparison between mBART25 and Random
baseline on low/medium-resource parallel corpora. Using mBART25
weights shows gains on all the low and medium resource pairs. While
fine-tuning fails in extremely low-resource setting such as En-Gu,
which only have roughly 10k instances:</p>

<div align="center">
    <img src="media/mBART/image5.png" width="750" />
</div>

<p>The following table shows a comparison between mBART25 and Random
baseline on high-resource parallel corpora in the direction of En
--&gt; X. As you can see, there weren’t consistent gains, and
pre-training slightly hurts performance when &gt;25M parallel sentence
are available as if they wash out the pre-trained weights
completely:</p>

<div align="center">
    <img src="media/mBART/image6.png" width="450" />
</div>

<blockquote>
  <p><strong>Note:</strong><br />
In the paper, they tried using back-translation with low-resource
language and mBART25 and it did improve.</p>
</blockquote>

<h2 id="analysis">Analysis</h2>

<p>In my opinion, this is one of the best parts about this paper where they
try to answer the most common questions regarding their paper:</p>

<ul>
  <li>
    <p><u><strong>How many languages should you pre-train on?</strong></u><br />
When monolingual data is plentiful, pre-training
pre-training on multiple languages slightly hurts the final
results (&lt;1 BLEU). On the other hand, when monolingual data is
limited, pre-training on more languages helps.</p>
  </li>
  <li>
    <p><u><strong>Is pre-training essential for using mBART?</strong></u><br />
Without any pre-training, mBART tends to overfit and perform much
worse than the baseline.</p>
  </li>
  <li>
    <p><u><strong>How many pre-training steps are needed?</strong></u><br />
After just 25K steps, pre-trained models outperform the
best baseline. The models keep improving by over 3 BLEU for the
rest of steps and have not fully converged after 500K steps.</p>
  </li>
  <li>
    <p><u><strong>Is fine-tuning helps with unseen pre-trained languages?</strong></u><br />
Surprisingly, yes! They performed an experiment where they used
mBART02 and mBART06 to translate En-Ar (English-Arabic), En-De
(English-German), and En-DI (English-Dutch) where they weren’t in
the pre-training data. And these two models showed competitive
results compared with mBART25. This result suggests that the
pre-trained Transformer layers learn universal properties of
language that generalize well even with minimal lexical overlap.</p>
  </li>
  <li>
    <p><u><strong>How does mBART behave with unseen Source or Target languages in
the pre-training?</strong></u><br />
If both sides are unseen, the performance (in terms of difference
from mBART25) is worse than where at least one language is seen
during pre-training. Fine-tuning unseen languages on source side
is more difficult than the target side. Although mBART06
outperforms mBART02 by a margin on when the source side is
missing.</p>
  </li>
</ul>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/multilingual-nmt/mBART';
      this.page.identifier = '/multilingual-nmt/mBART';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>