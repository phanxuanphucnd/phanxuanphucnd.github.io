<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>Multilingual NMT</title>
  <meta name="title" content="Multilingual NMT">
  <meta name="description" content="I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics: Cross-lingual Langluage Model,  Language Modeling,  Machine Translation,  Misc.,  Multilingual NMT, and  Word Embedding">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="Multilingual NMT">
  <meta itemprop="description" content="I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics: Cross-lingual Langluage Model,  Language Modeling,  Machine Translation,  Misc.,  Multilingual NMT, and  Word Embedding">
  <meta itemprop="image" content="//images/avatar.jpg">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="Multilingual NMT">
  <meta property="og:description" content="I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics: Cross-lingual Langluage Model,  Language Modeling,  Machine Translation,  Misc.,  Multilingual NMT, and  Word Embedding">
  <meta property="og:image" content="//images/avatar.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Multilingual NMT">
  <meta name="twitter:description" content="I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics: Cross-lingual Langluage Model,  Language Modeling,  Machine Translation,  Misc.,  Multilingual NMT, and  Word Embedding">
  
  <meta name="twitter:image" content="//images/avatar.jpg">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/multilingual-nmt/">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <div class="main">
  

  
  <div class="main-post-list">
    
    
    
    
    
    
    <h1 class="page-heading">2022</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/CeMAT/image3.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/multilingual-nmt/CeMAT">CeMAT</a>
            </h2>
            <p class="excerpt">CeMAT stands for “ Conditional masked language pretraining model
for Machine Translation” which is a bidirectional encoder and a
bidirectional decoder multilingual
Transformer
model with a cross-attention module for bridging them. CeMAT was
proposed by Huawei Noah’s Ark Lab in 2022 and published in their paper:
Universal Conditional Masked Language Pre-training for Neural Machine
Translation. The official code
for this paper can be found in Huawei Noah’s Ark Lab official GitHub
repository:
huawei-noah/CeMAT.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2022-03-17 00:00" class="post-list__meta--date date"> Published on arXiv on :
                17 Mar 2022</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
    </ol>
    <h1 class="page-heading">2021</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/multilinguality_in_transformers/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/multilingual-nmt/multilinguality_in_transformers">Multilinguality in Transformers</a>
            </h2>
            <p class="excerpt">The following paper: Do Multilingual Neural Machine Translation Models
Contain Language Pair Specific Attention
Heads? asks a very good question.
To answer it, the publishers tried to measure the importance of the
self-attention heads in the encoder and the encoder-decoder attention
heads of a many-to-one transformer. The NMT model was able to translate
French, German, Italian, Spanish, and Korean sentences to English. It
uses a variant of the Transformer-Big architecture with a shallower
decoder: 16 attention heads, 6 encoder layers, and 3 decoder layers on
TED2020 dataset.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2021-05-31 00:00" class="post-list__meta--date date"> Published on arXiv on :
                31 May 2021</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/mRASP2/image1.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/multilingual-nmt/mRASP2">mRASP2</a>
            </h2>
            <p class="excerpt">mRASP2 stands for “multilingual Random Aligned
Substitution Pre-training”. It’s mRASP2 because it’s
an extension to the
mRASP model
proposed by the same lab (ByteDance AI Lab) a year earlier. mRASP2
framework was proposed in 2021 and published in this paper: Contrastive
Learning for Many-to-many Multilingual Neural Machine
Translation. The official code
for this paper can be found in this GitHub repository:
mRASP2.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2021-05-20 00:00" class="post-list__meta--date date"> Published on arXiv on :
                20 May 2021</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
    </ol>
    <h1 class="page-heading">2020</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/XLM-T/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/multilingual-nmt/XLM-T">XLM-T</a>
            </h2>
            <p class="excerpt">XLM-T stands for “Cross-lingual Language Modeling-Transformer” which is
a multi-lingual machine translation model proposed by Microsoft in 2020
and published in their paper: XLM-T: Scaling up Multilingual Machine
Translation with Pretrained Cross-lingual Transformer
Encoders. The official code of this
paper can be found in Microsoft’s official GitHub repository:
unilm/xlmt. Most
existing MNMTs adopt a randomly initialized Transformer backbone. In
this work, the researchers used an a pre-trained cross-lingual
Transformer encoder such as
(XLM or
XLM-R) to
initialize both the encoder and decoder of the multilingual NMT model,
and then fine-tuned it with multilingual parallel data.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2020-12-31 00:00" class="post-list__meta--date date"> Published on arXiv on :
                31 Dec 2020</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/M2M-100/image1.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/multilingual-nmt/M2M-100">M2M-100</a>
            </h2>
            <p class="excerpt">M2M stands for “Many-to-Many” which is a multilingual NMT model using
many-to-many datasets. The model was created by Facebook AI in 2020 and
published in their paper: “Beyond English-Centric Multilingual Machine
Translation”. The official code
for this paper can be found on the official FairSeq repository:
m2m_100

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2020-10-21 00:00" class="post-list__meta--date date"> Published on arXiv on :
                21 Oct 2020</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/cMNMT/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/multilingual-nmt/cMNMT">cMNMT: Complete MNMT</a>
            </h2>
            <p class="excerpt">cMNMT stands for “Complete Multilingual Neural Machine Translation”
which is a multilingual NMT model proposed by Google Research in 2020
and published in their paper: “Complete Multilingual Neural Machine
Translation”. Multilingual Neural
Machine Translation models are called complete when they are
trained for all possible source-target pairs. The following figure shows
the difference between the data of an English-Centric MNMT (left) and a
complete MNMT (right) on a six-languages dataset:

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2020-10-20 00:00" class="post-list__meta--date date"> Published on arXiv on :
                20 Oct 2020</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/mRASP/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/multilingual-nmt/mRASP">mRASP</a>
            </h2>
            <p class="excerpt">mRASP stands for “multilingual Random Aligned Substitution Pre-training”
which is a pre-training method for multilingual NMT models proposed by
ByteDance AI Lab in 2020 and published in their paper: “Pre-training
Multilingual Neural Machine Translation by Leveraging Alignment
Information”. The official code
for this paper can be found on this GitHub repository:
mRASP.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2020-10-07 00:00" class="post-list__meta--date date"> Published on arXiv on :
                7 Oct 2020</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/MTL/image8.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/multilingual-nmt/MTL">MTL: Multi-task Learning</a>
            </h2>
            <p class="excerpt">MTL stands for “Multi-task Learning” and it is a framework that jointly
trains multilingual neural machine translation (MNMT) models on bitext
data and monolingual data. The bitext data is used for the translation
task while the monolingual data is used for the denoising language
modeling tasks. This framework was proposed by Microsoft in 2020 and
published in their paper: Multi-task Learning for Multilingual Neural
Machine Translation.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2020-10-06 00:00" class="post-list__meta--date date"> Published on arXiv on :
                6 Oct 2020</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/mBART-50/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/multilingual-nmt/mBART-50">mBART-50</a>
            </h2>
            <p class="excerpt">In this part, we are going to discuss this paper: Multilingual
Translation with Extensible Multilingual Pretraining and
Finetuning (the official code:
mbart)
which doubles the number of languages in
mBART to suppor
multilingual machine translation models of 50 languages without loss of
performance. Also, This paper tried to fix some of the issues found in
mBART such as:

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2020-08-02 00:00" class="post-list__meta--date date"> Published on arXiv on :
                2 Aug 2020</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/CRISS/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/multilingual-nmt/CRISS">CRISS</a>
            </h2>
            <p class="excerpt">CRISS stands for “Cross-lingual Retrieval for Iterative
Self-Supervised Training” which was created by FacebookAI in
2020 and published in this paper: “Cross-lingual Retrieval for
Iterative Self-Supervised
Training. The official code for
this paper can be found in the fairseq GitHub repository:
criss.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2020-06-16 00:00" class="post-list__meta--date date"> Published on arXiv on :
                16 Jun 2020</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/mBART/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/multilingual-nmt/mBART">mBART</a>
            </h2>
            <p class="excerpt">mBART stands for “Multilingual Bidirectional Auto-regressive
Transformer” which is a multilingual NMT model proposed by FacebookAI in
2020 and published in their paper: “Multilingual Denoising Pre-training
for Neural Machine Translation”.
The official code for this paper can be found in the fairseq GitHub
repository:
mbart.
mBART is the first method for pre-training a complete
sequence-to-sequence model by denoising full texts in multiple
monolingual data, while previous approaches have focused only on the
encoder/decoder.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2020-01-22 00:00" class="post-list__meta--date date"> Published on arXiv on :
                22 Jan 2020</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
    </ol>
    <h1 class="page-heading">2019</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/Analyzing_M4/image1.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/multilingual-nmt/Analyzing_M4">Analyzing M4 using SVCCA</a>
            </h2>
            <p class="excerpt">Multilingual Neural Machine Translation (MNMT) models have yielded large
empirical success in transfer learning settings. However, these
black-box representations are poorly understood, and their mode of
transfer remains elusive. This paper “Investigating Multilingual NMT
Representations at Scale”
published by Google in 2019 attempted to understand MNMT representations
(specifically Google’s
M4 model) using
Singular Value Canonical Correlation Analysis (SVCCA). Google’s
unofficial code for the SVCCA framework can be found on their official
GitHub repository: google/svcca.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2019-09-05 00:00" class="post-list__meta--date date"> Published on arXiv on :
                5 Sep 2019</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/M4/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/multilingual-nmt/M4">Google's M4 Model</a>
            </h2>
            <p class="excerpt">M4 stands for “Massively Multilingual, Massive Machine Translation”
which is a multilingual NMT model that is trained on over 25 billion
parallel sentences in 103 languages. This model was proposed by Google
AI in 2019 and published in their paper: Massively Multilingual Neural
Machine Translation in the Wild: Findings and
Challenges.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2019-07-11 00:00" class="post-list__meta--date date"> Published on arXiv on :
                11 Jul 2019</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/Massively_MNMT/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/multilingual-nmt/Massively_MNMT">Massively MNMT</a>
            </h2>
            <p class="excerpt">Massively MNMT is a multilingual many-to-many NMT model proposed by
Google Research in 2019 and published in their paper: Massively
Multilingual Neural Machine
Translation. Massively MNMT is a
standard Base-Transformer with 6 layers in both the encoder and the
decoder. To enable many-to-many translation, the authors added a
target-language prefix token to each source sentence.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2019-07-02 00:00" class="post-list__meta--date date"> Published on arXiv on :
                2 Jul 2019</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
    </ol>
    <h1 class="page-heading">2016</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/Multilingual_GNMT/image1.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/multilingual-nmt/Multilingual_GNMT">Multilingual Google's NMT</a>
            </h2>
            <p class="excerpt">GNMT stands for “Google Neural Machine Translation” which is a
bilingual machine translation architecture that was
discussed before in this post:
GNMT. Here, we
are going to discuss how they extended the bilingual nature of the GNMT
model to be multilingual. The Multilingual GNMT architecture, as seen in
the following figure, was proposed in 2016 by the Google Research team
and published in this paper: Google’s Multilingual Neural Machine
Translation System: Enabling Zero-Shot
Translation. The official code
for this paper can be found in the TensorFlow’s official GitHub repository:
TensorFlow/GNMT.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2016-11-14 00:00" class="post-list__meta--date date"> Published on arXiv on :
                14 Nov 2016</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
         </ol>
      
    <hr class="post-list__divider ">

    <!--  -->
  </div>
  </div>
      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>