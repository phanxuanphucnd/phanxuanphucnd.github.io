[
  
  
    
    
      
      
      {
        "collection": "Cross-lingual Langluage Model",
        "title"     : "mBERT: Multilingual BERT",
        "url"       : "/cross-lingual-lm/mBERT",
        "date"      : "04/11/2018",
        "content": "mBERT is a multilingual BERT pre-trained on 104 languages, released by the\nauthors of the original paper on Google Research’s official GitHub repository: google-research/bert\n on \nNovember 2018. mBERT follows the same structure of BERT. The only difference is\nthat mBERT is pre-trained on concatenated Wikipedia data for 104 languages and\nit does surprisingly well compared to cross-lingual word embeddings on zero-shot\ncross-lingual transfer in XNLI dataset.\n\nXNLI results are promising, but the question is: does mBERT learn a\ncross-lingual space that supports zero-shot transfer? This paper: Beto,\nBentz, Becas: The Surprising Cross-Lingual Effectiveness of\nBERT published in 2019 by John\nHopkins University explores the cross-lingual potential of mBERT on five\ndifferent NLP tasks: NLI, NER, POS tagging, MLDoc classification, and\ndependency parsing. According to the following graph, all five tasks\ncover 39 languages out of the 104 languages that mBERT was pre-trained\non. The official code of this paper can be found on this GitHub repository:\ncrosslingual-nlp.\n\n\n    \n\n\n\n  Note:\nI suggest reading the\nBERT part before\ngoing on to the next part.\n\n\nResults\n\nAll the following results were obtained by using the mBERT~BASE~ which\n$12$ transformer blocks, each has $12$ attention heads and $d_{h} = 768$\nhidden dimensions forming around $179M$ parameters. mBERT uses dropout\nof $0.1$ as a regularizer.\n\nFor each task, no preprocessing is performed except tokenization of\nwords into subwords with WordPiece. The vocabulary size for all 104\nlanguages about $120k$ vocabulary. For fine-tuning, they used Adam for\nfine-tuning with $\\beta_{1} = 0.9,\\ \\beta_{2} = 0.999$ and L2 weight\ndecay of $0.01$. They warmed up the learning rate over the first $10\\%$\nof batches using linearly decay. Best hyper-parameters was selected by\nsearching a combination of batch size, learning rate and the number of\nfine-tuning epochs with the following range:\n\n\n  \n    Learning rate:\n$\\left\\{ 2 \\times 10^{- 5},\\ 3 \\times 10^{- 5},\\ 5 \\times 10^{- 5} \\right\\}$.\n  \n  \n    Batch size: $\\left\\{ 16,\\ 32 \\right\\}$.\n  \n  \n    Number of epochs: $\\left\\{ 3,\\ 4 \\right\\}$.\n  \n\n\nIn the following results, ♠ denotes the mBERT version trained on bitext\ndata found in this paper: Massively multilingual sentence embeddings\nfor zeroshot cross-lingual transfer and\nbeyond while † denoted to XLM\nmodel proposed by this paper: Cross-lingual Language Model\nPretraining. All results were\nobtained by the model selected by development performance in English.\nHowever, ◊ denotes model selection with target language dev set instead.\nThe best results are going to be bold-faced, while the second best\nresults are going to be underlined according to the following NLP\ntasks:\n\n\n  MLDOC classification: On average, mBERT is the second best model\nafter the XLM model with a very small margin.\n\n\n\n    \n\n\n\n  XNLI: On average, mBERT performs relatively worse than XLM.\n\n\n\n    \n\n\n\n  NER: On average, mBERT outperforms other models on this task.\n\n\n\n    \n\n\n\n  POS Tagging:\n\n\n\n    \n\n\n\n  Dependency Parsing:\n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Cross-lingual Langluage Model",
        "title"     : "XLM",
        "url"       : "/cross-lingual-lm/XLM",
        "date"      : "22/01/2019",
        "content": "XLM stands for “Cross-lingual Language Modeling” which is a model\ncreated by FacebookAI in 2019 and published in this paper:\n“Cross-lingual Language Model Pretraining”.\nXLM is an 12-layer encoder-transformer of with 1024\nhidden units, 16 heads, and GELU activation.\n\nXLM was trained on multiple languages with new pre-training objectives\nthat pushed the XNLI benchmark by an absolute gain of 4.9% accuracy and\nobtained 34.3 BLEU on WMT’16 German-English which is 9 BELU more than\nthe state-of-the-art back then. The official code for this paper can be\nfound in the FairSeq official repository:\nfairseq/cross_lingual_language_model.\n\nIn this paper, the authors presented three language modeling objectives\nfor the language modeling. Two of them only require monolingual data\n(unsupervised), while the third one requires parallel sentences\n(supervised). And these objectives are:\n\n\n  \n    CLM (Causal Language Modeling).\n  \n  \n    MLM (Masked Language Modeling).\n  \n  \n    TLM (Translation Language Modeling).\n  \n\n\nBefore getting into pre-training, let’s first check the vocabulary used\nfor our XLM.\n\nShared Vocabulary\n\nIn the paper, they processed all languages with the same shared\nvocabulary created through Byte Pair Encoding (BPE). They used 80k BPE\nsplits and a vocabulary of 95k. This greatly improves the alignment of\nembedding spaces across languages that share either the same alphabet or\nother tokens such as digits or proper nouns.\n\nThey learned the BPE splits on the concatenation of sentences sampled\nrandomly from the monolingual corpora. Sentences are sampled according\nto a multinomial distribution with probabilities\n$\\left[ q_i \\right]_{i = 1..N}$, where $N$ is the number of\nlanguages and $n_{i}$ is the number of sentences in the $i^{th}$\nmonolingual corpora:\n\n\\[q_{i} = \\frac{p_{i}^{\\alpha}}{\\sum_{j = 1}^{N}p_{j}^{\\alpha}},\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ p_{i} = \\frac{n_{i}}{\\sum_{k = 1}^{N}n_{k}}\\]\n\nSampling with this distribution increases the number of tokens\nassociated to low-resource languages and alleviates the bias towards\nhigh-resource languages. In particular, this prevents words of\nlow-resource languages from being split at the character level.\n\nPre-training Objectives\n\nIn this part, we are going to discuss the three language modeling\nobjectives used for the language modeling pre-training. The first two of\nthem only require monolingual data (unsupervised), while the third one\nrequires parallel sentences (supervised).\n\nCLM\n\nCLM stands for “Causal Language Modeling” which is one of the three\nobjectives used for learning the cross-lingual language model. This\nobjective is used to train the model predict the probability of a word\ngiven the previous words in a sentence\n$P\\left( w_{t} \\middle| w_{1},\\ …\\ w_{t - 1},\\ \\theta \\right)$.\nOpenAI’s GPT and GPT-2 are trained on this objective. You can refer to\nmy articles on GPT and GPT-2 if you’re interested in the details of this\nobjective.\n\nMLM\n\nMLM stands for “Masked Language Modeling” which is the second objective\nout of the three objectives used for learning the cross-lingual language\nmodel. MLM is the same objective used for BERT where we randomly sample\n15% of the BPE tokens from the text streams, replace them by a [MASK]\ntoken 80% of the time, by a random token 10% of the time, and we keep\nthem unchanged 10% of the time.\n\n\n    \n\n\nIn face, there are two differences between this approach and the one\nused with BERT. And they are:\n\n\n  \n    Here, they used text streams of an arbitrary number of sentences\n(truncated at 256 tokens) instead of pairs of sentences.\n  \n  \n    To counter the imbalance between frequent tokens (e.g. punctuations\nor stop words) and rare tokens, they sub-sampled the frequent\noutputs according to a multinomial distribution, whose weights are\nproportional to the square root of their invert frequencies.\n  \n\n\nTLM\n\nTLM stands for “Translation Language Modeling” which is the third and\nlast objective used for learning the cross-lingual language model. Both\nthe CLM and MLM objectives are unsupervised and only require monolingual\ndata. However, these objectives cannot be used to leverage parallel data\nwhen it is available. So, this objective is used to leverage the\nexistence of parallel data to improve cross-lingual pre-training.\n\nTLM is an extension of MLM, where instead of considering monolingual\ntext streams, we concatenate parallel sentences and perform MLM over\nthis concatenation. For example, let’s assume that we have a parallel\nentry of two languages English and French as shown below:\n\n\n    \n\n\nTo predict a word masked in an English sentence, the model can either\nattend to surrounding English words or to the French translation,\nencouraging the model to align the English and French representations.\nIn particular, the model can leverage the French context if the English\none is not sufficient to infer the masked English words. To facilitate\nthe alignment, we also reset the positions of target sentences.\n\nFine-tuning\n\nIn the paper, the XLM model was fine-tuned on various tasks -all from\nthe XNLI dataset- which shows that XLM can be used for a wide variety of\ncross-lingual tasks:\n\n\n  Zero-shot Cross-lingual Classification:\nThey fine-tuned XLMs on a cross-lingual classification by adding a\nlinear classifier on top of the first hidden state of the pretrained\nTransformer, and fine-tuned all parameters on the English NLI\ntraining dataset and evaluated it on the other 15 XNLI languages:\n\n\n\n    \n\n\n\n  Unsupervised Machine Translation:\nThey pretrained the entire encoder and decoder with a\ncross-lingual language model and explored various initialization\nschemes as shown below:\n\n\n\n    \n\n\n\n  \n    Low-resource Language Modeling:\nHere’s where “languages with the same script or similar words\nprovide better mapping” comes into the picture. For example, around\n80% of Nepali tokens (low-resource language) is in common with Hindi\n(high-resource-language):\n  \n  \n    Cross-lingual Word Embedding:\nSince we have a shared vocabulary, the lookup table (or embedding\nmatrix) of the XLM model gives us the cross-lingual word embeddings.\n  \n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Cross-lingual Langluage Model",
        "title"     : "XNLG",
        "url"       : "/cross-lingual-lm/XNLG",
        "date"      : "23/09/2019",
        "content": "XNLG stands for “Cross-lingual Natural Language Generation” which is an\nencoder-decoder cross-lingual model designed for Natural Language\nGeneration (NLG) tasks such as question generation and abstractive\nsummarization. This model was created by Microsoft in 2019 and published\nin their paper: Cross-Lingual Natural Language Generation via\nPre-Training. The official code\nfor this paper is found in the following GitHub repository:\nxnlg.\n\nAs shown in the following figure, XNLG is an encoder-decoder\nTransformer\nmodel pre-trained on on monolingual NLG training data and can be\nfine-tuned later on other languages which can boost performance for the\nlow-resource settings.\n\n\n    \n\n\nPre-training\n\nGiven a parallel corpus $\\mathcal{D}_{p}$ and a monolingual corpus\n$\\mathcal{D}_{m}$, the XNLG pre-training is done in two stages as\nshown below:\n\n\n  Stage #1: pre-trains the encoder on two tasks: Masked Language\nModeling (MLM) and cross-lingual Masked Language Modeling (XMLM)\nwhere the encoder learns to minimize:\n\n\n\\[\\mathcal{L}_{1} = \\sum_{\\left( x,y \\right) \\in \\mathcal{D}_{p}}^{}\\mathcal{L}_{\\text{XMLM}}^{\\left( x,y \\right)} + \\sum_{x \\in \\mathcal{D}_{m}}^{}\\mathcal{L}_{\\text{MLM}}^{\\left( x \\right)}\\]\n\n\n  Stage #2: pre-trains the model\nwhere the decoder parameters are updated while freezing the encoder\nparameters on two tasks: (Denoising Auto-Encoding) DAE and\ncross-lingual Auto-Encoding (XAE). Here, the model learns to\nminimize:\n\n\n\\[\\mathcal{L}_{2} = \\sum_{\\left( x,y \\right) \\in \\mathcal{D}_{p}}^{}\\mathcal{L}_{\\text{XAE}}^{\\left( x,y \\right)} + \\sum_{x \\in \\mathcal{D}_{m}}^{}\\mathcal{L}_{\\text{DAE}}^{\\left( x \\right)}\\]\n\n\n    \n    \n\n\nThese are the the two stages of pre-training in XNLG, now let’s dive\ndeeper into pre-training tasks mentioned above:\n\n\n  Masked Language Modeling (MLM):\nThe masked language modeling (MLM) task was first proposed by the\nBERT paper.\nGiven an input sequenc $x$, MLM randomly masks $15\\%$ of the tokens\nin a monolingual sentence. Each masked token is substituted with a\nspecial token $\\left\\lbrack M \\right\\rbrack$, a random token, or the\nunchanged token with probabilities of $80\\%$, $10\\%$, and $10\\%$,\nrespectively to create a masked sequence $x_{\\backslash M_{x}}$\nknowing that $M_{x}$ is the set of randomly masked positions. The\nmonolingual MLM loss is defined as:\n\n\n\\[\\mathcal{L}_{\\text{MLM}}^{\\left( x \\right)} = - \\sum_{i \\in M_{x}}^{}{\\log\\left( p\\left( x_{i} \\middle| x_{\\backslash M_{x}} \\right) \\right)}\\]\n\n\n  Cross-lingual Masked Language Modeling (XMLM):\nSimilar to MLM, the masked token prediction task can be extended to\ncross-lingual settings. Given a parallel corpus, bilingual sentences\n$\\left( x,\\ y \\right)$ can be concatenated to form one sequence\nwhich can be used as the input of MLM. Given $M_{x}$ and $M_{y}$ as\nthe sets of masked positions of $x$ and $y$ respectively, the XMLM\nloss is defined as:\n\n\n\\[\\mathcal{L}_{\\text{XMLM}}^{\\left( x,\\ y \\right)} = - \\sum_{i \\in M_{x}}^{}{\\log\\left( p\\left( x_{i} \\middle| x_{\\backslash M_{x}},\\ y_{\\backslash M_{y}} \\right) \\right)} - \\sum_{i \\in M_{y}}^{}{\\log\\left( p\\left( y_{i} \\middle| x_{\\backslash M_{x}},\\ y_{\\backslash M_{y}} \\right) \\right)}\\]\n\n\n    \n\n\n\n  Denoising Auto-Encoding (DAE):\nGiven monolingual sentence $x$, DAE applies noise functions\nover the input sentence producing perturbed sentence $\\widehat{x}$.\nIts objective is to train the whole model to restore the original\nsentence. The noise functions applied here are: shuffling, randomly\ndropping tokens with a probability of $0.1$, and randomly replacing\ntokens with the special padding token [P] with a probability of\n$0.1$. The DAE loss function is defined as:\n\n\n\\[\\mathcal{L}_{\\text{DAE}}^{\\left( x \\right)} = - \\sum_{i = 1}^{\\left| x \\right|}{\\log\\left( p\\left( x_{i} \\middle| \\widehat{x},\\ x_{&amp;lt; i} \\right) \\right)}\\]\n\n\n  Cross-Lingual Auto-Encoding (XAE):\nXAE is a the multilingual-version DAE which can be viewed as a\nmachine translation task. The cross-lingual auto-encoding loss is:\n\n\n\\[\\mathcal{L}_{\\text{XAE}}^{\\left( x,\\ y \\right)} = - log\\left( p\\left( y \\middle| x \\right) \\right) - log\\left( p\\left( x \\middle| y \\right) \\right)\\]\n\n\n    \n\n\nFine-tuning\n\nIn the fine-tuning procedure, there are two scenarios:\n\n\n  \n    Fine-tuning for Any-to-Others NLG:\nFine-tuning any language to non-English. In this scenario,\nthey observed catastrophic forgetting of target language. To\novercome that, they keep the decoder and the word embeddings frozen\nand only update the encoder parameters during fine-tuning.\n  \n  \n    Fine-tuning for Any-to-English NLG:\nFine-tuning any language to English. In this scenario, they freeze\nthe encoder parameters, and update the decoder parameters.\n  \n\n\n\n  Note:\nWhen the target language is the same as the language of training data,\nthey fine-tuned all parameters.\n\n\nExperiments\n\nThey conducted experiments over two cross-lingual NLG downstream tasks:\ncross-lingual question generation, and cross-lingual abstractive\nsummarization. They used a pre-trained XNLG with a 10-layer encoder and\na 6-layer decoder. For every Transformer layer, they used $1024$ hidden\nunits, $8$ attention heads, and GELU activations.\n\nIn the first pre-training stage, they directly used the pre-trained XLM\nto initialize the XNLG parameters. In the second pre-training stage,\nthey used Wikipedia as the monolingual data for the DAE objective, and\nMultiUN as the parallel data for the XAE objective. In pre-training,\nthey used Adam optimizer with a linear warm-up over the first $4k$ steps\nand linear decay for later steps, and the learning rate is set to\n$10^{- 4}$. The pre-training batch size is $64$, and the sequence length\nis set to $256$.\n\nFor fine-tuning on downstream NLG tasks, they used Adam optimizer with a\nlearning rate of $5 \\times 10^{- 6}$. They set the batch size as $16$\nand $32$ for question generation and abstractive summarization,\nrespectively. They truncate the input sentences to the first $256$\ntokens. During decoding, they used beam search with a beam size of $3$,\nand limit the length of the target sequence to $80$ tokens.\n\nThe following table shows the evaluation results of monolingual\nsupervised question generation for English and Chinese. BL is short for\nBLEU, MTR for METEOR, and RG for ROUGE.\n\n\n    \n\n\nThe following table shows the evaluation results of zero-shot\nChinese-Chinese question generation. These results show that XNLG\nconsistently performs better than baselines in both zero-shot and\nmonolingual supervised setting.\n\n\n    \n\n\nThe following table shows the ROUGE evaluation results of supervised\nmonolingual summarization. These results show that XNLG outperforms all\nthe baseline models on both French and Chinese AS:\n\n\n    \n\n\nTo check the effect of pre-training techniques on the performance, they\nconduct ablation studies models were evaluated on zero-shot\nChinese-Chinese question generation task. The results show that the\nmodel benefits from the DAE objective for the zero-shot Chinese question\ngeneration task. The results also demonstrate that combining DAE and XAE\ncan alleviate the spurious correlation issue and improves cross-lingual\nNLG.\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Cross-lingual Langluage Model",
        "title"     : "Unicoder: Universal Encoder",
        "url"       : "/cross-lingual-lm/Unicoder",
        "date"      : "03/11/2019",
        "content": "Unicoder is an abbreviation of “Universal Encoder” which is a language\nencoder that is universal across different number of languages. The\nUnicoder model was proposed by Microsoft in 2019 and published in this\npaper: Unicoder: A Universal Language Encoder by Pre-training with\nMultiple Cross-lingual Tasks.\nThe official code for this paper can be found on Microsoft’s official\nGitHub repository:\nmicrosoft/Unicoder.\n\nA universal language encoder, or Unicoder, is a model that is able to\ngenerate a sentence representation embedding independent on the input\nlanguage. From the name of the paper, we can see that they were able to\ncreate this universal language encoder by using pre-training of\ncross-lingual tasks. These cross-lingual tasks help Unicoder to learn\nthe mappings of different languages. And that’s what we are going to\ntalk about next.\n\nPre-training Objectives\n\nIn this part, we are going to discuss the five language modeling\nobjectives used for the language modeling pre-training in the Unicoder.\nSince the XLM model outperformed mBERT in cross-lingual tasks, Unicoder\nuses the same structure as XLM and uses the same pre-training tasks\nalong with three more novel pre-training tasks; all listed below:\n\nMLM\n\nMLM stands for “Masked Language Modeling” which is the first objective\nout of the five objectives used for learning the Unicoder. MLM is the\nsame objective used for BERT where we randomly sample $15\\%$ of the BPE\ntokens from the text streams, replace them by a $\\lbrack MASK\\rbrack$\ntoken $80\\%$ of the time, by a random token $10\\%$ of the time, and we\nkeep them unchanged $10\\%$ of the time.\n\n\n    \n\n\nIn fact, there are two differences between this approach and the one\nused with BERT:\n\n\n  \n    Here, they used text streams of an arbitrary number of sentences\n(truncated at 256 tokens) instead of pairs of sentences.\n  \n  \n    To counter the imbalance between frequent tokens (e.g. punctuations\nor stop words) and rare tokens, they sub-sampled the frequent\noutputs according to a multinomial distribution, whose weights are\nproportional to the square root of their invert frequencies.\n  \n\n\nTLM\n\nTLM stands for “Translation Language Modeling” which is the second\nobjective used for pre-training the Unicoder. MLM objective is\nunsupervised and only requires monolingual data and cannot be used to\nleverage parallel data when it is available. So, this objective is used\nto leverage the existence of parallel data to improve cross-lingual\npre-training.\n\nTLM is an extension of MLM objective, where instead of considering\nmonolingual text streams, we concatenate parallel sentences and perform\nMLM over this concatenation. For example, let’s assume that we have a\nparallel entry of two languages English and French as shown below:\n\n\n    \n\n\nTo predict a word masked in an English sentence, the model can either\nattend to surrounding English words or to the French translation,\nencouraging the model to align the English and French representations.\nIn particular, the model can leverage the French context if the English\none is not sufficient to infer the masked English words. To facilitate\nthe alignment, we also reset the positions of target sentences.\n\nWord Recovery\n\nThis task aims to let the pre-trained model learn the underlying word\nalignments between two languages. This task is mainly motivated by the\nattention matrix used in the neural machine translation models. So,\nlet’s first recap how that attention mechanism worked:\n\nGiven a bilingual sentence pair $\\left( X,\\ Y \\right)$ of two languages\n$\\left( s,\\ t \\right)$ of lengths $n$ and $m$ respectively, we obtain\nword embeddings $\\left( X^{s},\\ Y^{t} \\right)$. The XLM way of doing\nthat is by adding the language and position embeddings over this pair to\nobtain word embeddings:\n\n\\[X^{s} = \\left\\{ x_{1}^{s},...x_{n}^{s} \\right\\},\\ \\ \\ \\ \\ \\ \\ Y^{t} = \\left\\{ y_{1}^{t},...y_{m}^{t} \\right\\}\\]\n\nNow that we have word embeddings, let’s apply the attention mechanism to\nobtain attentive embeddings\n$X^{t} = \\left\\{ x_{1}^{t},…x_{n}^{t} \\right\\}$. Using a trainable\nweight $W \\in \\mathbb{R}^{3 \\times h}$ where $h$ is the word embedding\ndimension, the attention matrix\n$A_{\\text{ij}} \\in \\mathbb{R}^{m \\times n}$ will be:\n\n\\[A_{\\text{ij}} = W\\left\\lbrack x_{i}^{s},\\ y_{j}^{t},\\ x_{i}^{s}\\bigodot y_{j}^{s} \\right\\rbrack\\]\n\nWhere:\n\n\n  \n    $x_{i}^{s}$ is the $i^{\\text{th}}$ word embedding of the source\nsentence $s$.\n  \n  \n    $y_{j}^{t}$ is the $j^{\\text{th}}$ word embedding of the target\nsentence $t$.\n  \n  \n    $\\bigodot$ is element-wise multiplication.\n  \n  \n    $\\lbrack a,\\ b\\rbrack$ is the concatenation of two vectors; $a$ and\n$b$.\n  \n\n\nNow, the attentive embeddings $X^{t} = \\left\\{ x_{1}^{t},…x_{n}^{t} \\right\\}$\nwill be computed as:\n\n\\[x_{i}^{t} = \\sum_{j = 1}^{n}{\\text{softmax}\\left( A_{\\text{ij}} \\right)y_{j}^{t}}\\]\n\nThat’s how the attention mechanism works. Given that attentive embedding\n$X^{t}$ to this task, the model tries to predict the original sentence\n$X$ as shown in the following graph:\n\n\n    \n\n\nParaphrase Classification\n\nThis task takes the concatenation of two sentences from different\nlanguages as input and classifies whether they are with the same meaning\nor not. This task is shown below in the following figure:\n\n\n    \n\n\nThe dataset used in this task is created from machine translation\ndatasets. Each bilingual sentence pair $\\left( X,\\ Y \\right)$ servers as\na positive sample. For negative samples, the most straight forward\nmethod is to replace $Y$ to a random sampled sentence from the target\nlanguage. But this will make the classification task too easy.\n\n\n    \n\n\nTo make the negative samples hard, they\ntrained a light-weight paraphrase model with random negative samples.\nThen, they used this model to select sentence with high similarity score\nto $X$ but doesn’t equal to $Y$ as hard negative samples. They chose\nDeep Averaging Network (DAN) from this\npaper as the light-weight\nmodel. The dataset had $50\\%$ positive samples and $50\\%$ negative\nsamples.\n\nCross-lingual Masked Language Model\n\nPrevious successful pre-training language models such as\nBERT and\nGPT, were conducted\non document-level corpus rather than sentence-level corpus.\nCross-lingual document is a sequence of sentences, and the sentences are\nwritten with different languages.\n\nThat’s why the researchers proposed cross-lingual masked language model\ntask whose input come from cross-lingual document. One issue is that\npeople won’t write cross-lingual document in most cases. To solve this\nissue, they decided to construct such the cross-lingual document-level\ncorpus by replacing the sentences to their translation. Then, they\nmasked this document-level corpus the same way as MLM as shown below:\n\n\n    \n\n\nIn the paper, they replaced sentences with even indices to their\ntranslation in the other language separating them with\n$\\left\\lbrack /S \\right\\rbrack$ token. They truncated the cross-lingual\ndocument till it became $256$-tokens long, then fed it to the Unicoder.\n\nData\n\nUnicoder is pre-trained on 15 languages. For MLM, they used the Wikipedia of\nthese 15 languages. The other four tasks needed MT dataset, so they used the\nsame dataset as the XLM paper which were collected from MultiUN, IIT Bombay\ncorpus, OpenSubtitles 2018, EUbookshop corpus and GlobalVoices. The number of\nparallel data they used is reported at the following table:\n\n\n    \n\n\n\n  Note:\n\n  \n    \n      When training, all language were sampled equally.\n    \n    \n      The max sequence length of MLM and cross-lingual language model\nis 256. For the other three tasks with two sentences as input, they\nset the max sequence length to 128 so the sum of them is 256.\n    \n  \n\n\nMultilingual Fine-tuning\n\nThe fine-tuning task used in this paper is machine translation.\nThere are three settings that people use when it comes to using machine\ntranslation for fine-tuning; and they are:\n\n\n  \n    TRANSLATE-TRAIN: fine-tune the model on pseudo translation of\ntarget language training set and test it on target\ntesting set.\n  \n  \n    TRANSLATE-TEST: fine-tune the model on source training\nset and test it on pseudo translation of source testing\nset.\n  \n  \n    TEST: fine-tune the model on source training set and\ntest it on target testing set.\n  \n\n\nTo further understand the difference , let’s assume that the source language is\nEnglish and the target language is Chinese. Each dataset has training set and\ntest set and the task is to evaluate on Chinese test, then the three settings\nwill do as the following figure:\n\n\n    \n\n\n\n  Note:\nThe source language and the target can be more than one language; but they can\nnever intersect. In other words, a language can’t be in the source and the\ntarget at the same time.\n\n\nInspired by multi-task learning, the authors proposed a new fine-tuning\nstrategy called “Multilingual Fine-tuning” where fine-tuning is done on\ntraining data of source language + pseudo translation of the source language\nas shown in the following figure:\n\n\n    \n\n\nResults\n\nThe Unicoder model is initialized from the XLM model, which means it is\na $12$-layer transformer with $1024\\ $hidden units, $16\\ $heads, GELU\nactivation. To build the vocabulary, they used BPE tokenization where\nthe vocabulary size is $95,000$. To train Unicoder, they used Adam\noptimizer with learning rate that starts from $1e^{- 5}$ with invert\nsquare root decay. As a regularizer, they used a dropout of $0.1$. The\nbatch size was set to $512$ in pre-training and $32$ in fine-tuning.\n\nThey fine-tuned Unicoder over these benchmarks:\n\n\n  XNLI: The following table shows test accuracy on the 15 XNLI\nlanguages. The source language here is English while the target\nlanguage changes. The tables shows that Unicoder with Multilingual\nFine-tuning achieves the best performance across all 15 languages.\n\n\n\n    \n\n\n    Using more languages in fine-tuning leads to better\nperformance on the XNLI benchmark:\n\n\n    \n\n\n\n  XQA: The following table shows test accuracy on the XQA. The\naverage column is the average of “French (fr)” and “German (de)”\nresult. And again, Unicoder with multilingual fine-tuning achieves\nthe best performance across all three languages.\n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Cross-lingual Langluage Model",
        "title"     : "XLM-R",
        "url"       : "/cross-lingual-lm/XLM-R",
        "date"      : "05/11/2019",
        "content": "XLM-R stands for “Cross-lingual Language Modeling-RoBERTa” which was\ncreated by FacebookAI in 2019 and published in this paper:\n“Unsupervised Cross-lingual Representation Learning at\nScale”. The goal of paper is to\nimprove the XLM model’s performance on both cross-lingual and\nmonolingual understanding tasks. XLM-R follows the XLM model\narchitecture with a changes that improve the performance at scale. And\nthese changes are:\n\n\n  Maked Language Modeling:\nIn XLM-R, they used the same MLM objective as the XLM model with\nonly once change which is they removed the language embeddings\nwhich allows the model to better deal with code-switching.\n\n\n\n    \n\n\n\n  \n    Removed TLM:\nThe focus of this paper is Unsupervised Cross-lingual Representation\nLearning. So, it made sense to remove the supervised objective in\nthe XLM model which was the Translation Language Modeling objective\nor TLM for short.\n  \n  \n    Scaling to 100 languages:\nXLM-R is trained on 100 languages unlike XLM which was trained on\njust 15 languages. And for the first time, XLM-R shows that it is\npossible to have a single large model for all languages, without\nsacrificing per-language performance.\n  \n\n\n\n    \n\n\n\n  \n    Bigger Vocabulary:\nXLM-R uses a shared vocabulary of 250k tokens while XLM uses 95k.\nThe vocabulary consists of subwords encoded using Byte-Pair Encoding\n(BPE).\n  \n  \n    Validation Stopping Criterion:\nXLM uses the perplexity as a stopping criterion for pre-training. In\nXLM-R, they used downstream tasks as a stopping criterion as they\nobserved that performance on downstream tasks continues to improve\neven after validation perplexity has plateaued.\n  \n  \n    Increasing the Training Data:\nThey collected monolingual data about 100 languages using common\ncrawl which was about\n2.5 Terabytes. The following figure shows the difference between\nWikipedia (the data used to train multilingual BERT or mBERT) and\nCommon Crawl (the data used to train XLM-R).\n  \n\n\n\n    \n\n\nAnd that is basically it! There are no novel parts in this paper. However, this\npaper provides a good analysis of multilingual models as we are going to see\nnext.\n\nTrade-offs\n\nAs I said earlier, this paper doesn’t provide any novel parts but it\nprovides a good understanding and analysis to the multilingual models\naccording to various factors:\n\nPerformance Vs # languages\n\nIn the paper, they called this the “the curse of multilinguality”. For a\nfixed sized model, the per-language capacity decreases as we increase\nthe number of languages. While low-resource language performance can be\nimproved by adding similar higher-resource languages. So, these two\nfactors have to be traded off against each other.\n\nUsing XNLI as a metric, the authors trained different fixed-size XLM-R models\nin 7, 15, 30, 60 and 100 languages. And they found out that; initially, as we\ngo from 7 to 15 languages, the model is able to take advantage of positive\ntransfer which improves performance, especially on low resource languages.\nBeyond this point the curse of multilinguality kicks in and degrades\nperformance across all languages.\n\n\n    \n\n\nThe issue is even more prominent when the capacity of the model is small. To\nshow this, they made the Transformer wider as they added more languages. They\ntrained models in 7, 30, and 100 languages with hidden size of 768, 960, and\n1152 respectively. As the following figure shows, the added capacity allows\nXLM-30 to be on par with XLM-7, thus overcoming the curse of multilinguality.\nBut the added capacity for XLM-100, however, is not enough and it still lags\nbehind.\n\n\n    \n\n\nPerformance Vs Low / High-resource\n\nHere, we study the effect of data sampling of high-resource (English and\nFrench) and low-resource (Swahili and Urdu) languages on the\nperformance. Specifically, we investigate the impact of varying the\n$\\alpha$ parameter which controls the exponential smoothing of the\nlanguage sampling rate. Recap the shared vocabulary part from XLM\nif you can’t relate.\n\nModels trained with higher values of $\\alpha$, see batches of high-resource\nlanguages more often which increases the performance on high-resource languages.\nAnd when we decrease the value of , the model see batches of low-resource\nlanguages more often. So, we need to trade-off between these two factors.\n\n\n    \n\n\nThey found that $\\alpha = 0.3$ is the optimal value for α if we are\nconsidering both low-resource and high-resource languages.\n\nPerformance Vs Vocabulary\n\nPreviously, we showed the importance of scaling the model size as we\nincrease the number of languages. Similarly, scaling the size of the\nshared vocabulary can improve the performance.\n\nTo illustrate this effect, they trained multiple models with different\nvocabulary sizes while keeping the overall number of parameters constant\nby adjusting the width of the transformer. And as shown in the following\nfigure, we observe a 2.8% increase in XNLI average accuracy as we\nincrease the vocabulary size from 32K to 256K:\n\n\n    \n\n\nPerformance Vs Data Size\n\nAs you have probably figured out, the more data you have, the better the\ncross-lingual language model will be. They compared the same model\ntrained on two different datasets: Wikipedia (60 Gigabytes) and Common\nCrawl (2.5 Terabytes).\n\n\n    \n\n\nPerformance Vs Batch Size\n\nSimilarly for the batch sizes, the bigger the batch size is, the better\nthe cross-lingual language model will be:\n\n\n    \n\n\nResults\n\nHere, we are going to talk about the results of XLM-R model in\ncomparison with all other state-of-the-art models on multiple tasks. In\nthe paper, they used different sizes of XLM-R which are:\n\n\n\n    \n        \n            \n            XLM-R BASE\n            XLM-R\n        \n    \n    \n        Transformer Blocks\n        12\n        24\n    \n    \n        Feed-Forward hidden neurons\n        768\n        1024\n    \n    \n        Attention Heads\n        12\n        16\n    \n    \n        Parameters\n        270 million\n        550 million\n    \n\n\n\nMultilingual Results\n\nHere, we are going to compare the XLM-R model over cross-lingual\nunderstanding tasks such as:\n\n\n  XNLI:\nThe following table shows the performance on the XNLI dataset\nwhich contains 15 different languages. In the table we can see that\nusing the translate-train-all approach which leverages training sets\nfrom multiple languages, XLM-R obtains a new state of the art on\nXNLI of 83.6% average accuracy:\n\n\n\n    \n\n\n\n  NER:\nThe following table shows the results using CoNLL2002 and CoNLL-2003\ndatasets using the F1 metric:\n\n\n\n    \n\n\n\n  Question Answering:\nThe following table summarizes the results on MLQA question\nanswering using the F1 and EM (exact match) scores for zero-shot\nclassification where models are fine-tuned on the English Squad\ndataset and evaluated on the 7 languages of MLQA:\n\n\n\n    \n\n\nMonolingual Results\n\nHere, we are going to compare the XLM-R model over monolingual\nunderstanding tasks such as:\n\n\n  GLUE:\nThe following table shows the XLM-R performance on the the GLUE\nbenchmark:\n\n\n\n    \n\n\nImportant Finding\n\nIn this paper, they proved that the multilingual models obtain better\nperformance over their monolingual counter-parts. They provided the\nfirst comprehensive study to assess this claim on the XNLI benchmark.\nFor comparison, they used XLM models (called it XLM-7) and monolingual\nBERT models on 7 languages and compare performance as shown in the\nfollowing table:\n\n\n    \n\n\nAccording to the former table, we can conclude the following:\n\n\n  \n    Monolingual BERT models outperform XLM-7 for both Wikipedia and CC\nby 1.6% and 1.3% average accuracy.\n  \n  \n    XLM-7 outperforms BERT models when it leverages the training sets\ncoming from multiple languages (translate-train-all).\n  \n\n"
      },
    
      
      
      {
        "collection": "Cross-lingual Langluage Model",
        "title"     : "ALM: Alternating Language Modeling",
        "url"       : "/cross-lingual-lm/ALM",
        "date"      : "03/04/2020",
        "content": "ALM stands for “Alternating Language Modeling” which is a novel\ncross-lingual pre-training method proposed by Microsoft in 2021 and\npublished in their paper: Alternating Language Modeling for Cross-Lingual\nPre-Training.\nThe official code for this paper can be found in the following\nrepository: ALM.\n\nThe idea behind ALM is to predict one language conditioned on the\ncontext of the other language which can minor the gap between the\nembeddings of languages and that’s very beneficial for the cross-lingual\nsetting. The following figure shows the difference between ALM and XLM.\nIn ALM, the input sequence of ALM is mixed with different languages.\n\n\n    \n\n\nCode-Switching\n\nGiven a bilingual sentence pair $\\left( X,Y \\right)$ with the source\nsentence $X = \\left\\{ x_{1},\\ x_{2},\\ …x_{N} \\right\\}$ and the target\ntranslation $Y = \\left\\{ y_{1},\\ y_{2},\\ …y_{M} \\right\\}$, where $N$\nand $M$ are the lengths of the source and target sentences respectively,\na code-switched sentence $U = \\left\\{ u_{1},\\ u_{2},\\ …x_{L} \\right\\}$\nof length $L$ is composed of phrases from $X$ and $Y$.\n\nFor each phrase $u_{\\left\\lbrack i:j \\right\\rbrack}$, it comes from\neither the source sentence\n$x_{\\left\\lbrack a:b \\right\\rbrack},\\ 1 \\leq a \\leq b \\leq N$ or the\ntarget sentence\n$y_{\\left\\lbrack c:d \\right\\rbrack},\\ 1 \\leq c \\leq d \\leq N$ where the\nconstraint is that these two phrases are the linguistic translation\ncounterpart in the parallel sentence $\\left( X,Y \\right)$. The following\nfigure shows different examples of code-switched data between English\nand Chinese:\n\n\n    \n\n\nWhen most of the code-switched sequence $U$ is derived from $X$, this\nsample is called major-source-language sample. When most of the\ncode-switched sequence $U$ is derived from $Y$, this sample is called\nmajor-target-language.\n\nDue to the scarcity of natural code-switched sentences, the researchers\nhad to construct them from bilingual sentence pairs. They did that by\nfollowing these steps:\n\n\n  \n    First, they performed word alignment with the GIZA toolkit between\nthe parallel sentences, and extract a bilingual phrase table using\nstatistical machine translation techniques.\n  \n  \n    Then, for each sentence pair in training corpus, they created the\nmajor-source-language samples by substituting some phrases in\nsource sentence with the corresponding target phrases with highest\nprobabilities in phrase table. And same for\nmajor-target-language, they substituted some phrases in target\nsentence with the corresponding source phrases.\n  \n  \n    Each phrase is limited to less than 5 words for both source language\nand target language. And the substituted words are less than 30% of\nthe total words in the sentence.\n  \n  \n    Each bilingual sentence pair is used to create multiple alternating\nlanguage sentences by randomly choosing the substituted phrases as\nshown in the following figure:\n  \n\n\n\n    \n\n\nPre-training\n\nALM uses the encoder-part of the Transformer-base architecture with 1024\nembedding and hidden units, 8 heads and learned positional embeddings.\nDuring training, they used BPE with 95K sub-word tokens. They used Adam\noptimizer with parameters of $\\beta_{1} = 0.9$ and $\\beta_{2} = 0.98$\naccompanied with a dropout rate of $0.1$. They set the\ninverse-square-root learning rate schedule with a linear warmup where\nthe number of warmup step is $4000$ and a learning rate of $0.0005$. The\nbatch size was set to $8192$ tokens. During decoding, they sued a beam\nsize of 8.\n\nMLM was done like the following; Given a parallel sentence pair, they\ncombined two sentences from different languages into a single\ncode-switched sequence as described above. Then, they mask out a certain\npercentage of words in the sequences and feed them into Transformer\nmodel to learn to predict the words being masked out as shown in the\nfollowing figure:\n\n\n    \n\n\nData used in pre-training was monolingual data from Wikipedia and\nbilingual data from IWSLT14 German-to-English translation dataset. The\ninput sentences to the ALM model were $10\\%$ from the source sentence\n(without code-switching), $10\\%$ from the target sentence (without\ncode-switching), and the rest $80\\%$ were code-switched. MLM was applied\nby randomly masking $15\\%$ of the tokens using:\n\n\n  \n    The [MASK] token $80\\%$ of the time.\n  \n  \n    A random token $10\\%$ of the time.\n  \n  \n    Keep them unchanged $10\\%$ of the time.\n  \n\n\nMachine Translation\n\nAfter pre-training ALM on the MLM objective, they fine-tuned it on the\nmachine translation task. Using parallel data from WMT14 English-German\ndataset and IWSLT14 German-English dataset, they used ALM as the encoder\nof machine translation, and construct a Transformer-based decoder\ninitialized from the ALM. In other words, they fed the source language\ninto ALM, and generated the target language with decoder.\n\nFine-tuning was done with the Adam optimizer with a linear warmup as the\npre-training. The learning rates was tuned based on the performance on\nthe validation set which was $5 \\times 10^{- 4}$ for IWSLT14\nGerman-English dataset and $10^{- 3}$ for WMT14 English-German. Early\nstopping was used.\n\nThe following figure shows the performance on the two datasets; the one\non the left shows the results of WMT14 and the one on the right shows\nthe results of IWSLT14. In both datasets, we can see that ALM\noutperforms all other models:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Cross-lingual Langluage Model",
        "title"     : "LaBSE: Language-agnostic Sentence Embeddings",
        "url"       : "/cross-lingual-lm/LaBSE",
        "date"      : "03/07/2020",
        "content": "LaBSE stands for “Language-agnostic BERT Sentence Embedding” which is a\nmultilingual model the produces language-agnostic sentence embeddings\nfor 109 languages. LaBSE model was proposed by Google AI in 2020 and\npublished in this paper under the same name: Language-agnostic BERT\nSentence Embedding. The official\ncode for this paper can be found in the following TensorFlow Hub link:\ntfhub/LaBSE.\n\nMask language modeling (MLM) pre-training task, which was originally\nproposed in the\nBERT model, has\nproven to be a powerful task for numerous NLP tasks. However, it doesn’t\nproduce good sentence-level embeddings unless the model has been\nfine-tuned on sentence-level benchmark. In this paper, the researchers\ndiscuss combining\nmBERT with MLM and\ntranslation language model (TLM) objectives.\n\nLaBSE is a dual-encoder architecture initialized with BERT and\npre-trained on both MLM and TLM objectives. Source and target sentences\nare encoded separately. The similarity between them is scored by the\ncosine similarity. Sentence embeddings are extracted from the last\nhidden state of the encoder [CLS] token, and additive margin softmax\nloss is used for training.\n\n\n    \n\n\nLaBSE is trained using 3-stage progressive stacking\nalgorithm where for an\n$L$ layer transformer encoder, we first learn a $\\frac{L}{4}$ layers\nmodel and then $\\frac{L}{2}$ layers and finally all $L$ layers. The\nparameters of the models learned in the earlier stages are copied to the\nmodels for the subsequent stages.\n\n\n  Note:\nTLM objective was first proposed in the\nXLM model. The only\ndifference here is that TLM doesn’t use language codes to encourage\nmultilinguality.\n\n\nAdditive Margin Softmax\n\nThe loss function used for training the LaBSE model is the additive\nmargin softmax loss function which is described in the following\nformula:\n\n\\[\\mathcal{L} = - \\frac{1}{N}\\sum_{i = 1}^{N}\\frac{e^{\\phi\\left( x_{i},\\ y_{i} \\right) - m}}{e^{\\phi\\left( x_{i},\\ y_{i} \\right) - m} + \\sum_{n = 1,\\ n \\neq i}^{N}e^{\\phi\\left( x_{i},\\ y_{n} \\right)}}\\]\n\nWhere $N$ is the number of sentences in the batch,\n$\\phi\\left( x,\\ y \\right)$ is the embedding similarity of $x$ and $y$\nwhich is set to $\\text{cosine}\\left( x,\\ y \\right)$, and $m$ is the\ndiscount margin. What this loss function tries to achieve is to rank the\ntrue translation $y_{i}$ of the input $x_{i}$ over all other $N - 1$\nother alternatives in the batch even after discounting $m$ value from\nthe similarity.\n\nNotice that this function is asymmetric and depends on whether the\nsoftmax is over the source or the target. In bi-directional ranking, the\nfinal loss function sums the source to target $\\mathcal{L}$, and target\nto source $\\mathcal{L}’$ losses:\n\n\\[\\overline{\\mathcal{L}} = \\mathcal{L} + \\mathcal{L}&#39;\\]\n\nData\n\nRegarding monolingual data, they used the 2019-35 version of\nCommonCrawl after removing lines &amp;lt; 10\ncharacters and those &amp;gt; 5000 characters. Also, they used data from\nWikipedia extracted from the 05-21-2020 dump using\nWikiExtractor. Finally, they\nclassified the monolingual sentences using an in-house quality\nclassifier which filters out any useless data. At the end, they had\naround 17 billion monolingual sentences.\n\nRegarding bilingual data, they mined the web pages using a bitext mining\nsystem similar to the one used in this\npaper. A small\nsubset from the extracted sentence pairs were evaluated by human\nannotators where they marked the pairs as either GOOD or BAD\ntranslations. Then, the extracted sentences were filtered by a\npre-trained contrastive-data-selection (CDS) scoring model similar to\nthe one used in this paper where\nthreshold is chosen such that 80% of the retrained pairs from the manual\nevaluation are rated as GOOD. The final corpus contains 6 billion\ntranslation pairs.\n\nThe distribution of monolingual &amp;amp; bilingual sentences for each language\nis shown in the following figure:\n\n\n    \n\n\nExperiments &amp;amp; Results\n\nIn all of this paper experiments, they employed the wordpiece\nmodel\nwhere a new cased vocabulary is built of $501,153$ subwords from the all\ndata sources. The language smoothing exponent from the vocab generation\ntool is set to $0.3$, as the distribution of data size for each language\nis imbalanced.\n\nThe encoder architecture follows the BERT-Base model which uses 12\nlayers transformer with 12 heads and 768 hidden size. The encoder\nparameters were shared for all languages. Sentence embeddings were taken\nfrom the [CLS] token representation of the last layer, The final\nembeddings were l2 normalized. Each encoder was initialized using a\npre-trained BERT model that was trained using a batch size of $8192$.\nThe max sequence length is set to $512$ and $20\\%$ of tokens (or $80$\ntokens at most) per sequence were masked the MLM and TLM predictions.\n\nLaBSE was trained using the 3-stage progressive stacking\nalgorithm that we talked\nabout earlier where the training steps for each stage were 400k, 800k,\n1.8M steps. It used a batch size of 2048 with max sequence length 64 for\nboth of the source and target. The final models were trained 50K steps\n(less than 1 epoch) using AdamW optimizer with initial learning rate\n$1e^{- 5}$ and linear weight decay.\n\nThe following table shows the [P]recision, [R]ecall and [F]-score\nof BUCC mining task. Following the original previous work, they\nperformed both of the forward search and backward search. Where forward\nsearch treats English as the target and the other language as source,\nbackward is vice versa. As seen from the table, the LaBSE outperforms\nthe previous models in all languages. It is worth to note that the\nprevious state-of-the-art (Yang et al., 2019a) are bilingual models,\nwhile LaBSE covers 109 languages.\n\n\n    \n\n\nThe following table shows precision@1 (P@1) for the experimented\nmodels on the United Nation parallel sentence retrieval task. They\ncompared LaBSE with the current state-of-the-art bilingual models from\nYang et al. (2019a) and public multilingual universal sentence encoder\n(m-USE) model with the transformer architecture. Again, LaBSE shows the\nnew state-of-the-art performance on 3 of the 4 languages:\n\n\n    \n\n\nThe following table shows the macro-average accuracy of different\nlanguage groups of the Tatoeba datasets. LaBSE outperforms all previous\nmodels on all combination of languages.\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Cross-lingual Langluage Model",
        "title"     : "InfoXLM",
        "url"       : "/cross-lingual-lm/InfoXLM",
        "date"      : "15/07/2020",
        "content": "InfoXLM stands for “Information-theoretic procedure for\nCross-Lingual Modeling” which is a cross-lingual language model proposed\nby Microsoft in 2020 and published in their paper: InfoXLM: An\nInformation-Theoretic Framework for Cross-Lingual Language Model\nPre-Training. The official code\nfor this paper can be found in Microsoft’s official UniLM GitHub repository:\nunilm/infoxlm.\n\nState-of-the-art cross-lingual pre-trained models are typically built\nusing monolingual data with masked language modeling (MLM) objective\nsuch as BERT and\nXLM-R; along side\nbilingual data with Translation Language Modeling (TLM) objective such\nas XLM. InfoXLM\ncombines these two objectives with a novel objective called XLCO or\n“Cross Lingual Contrast”.\n\n\\[\\mathcal{L} = \\mathcal{L}_{\\text{MLM}} + \\mathcal{L}_{\\text{TLM}} + \\mathcal{L}_{\\text{XLCO}}\\]\n\n\n  Note to Reader\nI think you should give the XLM post\na read before going on.\n\n\nWe know how to obtain MLM and TLM from XLM model:\n\n\\[\\mathcal{L}_{\\text{MLM}} = - \\log\\frac{\\exp\\left( \\theta_{T}\\left( c_{1} \\right)^{T}\\text{.}\\theta_{E}\\left( x_{1} \\right) \\right)}{\\sum_{x&#39; \\in \\mathcal{V}}^{}{\\exp\\left( \\theta_{T}\\left( c_{1} \\right)^{T}\\text{.}\\theta_{E}\\left( x&#39; \\right) \\right)}}\\]\n\n\\[\\mathcal{L}_{\\text{TLM}} = - \\log\\frac{\\exp\\left( \\theta_{T}\\left( c_{1} \\right)^{T}\\text{.}\\theta_{E}\\left( x_{1} \\right) \\right)}{\\sum_{x&#39; \\in \\mathcal{V}}^{}{\\exp\\left( \\theta_{T}\\left( c_{1} \\right)^{T}\\text{.}\\theta_{E}\\left( x&#39; \\right) \\right)}}\\]\n\n\\[\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  - \\log\\frac{\\exp\\left( \\theta_{T}\\left( c_{2} \\right)^{T}\\text{.}\\theta_{E}\\left( x_{2} \\right) \\right)}{\\sum_{x&#39; \\in \\mathcal{V}}^{}{\\exp\\left( \\theta_{T}\\left( c_{2} \\right)^{T}\\text{.}\\theta_{E}\\left( x&#39; \\right) \\right)}}\\]\n\nWhere:\n\n\n  \n    $x_{1}, x_{2}$ are the masked tokens.\n  \n  \n    $c_{1}, c_{2}$ are the corresponding contexts (the rest).\n  \n  \n    $\\theta_{E}$ is a look-up function that returns the token\nembeddings.\n  \n  \n    $\\theta_{T}$ is a Transformer that returns the final hidden vectors\nin position of $x_{1}$.\n  \n  \n    $\\mathcal{V}$ is the vocabulary.\n  \n\n\nXLCO\n\nXLCO stands for “Cross-lingual Contrast” which is a new objective for\npre-training cross-lingual language models inspired by the unified\ninformation-theoretic framework. The goal of XLCO is to distinguish the\ntranslation of an input sentence from a set of negative examples. The\nformula of this objective is:\n\n\\[\\mathcal{L}_{\\text{XLCO}} = - \\log\\frac{\\exp\\left( \\theta_{Q}\\left( c_{1} \\right)^{T}\\text{.}\\theta_{K}\\left( c_{2} \\right) \\right)}{\\sum_{c&#39; \\in \\mathcal{N}}^{}{\\exp\\left( \\theta_{Q}\\left( c_{1} \\right)^{T}\\text{.}\\theta_{K}\\left( c&#39; \\right) \\right)}}\\]\n\nWhere:\n\n\n  \n    $\\theta_{Q}$ is the query encoder that encodes $c_{1}$ and is\nupdated by back-propagation.\n  \n  \n    $\\theta_{K}$ is the key encoder that encodes $\\mathcal{N}$.\n  \n  \n    $\\mathcal{N}$ is the negative examples distribution which is\norganized as a queue, where a newly encoded example is added while\nthe oldest one is popped from the queue.\n\n    The query encoder and the key encoder are initialized with the same\nparameters, and fill the queue with a set of encoded examples until\nit reaches the desired size $\\left| \\mathcal{N} \\right|$. Notice\nthat the size of the queue remains constant during training. In the\npaper, they used a queue of length equals to $131,072$.\n  \n\n\nMixup Contrast\n\nMixup Contrast is an augmentation method the researcher used when\napplying XLCO task. It goes like this; for each parallel sentence\n$\\left\\langle c_{1},\\ c_{2} \\right\\rangle$, they concatenated it with a\nrandomly sampled translation pair\n$\\left\\langle d_{1},\\ d_{2} \\right\\rangle$ from another parallel corpus.\nThe two pairs are concatenated in a random order like\n$\\left\\langle c_{1}d_{1},\\ c_{2}d_{2} \\right\\rangle$ or\n$\\left\\langle c_{1}d_{2},\\ d_{1}c_{2} \\right\\rangle$. This method\nencourages pre-trained models to learn sentence boundaries and to\ndistinguish the order of multilingual texts.\n\nResults\n\nIn this paper, the researchers followed the model configurations of\nXLM-R when creating InfoXLM. InfoXLM~base~ used the Transformer\narchitecture with 12 layers and 768 hidden . InfoXLM~large~ used the\nTransfoerm architecture with 24 layers and 1,024 hidden states.\n\nThen, they initialized the parameters with XLM-R. The model was\noptimized using Adam optimizer with a batch size of $2048$ for a total\nof $150$K steps for InfoXLM~base~, and $200$K steps for InfoXLM~large~.\nThe learning rate is scheduled with a linear decay with $10$K warmup\nsteps, where the peak learning rate is set as $0.0002$ for\nInfoXLM~base~, and $0.0001$ for InfoXLM~large~. The momentum coefficient\nis set as $0.9999$ for InfoXLM~base~ and $0.999$ for InfoXLM~large~.\nThen, they compared InfoXLM with:\n\n\n  \n    mBERT which was pre-trained with MLM on Wikipedia in 102 languages\n  \n  \n    XLM which was pre-trained with both MLM and TLM tasks on Wikipedia\nin 100 languages\n  \n  \n    XLM-R which was pre-trained with MLM to the large CC-100 corpus in\n100 languages with much more training steps.\n  \n  \n    UNICODER which was initialized with XLM-R and they trained it using\nboth MLM and TLM.\n  \n  \n    InfoXLM (without XLCO).\n  \n\n\nAnd the following table shows this comparison on XNLI using 15\ndifferent languages. The model number #M=N indicates that each\nlanguage had a different model) while #M=1 means only one model is\nused for all languages. Also, results with “*” are taken from this\npaper while “(reimpl)” means\nthat the researchers have re-implemented it. Results of InfoXLM and\nXLM-R (reimpl) are averaged over five runs with different seeds:\n\n\n    \n\n\nThe past results show that InfoXLM outperforms all baseline models on\nthe two evaluation settings of XNLI. Moreover, removing XLCO object\nhurts the performance which shows that cross-lingual contrast is helpful\nfor zero-shot transfer in most languages.\n"
      },
    
      
      
      {
        "collection": "Cross-lingual Langluage Model",
        "title"     : "AMBER",
        "url"       : "/cross-lingual-lm/AMBER",
        "date"      : "15/10/2020",
        "content": "AMBER stands for “Aligned Multilingual Bidirectional\nEncodeR” is a cross-lingual language model that adopts the same\narchitecture as BERT; where the contextual embeddings of words/sentences\nwith similar meanings across languages are aligned together in the same\nspace. AMBER was proposed by Google Research in collaboration with\nCarnegie Mellon University in 2020 and published in their paper:\nExplicit Alignment Objectives for Multilingual Bidirectional\nEncoders. The official code for\nthis paper can be found in this GitHub repository:\namber.\n\nCross-lingual Alignment\n\nTo produce language-independent representations, AMBER was trained on\nmonolingual and parallel data using three alignment objectives that\nalign the multilingual word/sentence representations together. These\nthree alignment objectives are:\n\n\n  MLM or TLM:\nThis objective, proposed in\nBERT, takes a\npair of sentences $\\left( x,y \\right)$, and optimizes the prediction\nof randomly masked tokens in the concatenation of the sentence pair\n$\\left\\lbrack x;y \\right\\rbrack$. When $x$ and $y$ are in the same\nlanguage, it’s Masked Language Modeling (MLM). When they are in two\ndifferent languages, it’s Translation Language Modeling (TLM). This\ncan be described as follows where\n$\\left\\lbrack x;y \\right\\rbrack_{s}$ is the masked tokens of the\nconcatenation while $\\left\\lbrack x;y \\right\\rbrack_{\\backslash s}$\nis the unmasked tokens:\n\n\n\\[\\mathcal{L}_{\\text{MLM}}\\left( x,y \\right) = - \\mathbb{E}_{s\\sim\\left\\lbrack 1,\\left| \\left\\lbrack x;y \\right\\rbrack \\right| \\right\\rbrack}\\log\\text{ P}\\left( \\left\\lbrack x;y \\right\\rbrack_{s} \\middle| \\left\\lbrack x;y \\right\\rbrack_{\\backslash s} \\right)\\]\n\n\n  Sentence Alignment:\nThis objective encourages cross-lingual alignment of sentence\nrepresentations. Given sentence pair $\\left( x,y \\right)$, we\nseparately calculate sentence embeddings\n$\\left( c_{x},\\ c_{y} \\right)$ where the sentence embeddings is\ncalculated by averaging the embeddings in the final layer. Then the\nmodel tries to minimize the following loss function where $y’$ is\nany sentence in the mini-batch $\\mathcal{B}$:\n\n\n\n    \n\n\n\\[\\mathcal{L}_{\\text{SA}}\\left( x,y \\right) = - \\log\\ \\frac{e^{c_{x}^{\\intercal}c_{y}}}{\\sum_{y&#39; \\in \\mathcal{B}}^{}e^{c_{x}^{\\intercal}c_{y&#39;}}}\\]\n\n\n  Bidirectional Word Alignment:\nThis objective encourages bidirectional alignment of word embeddings\nby leveraging the attention mechanism in the\nTransformer\nmodel by minimizing the distance between the trace of the\nsource-to-target attention $A_{x \\rightarrow y}$ and\ntarget-to-source attention $A_{y \\rightarrow x}$ matrices. Since the\nTransformer has multiple attention heads, we average the trace of\nthe bidirectional attention matrices generated by all the heads as\nshown in the following formula:\n\n\n\n    \n\n\n\\[\\mathcal{L}_{\\text{WA}}\\left( x,y \\right) = 1 - \\frac{1}{H}\\sum_{h = 1}^{H}\\frac{\\text{tr}\\left( \\left( A_{x \\rightarrow y}^{h} \\right)^{\\intercal}\\left( A_{y \\rightarrow x}^{h} \\right) \\right)}{\\min\\left( \\left| x \\right|,\\left| y \\right| \\right)}\\]\n\nThey combined all three objectives to obtain the following total loss\nfunction:\n\n\\[\\mathcal{L}\\left( x,y \\right) = \\mathcal{L}_{\\text{MLM}}\\left( x,y \\right) + \\mathcal{L}_{\\text{SA}}\\left( x,y \\right) + \\mathcal{L}_{\\text{WA}}\\left( x,y \\right)\\]\n\n\n  Notes:\n\n  \n    AMBER with just MLM objective is the same as\n  mBERT.\n    Sentence Alignment objective is the same as\n  Additive Margin Softmax (AMS)\n  where the margin is zero $(m=0)$.\n  \n\n\nExperiments\n\nAMBER was pre-trained using MLM on the Wikipedia data for 1M steps first\nusing the default hyper-parameters as mBERT found\nhere except that they used a\nlarger batch of 8,192 sentence pairs. Then, they pre-training it using\nthe other two objectives for another 1M steps with a batch of 2,048\nsentence pairs from Wikipedia corpus and parallel corpus used to train\nXLM. As show in the\nfollowing table, shows the size of AMBER compared to other cross-lingual\nmodels:\n\n\n    \n\n\nAfter pre-training, they fine-tuned AMBER on English annotations and\napplied the model to predict on non-English data on the following tasks:\n\n\n  POS: Cross-lingual Part-Of-Speech (POS) benchmark which contains\ndata in 13 languages. The following table shows that AMBER achieves\nsimilar results to XLM-R despite being half its size:\n\n\n\n    \n\n\n\n  PAWS-X: is a paraphrase detection dataset in five different\nlanguages. The following table shows that AMBER achieving on-par\nresults with XLM-R large despite the fact that AMBER is one-fifth of\nits size:\n\n\n\n    \n\n\n\n  XNLI: is a natural language inference dataset in 15 languages.\nThe following table shows that AMBER is not as good as XLM-R on this\ndataset, but achieving better results than XLM-100.\n\n\n\n    \n\n\n\n  Tatoeba: is a sentence retrieval dataset in 14 different\nlanguages; where they try to find the English translation for a\ngiven a non-English sentence using maximum cosine similarity. The\nfollowing table shows AMBER achieving the best results on this\nbenchmark:\n\n\n\n    \n\n\nThe following table summarizes the average results over all\nlanguages for the past four benchmarks:\n\n\n    \n\n\n\n  Note:\nIn all of the previous dataset, AMBER (MLM) is achieving better results than\nmBERT despite having the same architecture because AMBER uses bigger batch\nsizes which is proven to be efficient as explain in\nRoBERTa paper.\n\n"
      },
    
      
      
      {
        "collection": "Cross-lingual Langluage Model",
        "title"     : "mT5: Multilingual T5",
        "url"       : "/cross-lingual-lm/mT5",
        "date"      : "22/10/2020",
        "content": "mT5 stands for “Multilingual Text-to-Text Transfer Transformer” which is\na multilingual variant of\nT5 trained on 101\nlanguages. mT5 was proposed by Google Research in 2020 and published in\ntheir paper: mT5: A Massively Multilingual Pre-trained Text-to-Text\nTransformer. The official code\ncan be found on Google Research’s official GitHub repository:\ngoogle-research/multilingual-t5.\n\nThe goal behind this paper and mT5 is to produce a massively\nmultilingual model that deviates as little as possible from the recipe\nused to create T5. So, they based mT5 on the\n“T5.1.1”\ncheckpoint which improves upon T5 architecture by using:\n\n\n  \n    GeGLU non-linearities instead of ReLU.\n  \n  \n    Scaling both $d_{\\text{model}}$ and $d_{\\text{ff}}$ instead of just\n$d_{\\text{ff}}$ in the larger models.\n  \n  \n    No dropout with pre-training.\n\n    To put mT5 into perspective, the following table provides a brief\ncomparison with existing cross-lingual language models such as:\nmBERT, XLM,\nXLM-R, and\nMARGE:\n  \n\n\n\n    \n\n\n\n  Note to reader:\nBefore getting deeper into this post, I suggest reading the T5\npost first. You\ndon’t have to read all of it, just read the\nC4 part and the\nbaseline part.\n\n\nmC4\n\nTo train mT5 on that many languages, they introduced a multilingual\nvariant of the C4\ndataset called mC4. mC4 comprises natural text in 101 languages drawn\nfrom the public Common Crawl web scrape.\n\nThe C4 (Colossal Clean Crawled Corpus) dataset was explicitly designed\nto be English only. In contrast, for mC4 they used cld3\ntool to identify over 100 languages.\nSince some of these languages are relatively scarce on the internet,\nthey made use of all of the 71 monthly web scrapes released so far by\nCommon Crawl.\n\nAn important heuristic filtering step in C4 was the removal of lines\nthat did not end in an English terminal punctuation mark. Since many\nlanguages do not use English terminal punctuation marks, they instead\napplied a “line length filter” that requires pages to contain at least\nthree lines of text with 200 or more characters. Other than that, they\nfollowed the C4’s filtering steps which are:\n\n\n  \n    They discarded any page with fewer than 5 sentences and only\nretained lines that contained at least 3 words.\n  \n  \n    They removed any page that contained any word on the “List of\nDirty, Naughty, Obscene or Otherwise Bad\nWords”.\n  \n  \n    They removed any line with the word JavaScript since it usually\nindicates an error on the web.\n  \n  \n    Some pages had placeholder “lorem ipsum” text; they removed any page\nwhere the phrase “lorem ipsum” appeared.\n  \n  \n    Some pages inadvertently contained code. Since the curly bracket “{“\nappears in many programming languages (such as Javascript, widely\nused on the web) but not in natural text, they removed any pages\nthat contained a curly bracket.\n  \n  \n    To de-duplicate the dataset, they discarded all but one of any\nthree-sentence span occurring more than once in the data set.\n  \n  \n    They detected each page’s primary language using cld3\ntool and removed those with a\nconfidence below 70%.\n\n    After these filters are applied, they grouped the remaining pages by\nlanguage and included in the corpus all languages with 10,000 or\nmore pages. This produces text in 101 language as shown in the\nfollowing figure which shows a histogram of the page counts for each\nlanguage:\n  \n\n\n\n    \n\n\n\n  Note:\nThe previous histogram shows 107 languages as detected by cld3; and\nthat’s because it handles the script variants of the same spoken\nlanguage. For example, ru is Russian in Cyrillic script and\nru-Latn is Russian in Latin script. Same applies for Japanese\n(ja), Hindi (hi), Greek (el), Chinese (zh), and Bulgarian (bg).\n\n\nPre-Training Details\n\nFollowing the original T5 recipe, they considered five model sizes in\ntheir experiments: mT5-Small (≈ 300M parameters),\nmT5-Base (580M), mT5-Large (1.2B),\nmT5-XL (3.7B), and mT5-XXL (13B).\nThe increase in parameter counts compared to the corresponding T5 model\nvariants comes from the larger vocabulary used in mT5.\n\n\n  Note:\nBecause mT5 is an encoder-decoder model, it has roughly twice as many\nparameters as correspondingly-sized encoder-only models such as XLM-R.\nFor example, the “XLM-R~Large~” has 550 million parameters whereas\nmT5-Large has around 1 billion.\n\n\nAll mT5 variants were pre-trained for 1 million steps on batches of 1024\ninput sequences, corresponding to roughly 1 trillion input tokens total.\nThis is the same amount of pre-training as T5 and about 1/6 as much as\nXLM-R. They used the same inverse square-root learning rate schedule\nused by T5 during pre-training, with the learning rate set to\n$\\frac{1}{\\sqrt{\\max\\left( n,k \\right)}}$ where $n$ is the current\ntraining iteration and $k = 10^{4}$ is the number of warm-up steps.\n\nFollowing the\nT5.1.1\nrecipe, they didn’t apply dropout during pre-training. They used the\nsame self-supervised objective as T5, with $15\\%$ of tokens masked and\nan average noise span length of $3$.\n\nSampling\n\nA major factor in pre-training multilingual models is how to sample data\nfrom each language. In this paper, they used temperature sampling to\nboost lower-resource languages by sampling examples according to the\nprobability $p\\left( L \\right) \\propto \\left| L \\right|^{\\alpha}$, where\n$p\\left( L \\right)$ is the probability of sampling text from a given\nlanguage $L$ during pre-training and $\\left| L \\right|$ is the number of\nexamples in the language. The hyper-parameter $\\alpha$ (typically with\n$\\alpha &amp;lt; 1$) controls how much to “boost” the probability of training\non low-resource languages.\n\nValues used by prior work include $\\alpha = 0.7$ for mBERT,\n$\\alpha = 0.3$ for XLM-R, and $\\alpha = 0.2$ for M4. They tried all\nthree of these values and found $\\alpha = 0.3$ to give a reasonable\ncompromise between performance on high- and low-resource languages.\n\nVocabulary\n\nThe fact that mT5 model covers over 100 languages necessitates a larger\nvocabulary. Following\nXLM-R, they\nincreased the vocabulary size to 250,000 wordpieces. As in T5, they used\nSentencePiece model trained with the language sampling rates used during\npre-training. To accommodate languages with large character sets like\nChinese, they used a character coverage of $0.99999$ and enable\nSentencePiece’s “byte-fallback” feature to ensure that any string can be\nuniquely encoded.\n\nFine-tuning\n\nTo validate the performance of mT5, they evaluated it on 6 tasks from\nthe XTREME multilingual benchmark:\n\n\n  \n    The XNLI entailment task covering 14 languages.\n  \n  \n    The XQuAD reading comprehension covering 10 languages.\n  \n  \n    The MLQA reading comprehension covering 7 languages.\n  \n  \n    The TyDi QA reading comprehension covering11 languages.\n  \n  \n    The Named Entity Recognition (NER) dataset of WikiAnn covering 40\nlanguages.\n  \n  \n    The PAWS-X paraphrase identification dataset covering 7 languages.\n  \n\n\nAll tasks are casted into the text-to-text format. For example, if there are\nmultiple NER entities, then they are concatenated in the order they appear, and\nif there are no entities then the target text is “None”.\n\nFor fine-tuning, they used a constant learning rate of $0.001$ and dropout rate\nof $0.1$ for all tasks. They used a batch size of $2^{17}$ for most tasks but\nincreased this up to $2^{20}$ in a few cases based on performance on the\nvalidation set. For early stopping, they saved checkpoints every $200$ steps\nand choose the checkpoint with the highest validation performance.\n\nRegarding fine-tuning, they considered three variants:\n\n\n  Zero-shot: where the model is fine-tuned only on English\ndata. The following table shows that mT5-XXL achieves state of the\nart results\n\n\n\n    \n\n\n\n  translate-train: adding machine translations from English into\neach target language.\n\n\n\n    \n\n\n\n  in-language multi-task: training on gold data in all target\nlanguages:\n\n\n\n    \n\n\nFrom the past results we can see that the model capacity is key to improving\nperformance on variants of the TyDi QA GoldP task in the absence of “gold”\nmultilingual data: For the smallest model, training on gold datasets\n(in-language multitask) achieves dramatically better performance than using\nweakly supervised data (translate-train) or English-only data (zero-shot),\nwhereas the gap between these three settings is much smaller for the largest\nmodel. The following figure points that out clearly as it shows the average\nF1 on the TyDi QA GoldP task across languages:\n\n\n    \n\n\nAccidental Translation\n\nAccidental translation is a problematic behavior with the mT5 model in\nthe zero-shot setting where the model start translating part or all of\nthe masked span into English (the language of all fine-tuning data). On\nthe one hand, it is remarkable that mT5 performs “spontaneous”\ntranslation despite never seeing parallel training data. On the other,\nit led to hurting the performance. This problem happens across all model\nsizes and all XQuAD languages, but happens more in mT5-Small and\nmT5-Base.\n\nThe researchers believe that the reason behind that is that the model\nhas never observed a non-English target during fine-tuning. To fix that,\nthey decided to use a technique called “Domain Preserving Training\n(DPT)” which is to mix a unsupervised task with fine-tuning.\n\nThe unsupervised task they planned to use was the same as the mC4 task\ndefinition as in pre-training, with just two adjustments:\n\n\n  \n    First, they removed all “sentinel” tokens from the target sequence.\n  \n  \n    Second, they reduced the language sampling parameter $\\alpha$ from\n$0.3$ to $0.1$. This produces a near uniform distribution of\nlanguages, encouraging the model to treat all languages as equally\nlikely.\n  \n\n\nThen, they mixed a small amount of the unsupervised task (covering 101\nlanguages) into XQuAD fine-tuning, at a ratio of just $1:100$. The following\nfigure shows the results on XQuAD zero-shot error rates. The addition of this\nsmall amount of multilingual data has a marked effect on the mT5- Small and\nmT5-Base models reducing the illegal prediction rates by more than $70%$\n(relative), and contributing to an overall reduction in errors:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Cross-lingual Langluage Model",
        "title"     : "VECO: Cross-lingual Pre-training",
        "url"       : "/cross-lingual-lm/VECO",
        "date"      : "30/10/2020",
        "content": "VECO stands for “Variable and Flexible Cross-lingual\nPre-training” which is a pre-training approach for cross-lingual\nlanguage models that uses “Cross-Attention Masked Language Modeling”\n(CA-MLM) objective. VECO was proposed by Alibaba Group in 2020 and\npublished in their “VECO: Variable and Flexible Cross-lingual\nPre-training for Language Understanding and\nGeneration” paper. The official\ncode for this paper can be found on Alibaba’s official GitHub\nrepository:\nalibaba/VECO.\n\nCA-MLM Pre-training\n\nThe special thing about VECO pre-training is that it plugs a\ncross-attention module into the\nTransformer\narchitecture to explicitly build the cross-relation between languages\nwhen pre-training using the Cross-Attention Masked Language Modeling MLM\n(CA-MLM) objective. The following figure shows the difference between\nMLM pre-training without plugging cross-attention module (left) and with\nplugging the cross-attention module (right):\n\n\n    \n\n\n\n  Note:\nMLM here is different than MLM objective proposed in the\nBERT paper. As we\ncan see, here MLM is applied on both encoder and decoder.\n\n\nGiven a pair of input $\\left( x,\\ y \\right)$ and its MLM corrupted\nversion $\\left( \\widehat{x},\\ \\widehat{y} \\right)$, the model tries to\nminimize the following loss:\n\n\\[\\mathcal{L}\\left( x,\\ y \\right) = - \\log P\\left( x \\middle| \\widehat{x};\\ \\theta_{s} \\right) - \\log P\\left( x \\middle| \\widehat{y},\\ \\widehat{x};\\ \\theta_{s},\\ \\theta_{c} \\right)\\]\n\n\\[\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  - \\log P\\left( y \\middle| \\widehat{y};\\ \\theta_{s} \\right) - \\log P\\left( y \\middle| \\widehat{x},\\ \\widehat{y};\\ \\theta_{s},\\ \\theta_{c} \\right)\\]\n\nWhere $\\theta_{s}$ and $\\theta_{c}$ are the parameters of self-attention\nand cross-attention modules respectively.\n\n\n  Note:\nIn case of monolingual data, CA-MLM uses two adjacent sentences as the\n$\\left( x,\\ y \\right)$ pair.\n\n\nCross-Attention Module\n\nAs said before, the VECO pre-training extends the\nTransformer\narchitecture and plugs a cross-attention module in each layer. Now, each\nlayer has three modules: self-attention module, a plug-and-play\ncross-attention module, and a feed-forward linear module. Both\nself-attention and cross-attention modules are based on the multi-head\nattention\nmechanism.\n\nAn attention function can be described as mapping a query $Q$ and a set\nof key-value $K,V$ pairs to an output. For the self-attention module,\nall the queries, keys and values are the same representations from the\nprevious layer. Specifically, for the $l^{\\text{th}}$ layer, the output\nof a self-attention head $A_{l}^{s}$ is computed via:\n\n\\[Q = H^{l - 1}W_{l}^{Q},\\ \\ \\ K = H^{l - 1}W_{l}^{K},\\ \\ \\ V = H^{l - 1}W_{l}^{V}\\]\n\n\\[A_{l}^{s} = \\text{softmax}\\left( \\frac{QK^{T}}{\\sqrt{d_{k}}} \\right)V\\]\n\nWhere $H^{l - 1}$ is the output of the previous layer, and\n$W_{i}^{Q},W_{i}^{K},W_{i}^{V}$ are learned parameter matrices of the\nself-attention.\n\nFor the cross-attention module, the queries come from the previous\nlayer, and the keys and values come from the last layer’s\nrepresentations of paired input. Specifically, for the $l^{\\text{th}}$\nlayer, the output of a cross-attention head $A_{l}^{c}$ is computed via:\n\n\\[Q = S^{l - 1}U_{l}^{Q},\\ \\ \\ K = H^{L}U_{l}^{K},\\ \\ \\ V = H^{L}U_{l}^{V}\\]\n\n\\[A_{l}^{c} = \\text{softmax}\\left( \\frac{QK^{T}}{\\sqrt{d_{k}}} \\right)V\\]\n\nWhere $S^{l - 1}$ is the output of the previous layer,\n$U_{i}^{Q},U_{i}^{K},\\ U_{i}^{V}$ are learned parameter matrices of the\ncross-attention, and finally $H^{L}$ is the output of the last layer.\n\n\n  Note:\nThere are two types of contextualized vector representation:\n\n  \n    $H$: Which depends on the masked sequence:\n  \n\n\\[P\\left( x \\middle| \\widehat{x} \\right) = \\text{softmax}\\left( f\\left( H_{x}^{L} \\right) \\right),\\ \\ \\ \\ \\ P\\left( y \\middle| \\widehat{y} \\right) = \\text{softmax}\\left( f\\left( H_{y}^{L} \\right) \\right)\\]\n\n  \n    $S$: Which depends on the masked paired sequence:\n  \n\n\\[P\\left( x \\middle| \\widehat{x},\\ \\widehat{y} \\right) = \\text{softmax}\\left( f\\left( S_{x}^{L} \\right) \\right),\\ \\ \\ \\ \\ P\\left( y \\middle| \\widehat{x},\\ \\widehat{y} \\right) = \\text{softmax}\\left( f\\left( S_{y}^{L} \\right) \\right)\\]\n\n\nFurthermore, this cross-attention module can be plugged-in or\nplugged-out on-demand which makes it very suitable for both\ncross-lingual language understanding tasks and generation tasks as we\nare going to see next.\n\nRegrading cross-lingual understanding tasks on the XTREME benchmark,\nVECO ranks first at the\nleaderboard at the submission\ndeadline. Regrading cross-lingual generation tasks such as machine\ntranslation, VECO outperforms existing state-of-the-art models by 1∼2\nBLEU score.\n\nFine-tuning\n\nAs illustrated in the following figure, VECO is very flexible when\nfine-tuning on various downstream tasks due to the plug-and-play\ncross-attention module. It can be used for initializing the encoder-only\nTransformer for natural language understanding (NLU) tasks and\nencoder-decoder Transformer for natural language generation (NLG) tasks.\n\n\n    \n\n\nDuring NLU fine-tuning, there are two approaches that we can choose\nfrom:\n\n\n  \n    Plug-out fine-tuning: is to unplug the cross-attention module\nfrom the pre-trained model which makes the model very similar to\nmBERT.\n  \n  \n    Plug-in fine-tuning: is to plug the cross-attention module into\nthe fine-tuned model. This can be used if the other language y is\navailable in the downstream task. In this approach, the two\nrepresentations $\\left\\lbrack H_{x}^{L};S_{x}^{L} \\right\\rbrack$ are\nconcatenated to predict the label of $x$ and\n$\\left\\lbrack H_{y}^{L};S_{y}^{L} \\right\\rbrack$ are concatenated to\npredict the label of $y$.\n  \n\n\nExperiments &amp;amp; Results\n\nThey collected monolingual and bilingual corpus covering 50 languages.\nFor monolingual training datasets, they extracted 1.36TB data in 50\nlanguages from the\nCommonCrawl Corpus, which\ncontains 6.5G sentences and 0.4G documents. The following table has\nstatistics for all 50 languages monolingual data where the values are in\nmillion:\n\n\n    \n\n\nThey up/down-sample the monolingual text like\nXLM from each\nlanguage with a smoothing parameter $\\alpha = 0.5$ where $D_{l}$ is the\nnumber of sentences in language $l$.\n\n\\[p_{l} = \\left( \\frac{D_{l}}{\\sum_{k}^{}D_{k}} \\right)^{\\alpha}\\]\n\nFor bilingual data, they extracted 6.4G parallel sentences across 50\nlanguages from the OPUS website.\n\nIn this paper, they used pre-trained two VECO models following the\nXLM-R model. The\nlarge one is a 24-layer model with 1024 embedding/hidden size and 4096\nfeed-forward size, while the small one is 6 layers with 768\nembedding/hidden size and 3072 feed-forward size. Also, they used the\nsame 250K vocabulary that was used by\nXLM-R . The full\nlist of the model can be seen in the following table:\n\n\n    \n\n\nThe following table shows a comparison between VECO and other baselines:\n\n\n    \n\n\nWhen fine-tuning on natural language understanding tasks, VECO\noutperforms previous cross-lingual models on all datasets of the XTREME\nbenchmark:\n\n\n    \n\n\nWhen fine-tuning on natural language generation tasks (machine\ntranslation), VECO outperforms the randomly initialized same-sized\nTransformer baseline by 2.3 BLEU points. Moreover, it even beats the\n(randomly initialized) state-of-the-art Deep-Transformer, which is three\ntimes deep as VECO.\n\n\n    \n\n\nAnd to investigate where the improvement in VECO comes from, they\ntrained XLM,\nmBART and VECO model\nfrom scratch using the same datasets and parameter settings where all of\nthem were pre-trained on MLM and TLM tasks and fine-tuned on XNLI\ndownstream task and machine translation De-En pair from the IWSLT14\ndataset. The results are shown in the following table:\n\n\n    \n\n\nThe previous table shows that:\n\n\n  \n    When pre-training on monolingual data only, VECO outperforms XLM by\n0.8 points on the XNLI dataset and 0.3 BLEU scores on the IWSLT\ndataset which suggests that CA-MLM can still benefit from\nadjacent sentences in monolingual corpus. Remember that\nCA-MLM uses adjacent sentences as the $\\left( x,y \\right)$ pair.\n  \n  \n    When pre-training on both monolingual and bilingual data, VECO\nachieved a larger improvement compared to XLM, with 3.2 and 2.1\npoints improvement on the two datasets, respectively. It\nreveals that CA-MLM objective of VECO can better utilize the bilingual\ncorpus, compared to only optimized by TLM and MLM of XLM.\n  \n\n"
      },
    
      
      
      {
        "collection": "Cross-lingual Langluage Model",
        "title"     : "mT6: Multilingual T5 with Translation Pairs",
        "url"       : "/cross-lingual-lm/mT6",
        "date"      : "18/04/2021",
        "content": "mT6 stands for “Multilingual Text-to-Text Transfer Transformer with\nTranslation pairs” which is an attempt to improve the performance of the\nmT5 model by\nincorporating translation objectives into the pre-training part. This\nmodel was proposed by Microsoft Research in 2021 and published in this\npaper: mT6: Multilingual Pretrained Text-to-Text Transformer with\nTranslation Pairs.\n\nA little bit of background: the mT5 model was pre-trained on mC4 dataset\n(a multilingual version of the\nC4 corpus) with a\nmasked language modeling “span-corruption” objective, where the encoder\nis fed a chunk of text with random spans replaced with a mask token, and\nthe decoder must reconstruct the masked-out tokens. MT6 differs from MT5\nin terms of both pre-training tasks and the training objective. We are\ngoing to talk about that next.\n\nPre-training Tasks\n\nOne of the pre-training tasks used for T5 model and consequently mT5 was\nSpan Corruption which first randomly masks several spans of the input\nsentence then the output is the concatenation of the original tokens of\nthe masked spans, each of which starts with a unique mask token to\nindicate the span to be decoded as shown in the following figure:\n\n\n    \n\n\nIn the paper, they presented three more text-to-text pre-training tasks\nfor improving mT5 with translation data. These pre-training tasks are:\n\n\n  Machine Translation (MT):\nThis is a typical text-to-text task with the goal of translating a\nsentence from the source language into a target language.\n\n\n\n    \n\n\n\n  Translation Pair Span Corruption (TPSC):\nInspired by the\nMASS objective,\nthis task aims to predict the masked spans from a translation pair\ninstead of a monolingual sentence.\n\n\n\n    \n\n\n\n  Translation Span Corruption (TSC):\nA potential issue of TPSC is that the spans in the target sequence\ncan be organized in unnatural word order. As shown in Figure 2, the\noutput sequence of TPSC is organized as ”[M1] for your [M2]\nlast week [M3] invitation [M4]“. It can be found that the\nFrench word “invitation” is after the English word “week”, which\ncould harm the language model of the decoder. This motivated them to\npropose the translation span corruption (TSC) task where they only\nmasked and predict the spans in one language.\n\n\n\n    \n\n\nRegarding the data, they used natural sentences from CCNet in 94 languages for\nmonolingual text-to-text tasks and parallel corpora of 14 English-centric\nlanguage pairs, collected from MultiUN, IIT Bombay, OPUS, and WikiMatrix.\n\nPNAT Objective\n\nRecall that the backbone architecture of mT5 is the simple\nencoder-decoder Transformer which is trained to predict the target text\nconditioned on the input source text in auto-regressive manner. Let $x$\nand $y$ denote the input sequence and the output sequence respectively,\nthe loss function of mT5 will be:\n\n\\[\\mathcal{L} = - \\sum_{i = 1}^{\\left| y \\right|}{\\log\\left( p\\left( y_{i} \\middle| x,\\ y_{&amp;lt; i} \\right) \\right)}\\]\n\nTo encourage mT6 to utilize more information from the encoding side\nwhile preserving the ability of auto-regressive decoding, they proposed\na new training objective for text-to-text training, called partially\nnon-auto-regressive decoding (PNAT). Let’s see how PNAT works.\n\nGiven an input sequence containing $m$ spans, they divided it into\ngroups $n_{g}$ groups and trained the model to decode each group\nseparately where the prediction is only conditioned on the tokens from\nthe same group as shown in the following figure:\n\n\n    \n\n\nFor the $j^{\\text{th}}$ group, $l_{j}$ and $r_{j}$ are the start\nposition and the end position respectively. The PNAT objective is\ndefined as:\n\n\\[\\mathcal{L}^{\\text{PNAT}} = - \\sum_{j = 1}^{n_{g}}{\\sum_{i = l_{j}}^{r_{j}}{\\log\\left( p\\left( y_{i} \\middle| x,\\ y_{l_{j}}\\text{\\ ...\\ }y_{i - 1} \\right) \\right)}}\\]\n\nThe mT6 model is jointly pre-trained on both monolingual and parallel\ncorpora using the original Span Corruption objective along with one of\nthe three different pre-training tasks proposed in this paper. So, the\noverall pre-training objective is:\n\n\\[\\mathcal{L}_{mT6} = \\mathcal{L}_{\\text{SC}}^{\\text{PNAT}} + \\mathcal{L}_{X}^{\\text{PNAT}},\\ \\ \\ \\ x \\in \\left\\{ MT,\\ TPSC,\\ TSC \\right\\}\\]\n\nResults\n\nIn all of the experiments, they considered the mT5-small model, with\n$d_{\\text{model}} = 512$, $d_{\\text{ff}} = 1024$, $6$ attention heads,\nand 8 layers for both the encoder and the decoder. They used the\nvocabulary provided by XLM-R, and extended it with 100 unique mask\ntokens for the span corruption tasks. They pre-trained mT6 for $0.5M$\nsteps with batches of $256$ length-$512$ input sequences. The model was\noptimized by the Adam optimizer with a linear learning rate scheduler.\nAll hyper-parameters needed for pre-training and fine-tuning this model\nare described below:\n\n\n    \n\n\nAnd the following are the results on fine-tuning benchmarks:\n\n\n  XTREME: All results are averaged over five runs.\n\n\n\n    \n\n\n  Gigaword multilingual Abstractive summarization: RG is short for\nROUGE. mT5 &amp;amp; mT6 results are averaged over three runs:\n\n\n\n    \n\n\n\n  Wikilingua cross-lingual summarization: All results are ROUGE-2\nscores and averaged over three runs:\n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Cross-lingual Langluage Model",
        "title"     : "∆LM: Delta Language Modeling",
        "url"       : "/cross-lingual-lm/DeltaLM",
        "date"      : "25/06/2021",
        "content": "DeltaLM (∆LM) is a pre-trained multilingual encoder-decoder model whose\nencoder and decoder are initialized with a pre-trained multilingual\nencoder, and trained in a self-supervised way. DeltaLM was proposed by\nMicrosoft in 2021 and published in their paper: “∆LM: Encoder-Decoder\nPre-training for Language Generation and Translation by Augmenting\nPretrained Multilingual\nEncoders”. The official code for\nthis paper can be found on Microsoft’s GitHub repository:\nmicrosoft/deltalm.\n\n\n    \n\n\nIn the paper, they decided to use\nInfoXLM as the\npre-trained Multilingual encoder to take advantage of its strong\nperformance. InfoXLM uses the large-scale monolingual data and bilingual\ndata and is jointly trained with a combination of the masked language\nmodel (MLM), translation language model (TLM), and cross-lingual\ncontrast (XLCO) objectives. It has a shared vocabulary of 250,000 tokens\nbased on the SentencePiece model.\n\nOne challenge is how to initialize the decoder since the architecture of\nthe decoder is different from that of the encoder. To overcome this\nproblem, they introduced a new architecture for the decoder and called\nit “interleaved decoder”. This new architect has a more consistent\nstructure with the encoder. In this way, the decoder can fully leverage\nall weights of the pre-trained encoder.\n\nInterleaved Decoder\n\nAs shown at the middle of the following figure, the standard\nTransformer\ndecoder consists of three modules: self-attention, cross-attention, and\nfeed-forward network (FFN). In\nXLM model, they\ninitialized the self-attention and the FFN with the weights of the\npre-trained encoder, while the cross-attention is initialized with\neither random weights or the same weights as the self-attention.\nHowever, to better leverage the full weights of the pre-trained encoder,\nthey proposed a new-yet-similar architecture as seen on the right side\nof the following figure:\n\n\n    \n\n\nIn the interleaved decoder, they interleaved the FFNs and the attention\nmodules, so that the structure is consistent with the pre-trained\nencoder at the first part and similar to the decoder at the second part.\nThe residual connections and the layer normalizations were performed in\neach sub-layers in the same way as vanilla Transformer layers.\n\nNow, with the interleaved structure, the decoder can be directly\ninitialized with the pre-trained encoder. More specifically, the\nself-attentions and the bottom FFNs can be initialized using the odd\nlayers of the InfoXLM pre-trained model, while the cross-attentions and\nthe top FFNs can be initialized with the corresponding even layers.\n\nPre-training\n\n∆LM was pre-trained using 6TB multilingual data, which is a combination\nof CC100, CC-Net, and Wikipedia, covering 100 languages. Also, it was\npre-trained using 88GB of bilingual data from CCAligned and OPUS, which\nhas 77 languages. DeltaLM was pre-trained on two pre-training tasks:\n\n\n  Span Corruption:\nAs shown in the following figure, span corruption is the task of\nreconstructing the text spans based on the masked input sentence. It\nis proven to be effective for pre-training an encoder-decoder model.\nIn this work, they followed\nmT5 model to\napply this pre-training task to pre-train ∆LM on large-scale\nmonolingual corpora.\n\n\n\n    \n\n\n\n  Translation Span Corruption:\nAs shown in the following figure, two parallel sentences are\nconcatenated together, then masked. And the model will have to\nfigure out the text spans based on the input masked translation\npair. In this work, they followed\nmT6 model to\nleverage large-scale bilingual corpora\n\n\n\n    \n\n\nDeltaLM was pre-trained the model for $600,000$ steps with $2,048$ samples per\nbatch and the input length was $512$ tokens. For the span corruption task, the\nprobability of corrupted tokens is $15\\%$ and the average length of spans is\n$3$. For the translation span corruption, the probability of corrupted tokens\nis $50\\%$ and the span length is $3$.\n\nResults\n\nIn this paper, they were considering the base-size Transformer model,\nwith $768$ hidden size, $3,072$ FFN dimension, $12$ attention heads, and\n12 encoder/decoder layers. As said before, DeltaLM was initialized using\nInfoXLM-BASE. They used the Adam optimizer with\n$\\beta_{1} = 0.9,\\ \\beta_{2} = 0.999$ with a linear learning rate\nscheduler with $10,000$ warm-up steps. A gradient clipping of $1.0$ was\nused.\n\n\n  Machine Translation: They evaluated the models on the\nlarge-scale WMT-10 benchmark dataset which is a collection of 32.5\nmillion parallel data in English (En), French (Fr), Czech (Cs),\nGerman (De), Finnish (Fi), Latvian (Lv), Estonian (Et), Romanian\n(Ro), Hindi (Hi), Turkish (Tr) and Gujarati (Gu). The evaluation was\ndone on the test dataset:\n\n\n\n    \n\n\n\n  Question Generation: this task takes an answer and the\ncorresponding passage as the input and generates the related\nquestion. They used the Chinese XQG dataset where they split into\n135k/5k/3k samples as the training/validation/test sets.\n\n\n\n    \n\n\n\n  Abstractive Text Summarization: this task produces the main\npoints of the input documents with new brief sentences. They used\nthe French XGiga where it’s split ito 500k/5k/5k pairs for\ntraining/validation/test, respectively.\n\n\n\n    \n\n\n\n  Cross-lingual Text Summarization: This task aims to generate the\nsummary of the input document in different languages. They used\nWikiLingua dataset which is a large-scale multilingual dataset with\nabout 770k article-summary pairs.\n\n\n\n    \n\n\n\n  Cross-lingual Data-to-text Generation: This task requires an\ninput of multiple triplets and generates a natural description based\non the input data. They used WebNLG dataset which is a bilingual\ndataset of parallel DBpedia triple sets and short texts. The\nlanguage directions are English-English and English-Russian. It\ncontains about 17k triple sets and 45k short texts in English as\nwell as 7k triple sets and 19k texts in Russian.\n\n\n\n    \n\n\n\n  Zero-shot Abstractive Summarization: They trained the model on\nthe English-English training set and evaluate it on the\nFrench-French and Chinese-Chinese test sets. They used XGiga dataset\nwhere the training data consists of 50k text-summary pairs, while\nboth the validation and test sets have 5k samples.\n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Cross-lingual Langluage Model",
        "title"     : "XLM-E: XLM via ELECTRA",
        "url"       : "/cross-lingual-lm/XLM-E",
        "date"      : "30/06/2021",
        "content": "XLM-E stands for “Cross-lingual Language Modeling via Electra” which is\na cross-lingual language model that was pre-trained on two\nELECTRA-style\ntasks as we are going to see later. XLM-E was proposed by Microsoft in\n2021 and published in their paper: Cross-lingual Language Model\nPre-training via ELECTRA. The\nofficial code for this paper can be found on Microsoft’s GitHub\nrepository: microsoft/unilm.\n\nUsing a pre-trained language model and then fine-tune it on downstream\ntasks has become a de facto trend in the field. However, these\npre-training techniques such as (Masked Language Modeling (MLM),\nTranslation Masked Modeling (TLM), ...etc.) usually requires massive\ncomputation resources. As shown in the following figure, XLM-E (red\nline) achieves 130x speedup compared with XLM-R augmented with TLM and\naround 100x speedup compared with\nXLM-R,\nXLM-Align, and\nInfoXLM:\n\n\n    \n\n\nPre-training\n\nSimilar to ELECTRA,\nXLM-E has two Transformer components, i.e., generator and discriminator. The\ngenerator predicts the masked tokens given the masked sentence or translation\npair, and the discriminator distinguishes whether the tokens are replaced by\nthe generator. XLM-E was pre-trained on two different pre-training tasks:\nmultilingual replaced token detection (MRTD), and\ntranslation replaced token detection (TRTD). The\noverall training objective is to minimize:\n\n\\[\\mathcal{L} = \\mathcal{L}_{\\text{MLM}} + \\lambda\\mathcal{L}_{\\text{MRTD}} + \\mathcal{L}_{\\text{TLM}} + \\lambda\\mathcal{L}_{\\text{TRTD}}\\]\n\nMultilingual RTD\n\nThis pre-training task is a multilingual form of the Replaced Token\nDetection (RTD) task introduced in the\nELECTRA model.\nHere is a small recap on RTD. The RTD task requires the model to\ndistinguish real input tokens from corrupted multilingual sentences. The\ninput sentences get corrupted by the generator model, and the\ndiscriminator should be able to classify the real tokens from the\nreplaced ones as shown in the following figure:\n\n\n    \n\n\nThe multilingual RTD is exactly the same with a few differences:\n\n\n  \n    The input text can be in various languages.\n  \n  \n    Both the generator and the discriminator are shared across\nlanguages. The vocabulary is also shared for different languages.\n  \n  \n    Masking is done uniformly while it was only $15\\%\\ $in the ELECTRA\npaper.\n  \n\n\n\n  Note:\nThey also tried span masking, but it significantly weakened the generator’s\nprediction accuracy, which in turn harmed the pre-training.\n\n\nGiven a input sequence $x$ that was masked using $M_{e}$ set of random\npositions; the loss function of the generator $G$ is:\n\n\\[\\mathcal{L}_{\\text{MLM}}\\left( x;\\ \\theta_{G} \\right) = - \\sum_{i \\in M_{e}}^{}{\\text{log}\\left(p_{G}\\left( x_{i} \\middle| x^{\\text{masked}} \\right) \\right)}\\]\n\nThe loss function of the discriminator $D$ is the following; knowing\nthat $n$ is the length of $x$ and $r_{i}$ is the label of the output (1\nfor “yes” and 0 for “No”):\n\n\\[\\mathcal{L}_{\\text{MRTD}}\\left( x;\\theta_{D} \\right) = - \\sum_{i = 1}^{n}{\\log\\left( p_{D}\\left( r_{i} \\middle| x^{\\text{corrupt}} \\right) \\right)}\\]\n\nTranslation RTD\n\nTranslation RTD is a novel discriminative pre-training task which aims\nto distinguish real input tokens from the translation pairs concatenated\ntogether. An input translation pair $\\left( e,f \\right)$ gets\nconcatenated together into a single sentence and then treated the same\nway as MRTD as shown in the following figure:\n\n\n    \n\n\nGiven a concatenated translation pair $\\left\\lbrack e;f \\right\\rbrack$\nthat was masked using $M_{e}$ and $M_{f}$ sets of random positions for\n$e$ and $f$ respectively, the loss function of the generator $G$ is:\n\n\\[\\mathcal{L}_{\\text{TLM}}\\left( e,f;\\theta_{G} \\right) = - \\sum_{i \\in M_{e}}^{}{\\log\\left( p_{G}\\left( e_{i} \\middle| \\left\\lbrack e;f \\right\\rbrack^{\\text{masked}} \\right) \\right)}\\]\n\n\\[\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  - \\sum_{i \\in M_{f}}^{}{\\log\\left( p_{G}\\left( f_{i} \\middle| \\left\\lbrack e;f \\right\\rbrack^{\\text{masked}} \\right) \\right)}\\]\n\nThe loss function of the discriminator $D$ is the following; knowing\nthat $n$ is the length of the concatenation and $r_{i}$ is the label of\nthe output (1 for “yes” and 0 for “No”):\n\n\\[\\mathcal{L}_{\\text{TRTD}}\\left( e,f;\\theta_{D} \\right) = - \\sum_{i = 1}^{n}{\\log\\left( p_{D}\\left( r_{i} \\middle| \\left\\lbrack e;f \\right\\rbrack^{\\text{corrupt}} \\right) \\right)}\\]\n\n\n  Note:\nThe generators in these pre-training tasks act like language models.\nThat’s why their loss functions were marked as\n$\\mathcal{L}_{\\text{MLM}}$ and $\\mathcal{L}_{\\text{TLM}}$.\n\n\nGated Relative Position Bias\n\nIn this paper, they proposed to use gated relative position bias\ninspired by the GRU\ncells, in the self-attention mechanism. First, let’s recap how the\nself-attention mechanism works. Given, input tokens\n$\\left\\{ x_{1},\\ …x_{n} \\right\\}$, the self-attention mechanism uses\n$q_{i}$, $k_{i}$, and $v_{i}$ for each input to compute the head output\n${\\widetilde{h}}_{i}$ as shown below:\n\n\\[{\\widetilde{h}}_{i} = \\text{softmax}\\left\\lbrack \\frac{q_{i}W_{i}^{Q}\\left( k_{i}W_{i}^{K} \\right)^{T}}{\\sqrt{d_{k}}} \\right\\rbrack v_{i}W_{i}^{V}\\]\n\nWhere\n$W_{i}^{Q},W_{i}^{K} \\in \\mathbb{R}^{d_{m} \\times d_{k}},W_{i}^{V} \\in \\mathbb{R}^{d_{m} \\times d_{v}}$\nare learned matrices. In this paper, the self-attention output is\nslightly different as shown below:\n\n\\[{\\widetilde{h}}_{i} = \\text{softmax}\\left\\lbrack \\frac{q_{i}W_{i}^{Q}\\left( k_{i}W_{i}^{K} \\right)^{T}}{\\sqrt{d_{k}}} + b_{i - j} \\right\\rbrack v_{i}W_{i}^{V}\\]\n\nWhere $b_{i - j}$ denotes the gated relative position bias which is\ncomputed via:\n\n\\[g^{\\left( \\text{update} \\right)},\\ g^{\\left( \\text{reset} \\right)} = \\sigma\\left( q_{i}\\text{.u} \\right),\\ \\sigma\\left( q_{i}\\text{.v} \\right)\\]\n\n\\[b_{i - j} = d_{i - j} + g^{\\left( \\text{update} \\right)}d_{i - j} + \\left( 1 - g^{\\left( \\text{update} \\right)} \\right).wg^{\\left( \\text{reset} \\right)}d_{i - j}\\]\n\nWhere $d_{i - j}$ is learnable relative position bias, the vectors\n$u,\\ v \\in \\mathbb{R}^{d_{k}}$ are parameters, σ is the sigmoid\nfunction, and $w \\in \\mathbb{R}$ is a learnable value.\n\nResults\n\nIn the following experiments, they used a 12-layer Transformer as the\ndiscriminator, with hidden size of 768, and FFN hidden size of 3,072 while they\nused a 4-layer Transformer as the discriminator using the same hyper-parameters.\nThey used the same vocabulary with\nXLM-R that consists of\n250K subwords tokenized by SentencePiece.\n\n\n    \n\n\nThey jointly pre-trained the generator and the discriminator using Adam\noptimizer for 125K training steps with a dynamic batching of\napproximately 1M tokens. $\\lambda$ was set to $50$. Check the following\ntable for the full list of pre-training hyper-parameters.\n\nFor pre-training, they used the CC-100 dataset for the MRTD task which\ncontains texts in 100 languages collected from\nCommonCrawl. They used\nparallel corpora in 100 languages, collected from MultiUN, IIT Bombay,\nOPUS, WikiMatrix, and CCAligned, for the TRTD task. For sampling, they\nused temperature sampling of $T = \\frac{10}{7}$.\n\nAfter pre-training the XLM-E model was fine-tuned on various tasks from\nthe XTREME benchmark. The following table shows the hyper-parameters\nused when fine-tuning. The XTREME benchmark contains seven cross-lingual\nunderstanding tasks, namely part-of-speech tagging (POS) on the\nUniversal Dependencies v2.5, NER named entity recognition on the Wikiann\ndataset, cross-lingual natural language inference on XNLI, cross-lingual\nparaphrase adversaries from word scrambling (PAWS-X), and cross-lingual\nquestion answering on MLQA, XQuAD, and TyDiQA-GoldP.\n\n\n    \n\n\nThe following table shows the evaluation results on XTREME benchmark\nknowing that results of XLM-E and XLM-R~base~ are averaged over five\nruns.\n\n\n    \n\n\nAs seen from the previous table, XLM-E outperforms previous models on\nQuestion Answering and Classification problems while achieves\ncompetitive performance of Structured Predictions. All of that while\nuses substantially less computation:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Cross-lingual Langluage Model",
        "title"     : "nmT5: NMT + Multilingual T5",
        "url"       : "/cross-lingual-lm/nmT5",
        "date"      : "01/08/2021",
        "content": "nmT5 stands for “NMT + Multilingual Text-to-Text Transfer Transformer”\nwhich is an attempt to improve the performance of the\nmT5 model by\nincorporating parallel data into pre-training. This model was proposed\nby the same authors from Google Research as the mT5 paper. In 2021, it\nwas published in this paper: nmT5 - Is parallel data still relevant for\npre-training massively multilingual language\nmodels?.\n\nA little bit of background: the mT5 model was pre-trained on mC4 dataset\n(a multilingual version of the\nC4 corpus) with a\nmasked language modeling “span-corruption” objective, where the encoder\nis fed a chunk of text with random spans replaced with a mask token, and\nthe decoder must reconstruct the masked-out tokens. In this paper, they\nare trying different objectives to incorporate parallel data into\npre-training:\n\n\n  TLM (Translation Language Modeling):\nThis objective was first proposed by the\nXLM model and\nwas used for encoder only pre-training. In this paper, they extended\nit to the encoder-decoder setting.\n\n\n\n    \n\n\n\n  NMT (Neural Machine Translation):\nThe input is the source text and the target is its translation. A\nlanguage code is prefixed to the input to inform the model of the\ntarget language.\n\n\n\n    \n\n\n\n  Denoised-NMT:\nSimilar to NMT, but with mask spans in the source sentence. The\nmodel must now learn to implicitly perform language modeling of the\nsource language while translating into the target language.\n\n\n\n    \n\n\n\n  Denoised-NMT+LM:\nSimilar to Denoised-NMT, but instead of implicit language modeling,\nthe model must explicitly predict the source text in addition to the\ntranslation. The target is a concatenation of the translation and\nsource sentence, while the input is the masked source sentence.\n\n\n\n    \n\n\n\n  Note:\nnmT5 is the mT5 model with the NMT objective.\n\n\nResults\n\nIn this paper, they used the mT5-Large model to perform the following\nexperiments, which is a 24 layer encoder-decoder transformer model.\nInstead of training a new model from scratch, they started from the\npublicly available mT5-Large checkpoint - which has been trained for\nover 1 trillion tokens - and did a second stage pre-training with a mix\nof monolingual and parallel data.\n\nFor pre-training, they used monolingual data from mC4 and parallel data\nfrom OPUS-100 which contains 55M translations covering 100 languages.\nThe mC4 corpus consists of unlabeled web text covering 101 languages, of\nwhich 81 overlap with the OPUS-100 languages.\n\nStarting from publicly available mT5-Large checkpoints, they pre-trained\nfor 100K steps with a mix of monolingual and parallel objectives. The\nparallel data is mixed into monolingual data at a $10\\%$ ratio, which\namounts to roughly 4 passes over the OPUS-100 corpus. Examples from each\nlanguage pair were sampled using the same language sampling distribution\nas mT5 with\n$\\alpha = 0.3$.\n\nPre-training was done with a batch size of 1M tokens and fine-tuned with\n$131,072$ tokens, with a constant learning rate of $0.001$. For\nfine-tuning, they fine-tuned for $10,000$ steps for TyDiQA, MTOP, NER\nand $25,000$ for WikiLingua, since it is a much larger dataset.\nCheckpoint selection is done based on the validation set.\n\nThe following table shows the results averaged across all the languages.\nOverall, adding parallel data through neural machine translation\nobjectives improves scores for all 4 tasks, with the NMT objective\nperforming the best.\n\n\n    \n\n\nFrom the past table, we can see that all NMT-based objectives shows\ngains over mT5 across all tasks. Among these, NMT averages the best\namong all other objectives leading to 7.2 higher scores averaging across\nall four tasks:\n\n\n    \n\n\nModel Size\n\nResearchers of mT5 found out that cross-lingual performance of language\nmodels increases monotonically with model size, that’s why the mT5-XXL\nhad the highest performance across five out of six tasks.\n\nTo study the impact of model capacity here, the researchers also\nexperimented with larger model sizes. Using the mT5-XL size (3.7B\nparams, 3× larger than mT5-Large), they observed gains for all tasks\nwith nmT5. However, the magnitude of the gains is largely diminished,\nhinting that the need for parallel data reduces as model capacity\nincreases.\n\n\n    \n\n\nThis finding is particularly promising for low-resource languages, where\nit is difficult to obtain high-quality parallel data. At the same time,\nnmT5-Large substantially reduces the performance gap between mT5-Large\nand mT5-XL, covering 70% of the headroom. Since bigger models are\nexpensive to train and even more expensive to deploy, this opens up\navenues for effectively using parallel data to improve performance of\nsmaller language models.\n"
      },
    
  
  
    
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "RNN: Recurrent Neural Networks",
        "url"       : "/language-modeling/RNN",
        "date"      : "19/09/1985",
        "content": "The neural n-gram language model we&#39;ve seen earlier was trained using\nthe a window-sized subset of the previous tokens. And this falls short\nwith long sentences as where the contextual dependencies are longer than\nthe window size. Now, we need a model that is able to capture\ndependencies outside the window. In other words, we need a system that\nhas some kind of memory to save these long dependencies.\n\nHere, we are going to talk about RNN or Recurrent Neural Network.\nRecurrent Neural Networks (RNN) are very effective for Natural Language\nProcessing and other sequence tasks because they can read inputs (such\nas words) one at a time, and remember some information/context through\nthe hidden layer activations that get passed from one time-step to the\nnext. This allows a uni-directional RNN to take information from the\npast to process later inputs. A bi-direction RNN can take context from\nboth the past and the future.\n\nBut why we are going to use Recurrent Neural Network (RNN) and not the\nvanilla feed-forward type of neural networks?\n\n\n  \n    First, the vanilla neural network has fixed input and output. But in\napplications like the ones above, it is required to have a flexible\nneural network architecture with different inputs and outputs. For\nexample, a sentiment analysis application should have a flexible\nneural network that can deal with different sentence lengths.\n  \n  \n    The standard neural network losses an important criterion which is\nsharing information between different layers. Unlike the RNN that\ncan connect to any neuron in any layer. This criterion is called\n“Cyclical Connections”.\n  \n\n\nHistory Background\n\nHere, we are going to talk about how the RNN has evolved in the past few\ndecades. The first wave of artificial neural networks started in the\nmid-1980s. After that wave, it became clear that feed-forward networks\nbecame are limited since they are unable to capture temporal\ndependencies, which are dependencies that change over time. Biological\nneural networks have recurring connections, so appling recurrence to\nartificial intelligence made natural sense.\n\nThe first time to add memory to neural networks were the TDNN or “Time\nDelay Neural Network” in 1989. Then after one year in 1990, Jeffrey\nLocke Elman created Elman’s Network or (Simple RNN). Then, Michael Irwin\nJordan produces a network that is similar to Elman’s Network and called\nit “Jordan’s Network” … original!!\n\nAll these networks suffer from something called “Vanishing Gradients”\nwhich means that they can’t capture information with span more than 8 or\n10 steps back. In the mid-1990s, Long Short-Term Memory, or LSTM for\nshort, were invented to address this very problem. The key idea in LSTM\nwas the idea that some signals can be kept fixed by using gates, and\nreintroduced or not. After than GRU or Gated Recurrent Unit was invented\nto optimize LSTM in 2014 by Kyunghyun Cho.\n\nTypes of RNN\n\nThere are different types of RNN:\n\n\n  One-to-many RNN: This neural network is used when we have just\none input and multiple outputs like Music Generation\nApplication which has just one input like the genre of the music,\nand the output is a sequence of music notes.\n\n\n\n    \n\n\n\n  Many-to-one RNN: This neural network is used when we have\nmany inputs and just one output like the sentiment\nanalysis applications which have a sequence of sentences, and the\noutput is a rate of one to five stars.\n\n\n\n    \n\n\n\n  \n    Many-to-many RNN: This neural network is used when we have\nmany inputs and many outputs. And we have two types in\nthese neural network:\n\n    \n      \n        When the input size is the same as the output size like\nin Word Embedding problem.\n      \n      \n        When the input size is different than the output size\nlike in Machine Translation Applications which takes a sentence\nof a certain length and returns another sentence in another\nlanguage which probably has different length.\n      \n    \n  \n\n\n\n    \n\n\n\n  Notes:\n\n  \n    At one-to-many RNNs, we take the output and insert it back as input.\n  This operation is called “Sampling”.\n  \n\n  \n    \n\n\n  \n    At machine translation RNN models, we divide the RNN into two parts,\n  the first is the “encoder” part which takes the original sentence.\n  The second part is the “decoder” part which returns the translated\n  sentence. This architecture is called the “Autoencoding\n  architecture”.\n  \n\n\nRNN Cells\n\n\n    \n\n\nAs you can see, a recurrent neural network can be seen as the repetition\nof a single cell (RNN cell). The following figure describes the\noperations for a single time-step of an RNN cell. The basic RNN cell\ntakes as input $x^{\\left\\langle t \\right\\rangle}$ (current input) and\n$a^{\\left\\langle t - 1 \\right\\rangle}$ (previous hidden state containing\ninformation from the past), and outputs\n$a^{\\left\\langle t \\right\\rangle}$ which is given to the next RNN cell\nand used to predict $y^{\\left\\langle t \\right\\rangle}$.\n\n\n    \n\n\nThe RNN forward propagation consists of several operations:\n\n\n  \n    Initialize $a$ vector that will store all the hidden states computed\nby the RNN. Also, initialize the &quot;next&quot; hidden state as $a_{0}$\n(initial hidden state).\n  \n  \n    Start looping over each time step, your incremental index is $t$:\n\n    \n      \n        Calculate the cell operations.\n      \n      \n        Store the &quot;next&quot; hidden state in $a$ ($t^{th}$ position).\n      \n      \n        Store the prediction in $y$.\n      \n    \n  \n\n\nPros &amp;amp; Cons\n\nRNNs have several advantages:\n\n\n  \n    They can process input sequences of any length.\n  \n  \n    They have some kind of memory as computation for step $t$ can (in\ntheory) use information from many steps back.\n  \n  \n    The same weights are used for all different inputs which means that\nthe number of learn-able parameters are reduced and that number\ndoesn’t scale with the size of the data unlike the traditional\nlanguage models.\n  \n\n\nBut they also have some disadvantages:\n\n\n  Computation is slow - because it is sequential, it cannot be parallelized.\n\n\nIn practice, it is difficult to access information from many steps back\ndue to problems like vanishing and exploding gradients.\n\nLSTM Cell\n\nAs we have mentioned before that a standard recurrent neural network\nwill work well for short sentences, but it suffers from vanishing\ngradient problem. So, it works best when each output\n$y^{\\left\\langle t \\right\\rangle}$ can be estimated using mainly “local”\ncontext (meaning information from inputs\n$x^{\\left\\langle t^{‘} \\right\\rangle}$ where $t’$ is not too far from\n$t$).\n\nLSTM stands for Long Short-Term Memory network. It was proposed in 1997\nby Sepp Hochreiter and Jürgen Schmidhuber. Here, you will build RNN\nusing LSTM cells which is a more complex than standard RNN model. It is\nbetter at addressing vanishing gradients as it is better remembering a\npiece of information and keep it saved for many time-steps. The\nfollowing figure shows the operations of an LSTM-cell.\n\n\n    \n\n\nAs you can see, the LSTM has a lot of modifications over RNN cell. In\nRNN cell, we had just two inputs ($x^{\\left\\langle t \\right\\rangle}$,\n$a^{\\left\\langle t - 1 \\right\\rangle}$) and there were no gates. In\nLSTM, there are three inputs ( input $x^{\\left\\langle t \\right\\rangle}$,\ncell state $c^{\\left\\langle t - 1 \\right\\rangle}$, and activation\n$a^{\\left\\langle t - 1 \\right\\rangle}$). The cell state here represents\nthe long short-term memory of this architecture.\n\nAll these equations can be summarized into the following ones knowing\nthat\n$\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack$\nmeans that the activating parameter of the previous time-step\n$a^{\\left\\langle t - 1 \\right\\rangle}$ is concatenated with the input\nvector $x^{\\left\\langle t \\right\\rangle}$:\n\n\\[c^{\\left\\langle t \\right\\rangle} = \\Gamma_{f}^{\\left\\langle t \\right\\rangle} \\ast c^{\\left\\langle t - 1 \\right\\rangle} + \\Gamma_{u}^{\\left\\langle t \\right\\rangle} \\ast tanh\\left( W_{c}\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack + b_{c} \\right)\\]\n\n\\[a^{\\left\\langle t \\right\\rangle} = \\Gamma_{o}^{\\left\\langle t \\right\\rangle} \\ast tanh\\left( Wc^{\\left\\langle t \\right\\rangle} \\right)\\]\n\nwhere\n\n$\\Gamma_{f}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{f}\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack + b_{f} \\right)$\n(Forget Gate)\n\n$\\Gamma_{u}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{u}\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack + b_{u} \\right)$\n(Update Gate)\n\n$\\Gamma_{o}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{o}\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack + b_{o} \\right)$\n(Output Gate)\n\nNow, let’s get into these three gates in more details:\n\n\n  \n    Forget Gate:\n\n    \n      This gate controls how much of the information of the\nprevious cell state\n$c^{\\left\\langle t - 1 \\right\\rangle}$ should be forgot or\nkept while calculating the current cell state\n$c^{\\left\\langle t \\right\\rangle}$.\n    \n  \n\n\n\\[\\Gamma_{f}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{f}\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack + b_{f} \\right)\\]\n\n\n  \n    $W_{f}$ are weights that govern the gate’s behavior and they are\ntrainable.\n  \n  \n    The output vector $\\Gamma_{f}^{\\left\\langle t \\right\\rangle}$ has\nvalues from $0$ to $1$ since it uses the sigmoid activation\nfunction. So, if one of the features of\n$\\Gamma_{f}^{\\left\\langle t \\right\\rangle}$ is $0$ (or close to\n$0$), then it means that the LSTM should forget that\npiece of information in the corresponding component of\n$c^{\\left\\langle t - 1 \\right\\rangle}$ while calculating the value\nfor $c^{\\left\\langle t \\right\\rangle}$. If one of the features is\n$1$, then it will keep the information.\n  \n  \n    Update Gate (Input Gate):\n\n    \n      Similar to the forget gate, this gate controls how much of\nthe information of the current input state\n$\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack$\nshould be used while calculating the current cell\n$c^{\\left\\langle t \\right\\rangle}$ state matter now.\n    \n  \n\n\n\\[\\Gamma_{u}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{u}\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack + b_{u} \\right)\\]\n\n\n  \n    $W_{u}$ are weights that govern the gate’s behavior and they are\ntrainable.\n  \n  \n    The output vector $\\Gamma_{u}^{\\left\\langle t \\right\\rangle}$ has\nvalues from $0$ to $1$ since it uses the sigmoid activation\nfunction. So, if one of the features of\n$\\Gamma_{u}^{\\left\\langle t \\right\\rangle}$ is $1$ (or close to\n$1$), then it means that the LSTM should update that\npiece of information in the corresponding component of the input\n$\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack$\nwhile calculating the value for $c^{\\left\\langle t \\right\\rangle}$.\nIf one of the features is $0$, then it won’t use this feature.\n  \n  \n    Output gate:\n\n    \n      This gate controls how much of the information of the current\ncell state $c^{\\left\\langle t \\right\\rangle}$ should be used for the\noutput/activation.\n    \n  \n\n\n\\[\\Gamma_{o}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{o}\\left\\lbrack a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack + b_{o} \\right)\\]\n\n\n  \n    $W_{o}$ are weights that govern the gate’s behavior and they are\ntrainable.\n  \n  \n    The output vector $\\Gamma_{o}^{\\left\\langle t \\right\\rangle}$ has\nvalues from $0$ to $1$ since it uses the sigmoid activation\nfunction. So, if one of the features of\n$\\Gamma_{o}^{\\left\\langle t \\right\\rangle}$ is $1$ (or close to\n$1$), then it means that the LSTM should use that piece of\ninformation in the corresponding component of the cell state\n$c^{\\left\\langle t \\right\\rangle}$ while calculating the value for\nthe output $a^{\\left\\langle t \\right\\rangle}$. If one of the\nfeatures is $0$, then it won’t use this feature.\n  \n\n\n\n  Note:\nSome researchers have found out that the parameter\n$c^{\\left\\langle t - 1 \\right\\rangle}$ needs to be concatenated in the\nforget gate. So, instead of using just\n$x^{\\left\\langle t \\right\\rangle}$ and\n$a^{\\left\\langle t \\right\\rangle}$ in the forget gate, we need also to\nuse $c^{\\left\\langle t - 1 \\right\\rangle}$ as shown in the following\nequation which could increase the accuracy. This is known as the\n“peephole connections”:\n\n\\[\\Gamma_{f}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{f}\\left\\lbrack c^{\\left\\langle t - 1 \\right\\rangle},a^{\\left\\langle t - 1 \\right\\rangle},x^{\\left\\langle t \\right\\rangle} \\right\\rbrack + b_{f} \\right)\\]\n\n\nThis link states some interesting\nfacts about LSTM, give it a read!!!\n\nGRU Cell\n\nGRU stands for Gated Recurrent Unit. GRU was introduced in 2014 by\nKyunghyun Cho as another solution to the vanishing gradient problem\nbeside LSTM cell. The main objective behind creating GRUs is to create a\nsimpler cell than LSTM without sacrificing the performance too much.\n\nGRU&#39;s performance on certain tasks of music modeling and speech signal\nmodeling was found to be similar to that of LSTM. GRUs have been shown\nto exhibit even better performance on certain smaller datasets. However,\nas shown by Gail Weiss &amp;amp; Yoav Goldberg &amp;amp; Eran Yahav, the LSTM is\n&quot;strictly stronger&quot; than the GRU as it can easily perform unbounded\ncounting, while the GRU cannot. That&#39;s why the GRU fails to learn\nsimple languages that are learnable by the LSTM.\n\nHe following The structure of GRU is like the following:\n\n\n    \n\n\nIn GRU, we don’t have cell states as the one with LSTM. Here, we only\nhave activation (hidden state). And GRUs have two gates (Update Gate,\nReset Gate) unlike LSTM which have three gates. The main equations used\nwith GRU are the following ones:\n\n\\[a^{\\left\\langle t \\right\\rangle} = c^{\\left\\langle t \\right\\rangle} = \\left( 1 - \\Gamma_{u}^{\\left\\langle t \\right\\rangle} \\right) \\ast c^{\\left\\langle t - 1 \\right\\rangle} + \\Gamma_{u} \\ast {\\overset{\\sim}{c}}^{\\left\\langle t \\right\\rangle}\\]\n\n\\[{\\overset{\\sim}{c}}^{\\left\\langle t \\right\\rangle} = tanh\\left( Wx^{\\left\\langle t - 1 \\right\\rangle} + U\\left( \\Gamma_{r} \\ast c^{\\left\\langle t \\right\\rangle} \\right) + b_{c} \\right)\\]\n\n$\\Gamma_{u}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{u}x^{\\left\\langle t - 1 \\right\\rangle} + U_{u}a^{\\left\\langle t - 1 \\right\\rangle} + b_{u} \\right)$\n(Update Gate)\n\n$\\Gamma_{r}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{r}x^{\\left\\langle t - 1 \\right\\rangle} + U_{r}a^{\\left\\langle t - 1 \\right\\rangle} + b_{u} \\right)$\n(Reset Gate)\n\nNow, let’s get into these three gates in more details:\n\n\n  \n    Update Gate:\n\n    \n      This gate controls the what is kept from previous hidden\nstate $c^{\\left\\langle t - 1 \\right\\rangle}$ and is\nupdated to the new candidate update\n${\\overset{\\sim}{c}}^{\\left\\langle t \\right\\rangle}$ while\ncalculating the current hidden state $c^{\\left\\langle t \\right\\rangle}.\n    \n  \n\n\n\\[\\Gamma_{u}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{u}x^{\\left\\langle t - 1 \\right\\rangle} + U_{u}a^{\\left\\langle t - 1 \\right\\rangle} + b_{u} \\right)\\]\n\n\n  \n    $W_{u}$ and $U_{u}$ are weights that govern the gate’s behavior and\nthey are trainable.\n  \n  \n    The output vector $\\Gamma_{u}^{\\left\\langle t \\right\\rangle}$ has\nvalues from $0$ to $1$ since it uses the sigmoid activation\nfunction. So, if one of the features of\n$\\Gamma_{u}^{\\left\\langle t \\right\\rangle}$ is $1$ (or close to\n$1$), then it means that the GRU should update that\npiece of information in the corresponding component of hidden\nstate $c^{\\left\\langle t \\right\\rangle}$ using the update\ncandidate ${\\overset{\\sim}{c}}^{\\left\\langle t \\right\\rangle}$. If\none of the features is $0$, then update using the old value\nof the hidden state $c^{\\left\\langle t - 1 \\right\\rangle}$.\n  \n  \n    Reset Gate:\n\n    \n      This gate controls what parts of the previous hidden state\nshould be used to compute the new hidden state\n$c^{\\left\\langle t \\right\\rangle}$.\n    \n  \n\n\n\\[\\Gamma_{r}^{\\left\\langle t \\right\\rangle} = \\sigma\\left( W_{r}x^{\\left\\langle t - 1 \\right\\rangle} + U_{r}a^{\\left\\langle t - 1 \\right\\rangle} + b_{u} \\right)\\]\n\n\n  \n    $W_{r}$ and $U_{r}$ are weights that govern the gate’s behavior and\nthey are trainable.\n  \n  \n    The output vector $\\Gamma_{r}^{\\left\\langle t \\right\\rangle}$ has\nvalues from $0$ to $1$ since it uses the sigmoid activation\nfunction. So, if one of the features of\n$\\Gamma_{r}^{\\left\\langle t \\right\\rangle}$ is $0$ (or close to\n$0$), then it means that the GRU should reset that\npiece of information in the corresponding component of\n$c^{\\left\\langle t - 1 \\right\\rangle}$ while calculating the value\nfor $c^{\\left\\langle t \\right\\rangle}$. If one of the features is\n$1$, then it will keep the information.\n  \n\n\n\n  Note:\nAs a rule of thumb, use LSTM cells in your model unless you care much\nabout the size of the model or the memory needed. GRUs have fewer\nparameters to compute\n\n\nHyper-parameters\n\nIn sequence models, there are three main choices that we need to make\nwhen we want to build an RNN:\n\n\n  \n    Choosing the cell type either standard RNN, GRU or\nLSTM. Now, it’s clear that both GRU and LSTM are both much\nbetter than standard RNN, but which one is even better? Actually,\nthat depends on the task and the type of the dataset. According to\nthis paper: “Visualizing and Understanding Recurrent\nNetworks” by Andrej\nKarpathy, both LSTM and GRU were tested on two datasets: in the\nfirst dataset, GRU was better in all used sizes and in the second,\nGRU was better in some sizes and worse in other. So, when creating\nyour own model, you should try both.\n  \n  \n    Choosing number of layers that we need to stack. And in the same\npaper, the number of layers stacked together is the best at two\nlayers and when increasing it to three layers, it gets mixed\nresults. So, when creating your own model, you should try using\ntwo and three layers. When creating advanced sequence models like\nCTC, we usually use five or even seven layers often with the use\nof LSTM cells.\n  \n  \n    And in case of using word embedding, then another hyper-parameter is\nadded which is the embedding size. Experimental results in this\npaper: “How to Generate a Good Word\nEmbedding?” have shown that\nthe larger the word embedding is, the better; at least we reach\nthe size of $200$. So, we should try different sizes starting from\n$50$ till $200$ or $300$ as google did in this paper:\n“Distributed Representations of Words and Phrases and their\nCompositionality”\nor even $500$.\n  \n\n\nIn the following table, we are going to see different RNN architectures\non different tasks:\n\n\n\n    \n        \n            Task\n            Cell\n            #Layers\n            Layer Size\n            Embedding Size\n            Source\n        \n    \n    \n        Speech Recognition (500K vocabulary)\n        LSTM\n        7\n        1000\n        -\n        paper\n    \n    \n        Speech Recognition (82K vocabulary)\n        LSTM\n        5\n        600\n        -\n        paper\n    \n    \n        Speech Recognition (small vocabulary)\n        LSTM\n        1, 3, 5\n        250\n        -\n        paper\n    \n    \n        Seq2Seq (160K → 80k)\n        LSTM\n        4\n        1000\n        1000\n        paper\n    \n    \n        Image Captioning\n        LSTM\n        -\n        512\n        -\n        paper\n    \n    \n        Image Generation\n        LSTM\n        -\n        256, 400, 800\n        -\n        paper\n    \n    \n        Question Answering\n        LSTM\n        2\n        500\n        300\n        paper\n    \n    \n        Text Summarization (119K → 68K)\n        GRU\n        -\n        200\n        100\n        paper\n    \n\n\n\n\nGradient Clipping\n\nRecall that our loop structure usually consists of a forward pass, a\ncost computation, a backward pass, and a parameter update. Before\nupdating the parameters, we will need to perform gradient clipping when\nneeded to make sure that your gradients are not &quot;exploding” (taking on\noverly large values).\n\n\n    \n\n\nSo, we will implement a function that takes in the gradients and returns\na clipped version of gradients if needed. There are different ways to\nclip gradients; we will use a simple element-wise clipping procedure, in\nwhich every element of the gradient vector is clipped to lie between\nsome range $\\left\\lbrack - N,N \\right\\rbrack$.\n\ndef clip(gradient_lst, max_value):\n... for gradient in gradient_lst:\n...     np.clip(gradient, -max_value, max_value, out= gradient)\n...     return gradient_lst\n\n\nIf you want to use it on our variables, we can do like so:\n\ndWax, dWaa, dWya, db, dby = clip(dWax, dWaa, dWya, db, dby, 10)\n\n\nHere, we provided the max_value as $10$. If any component of the\ngradient vector is greater than $10$, it would be set to $10$; and if\nany component of the gradient vector is less than $- 10$, it would be\nset to $- 10$. If it is between $- 10$ and $10$, it is left alone.\n\nSampling\n\nthe Sampling is the process of use the output of a certain neuron as an\ninput to the following neurons. It’s used with generative models where\nyou need to generate such as language models. Let’s see how it’s done:\n\n\n  \n    First, we input the usual $x^{\\left\\langle 1 \\right\\rangle}$ and\n$a^{\\left\\langle 0 \\right\\rangle}$, apply the activation function\nand get the output.\n  \n  \n    Now our first time stamp $y^{\\left\\langle 1 \\right\\rangle}$will have\nsome max probability over possible outputs, so we choose a randomly\nsample according to the probabilities of the possible outputs using,\nfor example, the numpy command np.random.choice.\n  \n  \n    Next we then go on to the second time step which is expecting\n$y^{\\left\\langle 1 \\right\\rangle}$ as input to the next time-step.\n  \n  \n    And so on…\n  \n\n\nExample, let&#39;s say that we are creating a language model. And after we\nsampled the first word, the first word happened to be “the”, which is\nvery common choice of first word. Then we pass “the” as\n$x^{\\left\\langle 2 \\right\\rangle}$. And now we’re trying to figure out\nwhat is the chance of the second word given that the first word is\n“the”. Then we again use this type of sampling function to sample\n$y^{\\left\\langle 2 \\right\\rangle}$ and so on.\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "Neural N-gram Language Model",
        "url"       : "/language-modeling/Neural_N-gram",
        "date"      : "09/02/2003",
        "content": "As we discussed before, the n-gram language model has a few problems\nlike the data sparsity and the big storage need. That’s why these\nproblems were first tackled by Bengio et al in 2003 and published under\nthe name “A Neural Probabilistic Language\nModel”,\nwhich introduced the first large-scale deep learning for natural\nlanguage processing model. This model learns a distributed\nrepresentation of words, along with the probability function for word\nsequences expressed in terms of these representations. The idea behind\nthis architecture is to deal with the language model task as if it is a\nclassification problems where:\n\n\n  \n    The input is a window-sized subset of the previous tokens.\n  \n  \n    The output is the current token which could be any token from the\nvocabulary $V$.\n  \n  \n    Each token is represented using a one-hot vector.\n  \n  \n    The loss function is the cross entropy.\n  \n\n\nThe following figure shows a simplified version of the neural network\narchitecture that was created by Yoshua Bengio in 2003. In the original\nversion of the model, Bengio used the input word vectors with both the\nhidden layer and the output layer. This simplified version concatenated\nword embeddings for the input words:\n$e = \\left\\lbrack e^{\\left( 1 \\right)};e^{\\left( 2 \\right)};e^{\\left( 3 \\right)};e^{\\left( 4 \\right)} \\right\\rbrack$,\nthe red layer signifies the hidden layer:\n$h = f\\left( We + b_{1} \\right)$ , and the green output distribution is\na softmax over the vocabulary:\n$ŷ = \\text{softmax}\\left( Uh + b_{2} \\right)$.\n\n\n    \n\n\nAnd despite this model is way faster than other models and simpler to\nimplement, there were still some problems that need to be fixed:\n\n\n  \n    The fixed window is still a problem since some sentences need bigger\nwindows to catch the context. So, no matter how big your window\nis, it will never be enough for some sentences.\n  \n  \n    $e^{\\left( 1 \\right)},e^{\\left( 2 \\right)}$, … etc. are multiplied\nby completely different weights in W which means that the weight\nlearning in one section is not shared with the others and that’s\ncounter-intuitive. As shown in the following figure, we can see\nthat $e^{\\left( 1 \\right)}$ will only by multiplied by the blue\nregion of the weight matrix $W$, and $e^{\\left( 2 \\right)}$ will\nbe only multiplied by the green region, and so on.\n  \n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "Tree Recursive Neural Network",
        "url"       : "/language-modeling/Tree_Recursive_NN",
        "date"      : "28/07/2011",
        "content": "Tree Recursive Neural Network is a model created by Richard Socher et al.\nand published in this paper: Parsing Natural Scenes and Natural Language\nwith Recursive Neural\nNetworks.\nThe main idea behind Tree Recursive Neural Network is to provide a\nsentence embedding that could represent the meaning of the sentence the\nsame way we did with word embedding. So, two sentences that of different\nwords like “the country of my birth” and “the place where I was born”\nwill have similar vector despite having totally different words. The\nmeaning vector of a sentence is determined by actually two things:\n\n\n  \n    The meaning of the words forming this sentence.\n  \n  \n    The rules/structure that combined these words.\n  \n\n\nBack in the old days, we used certain rules determined by linguists to\nform a grammatical structure for a given sentence. The algorithm that\nforms such a tree is called “CKY” and it was widely used for “parsing”\nwhich is to form a binary tree for a given sentence that describes the\nsyntactic structure of the given sentence. So, given a sentence like\n“the day of my birth”, CKY will return a binary tree that looks like\nthis:\n\n\n    \n\n\nBut now in Tree Recursive NN, we are going to use word embeddings and\nRNN to from a tree of a given sentence. The same sentence will appear\nlike the following figure. As we can see, it has the same structure as\nbefore but an additional information which is the “sentence embedding”.\nThe embedding vector at the top of the structure is what makes this\nmodel super important:\n\n\n    \n\n\nHow are we going to calculate these sentence embedding in Tree Recursive\nNN? The idea is when we want to build a representation or a larger unit,\nwe need to take the representation of its children and stick them to some\nkind of neural network which will return two things:\n\n\n    \n\n\n\n  \n    A vector that is going to represent the meaning of the children.\n  \n  \n    The score of how reasonable the new node would be.\n  \n\n\nNow, we can use this simple neural network to parse a sentence. The\neasiest way to do that is to run a greedy parser that is going to look\nat what seems best and make decisions of every action and proceed along.\nWe can start of the sentence “the cat sat on the mat” and we are going\nto take each pair of words and calculate the representation of that pair\njust like so:\n\n\n    \n\n\nThen, we are going to take the pair that score best and merge it into a\nconstituent. At that point, we could repeat the same step after\nconsidering the new pair as a new word. So, in this example, we can see\nthat the pair “the cat” has the highest score. Then, we need to combine\nit into one constituent and recalculate the word pairs like so:\n\n\n    \n\n\nNow, the highest score is “the mat” word pair. So, we are going to\ncombine them into one constituent and recalculate the word pairs like\nso:\n\n\n    \n\n\nWe keep doing that till we form the Tree Recursive neural network:\n\n\n    \n\n\nRecursive NN\n\nAs we can see, we will be able to form a recursive tree depending on\nthis neural network that could combine two word-vectors into one with\nadditional information which is the score of how plausible this merge\nis. Now, let’s talk about how to form such a neural network.\n\nStandard Recursive NN\n\nThe simplest way of Neural Network to use here in our Tree Recursive NN\nis to form one single weight matrix Tree RNN like so:\n\n\n    \n\n\nWhich can be represented like so putting in mind that $\\begin{bmatrix}\nc_{1} \nc_{2} \n\\end{bmatrix}$ means concatenating the two word-vectors of the two\nchildren $c_{1}$ and $c_{2}$. The weights that we need to learn in this\nsimple neural network are $W$ and $U$:\n\n\\[parent = tanh\\left( W\\begin{bmatrix}\nc_{1} \\\\\nc_{2} \\\\\n\\end{bmatrix} + b \\right),score = U^{T}\\text{.parent}\\]\n\nBut this simple representation has a few problems:\n\n\n  \n    This simple single matrix could capture some phenomena but not\nadequate for complex, higher order composition and long sentences.\n  \n  \n    There is no interaction between the input words.\n  \n  \n    The composition function $tanh\\left( W\\begin{bmatrix}\nc_{1} \nc_{2} \n\\end{bmatrix} + b \\right)$ is the same for all syntactic categories.\nSo, we have just one weight matrix and it doesn’t matter if we are\nputting (adjective and a noun) or (a verb and an object) or even (a\nword and a period). For all these cases, we are using the same\nweight matrix $W$.\n  \n\n\nCVG\n\nCVG or Compositional Vector Grammars is another way to make a better and\nfaster Recursive NN. The idea behind this method is to combine the\nProbabilistic Context Free Grammar (PCFG) with the Tree Recursive NN.\n\nTO BE CONTINUED\n\nMax-Margin Loss Function\n\nHere, we will discuss a popular error metric known as the maximum margin\nobjective. The idea behind using this objective is to ensure that the\nscore computed for &quot;true&quot; labeled data points is higher than the score\ncomputed for &quot;false&quot; labeled data points.\n\nFor example, we want the score computed for the &quot;true&quot; sentence\n&quot;Museums in Paris are amazing&quot; as s and the score computed for the\n&quot;false&quot; labeled window &quot;Not all museums in Paris&quot; as S~c~\n(subscripted as c to signify that the window is &quot;corrupt&quot;). Then, our\nobjective function would be to maximize (S − S~c~) or to minimize (S~c~\n− S):\n\nHowever, the above optimization objective is risky in the sense that it\ndoes not attempt to create a margin of safety. We would want the\n&quot;true&quot; labeled data point to score higher than the &quot;false&quot; labeled\ndata point by some positive margin Δ. In other words, we would want\nerror to be:\n\n\\[Loss = max\\left( \\Delta + S_{c} - S,0 \\right)\\]\n\nWhat we want to do is to find the best Tree which has the highest score,\nand we are kind of approximated this by getting the best constituent at\nevery particular point in a time. So, the final thing is to set a loss\nfunction that we need to optimize. The loss function we are going to use\nhere is called the “max-margin loss function”:\n\n\\[s\\left( x,y \\right) = \\sum_{n \\in nodes\\left( y \\right)}^{}s_{n}\\]\n\n\\[J = \\sum_{i}^{}{s\\left( x_{i},y_{i} \\right)} - \\max_{y \\in A\\left( x_{i} \\right)}\\left( s\\left( x_{i},y \\right) + \\Delta\\left( y,y_{i} \\right) \\right)\\]\n\nWhere, $x$ is a sentence, $y$ is a parse tree, $A\\left( x \\right)$ is a\nstructure search for the best tree. We have used the greedy search, but\na good substitute would be a beam search. The loss\n$\\Delta\\left( y,y_{i} \\right)$ penalizes all incorrect decisions.\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "GCNN: Gated CNN",
        "url"       : "/language-modeling/GCNN",
        "date"      : "23/12/2016",
        "content": "One of the major defects of Seq2Seq models is that it can’t process\nwords in parallel. For a large corpus of text, this increases the time\nspent translating the text. CNNs can help us solve this problem. In this\npaper: “Language Modeling with Gated Convolutional\nNetworks”, proposed by FAIR\n(Facebook AI Research) in 2017, the researchers developed a new\narchitecture that uses gating mechanism over stacked convolution layers\nthat outperforms the\nSeq2Seq model.\n\n\n    \n\n\nUsing stacked convolutions layers is more efficient since it allows\nparallelization over sequential tokens. Using a kernel size of $k$ over\na context of size $N$, this new architecture will perform\n$O\\left( \\frac{N}{k} \\right)$ operations unlike recurrent networks which\nwill perform a linear number $O\\left( N \\right)$ of operations. The\nformer figure illustrates the model architecture; where:\n\n\n  \n    The input to the model is a sequence of words $w_{0},\\ …w_{N}$.\n  \n  \n    Each word is represented by a vector embedding stored in a lookup\ntable $D^{\\left| V \\right| \\times e}$ where $\\left| V \\right|$ is\nthe vocabulary and $e$ is the embedding size.\n  \n  \n    After the lookup table, the input will be represented as word\nembeddings:\n  \n\n\n\\[E = \\left\\lbrack D_{w_{0}},\\ ...\\ D_{w_{N}} \\right\\rbrack\\]\n\n\n  The hidden layers $h_{0},\\ …h_{L}$, where $L$ is the number of\nlayers, are computed as:\n\n\n\\[h_{l}\\left( X \\right) = \\left( X*W + b \\right) \\otimes \\sigma\\left( X*V + c \\right)\\]\n\nWhere $X \\in \\mathbb{R}^{N \\times m}$ is the input of layer $h_{l}$\n(either word embeddings or the outputs of previous layers),\n$W \\in \\mathbb{R}^{k \\times m \\times n}$, $b \\in \\mathbb{R}^{n}$,\n$V \\in \\mathbb{R}^{k \\times m \\times n}$, and $c \\in \\mathbb{R}^{n}$ are\nlearned parameters, $m$ and $n$ are respectively the number of input and\noutput feature maps, $\\sigma$ is the sigmoid function and $\\otimes$ is\nthe element-wise product between matrices.\n\n\n  Note:\nWhen convolving inputs, they made sure that $h_{i}$ does not contain\ninformation from future words by shifting the convolutional inputs to\nprevent the kernels from seeing future context.\n\n\nAdaptive Softmax\n\nThe simplest choice to To obtain model’s predictions at the last layer\nis to use a softmax layer, but this choice is often computationally\ninefficient for large vocabularies. A better choice could be\nhierarchical softmax (Morin &amp;amp; Bengio, 2005).\n\nIn the paper, they chose an improvement of the latter known as adaptive\nsoftmax which assigns higher capacity to very frequent words and lower\ncapacity to rare words. This results in lower memory requirements as\nwell as faster computation at both training and test time. You can find\nan efficient implementation of Adaptive Softmax in Facebook Research’s\nofficial GitHub repository:\nfacebookresearch/adaptive-softmax.\n\nExperiments\n\nAll experiments in this paper were using two public large-scale language\nmodeling datasets: Google’s Billion\nword\ndataset (one billion token) and\nWikiText-103\ndataset (100M tokens). For both datasets, $\\left\\langle S \\right\\rangle$\nand $\\left\\langle /S \\right\\rangle$ tokens were added at the start and\nend of each line respectively. In terms of optimization, they\ninitialized the layers of the model with the He initialization with the\nlearning rate sampled uniformly in the interval\n$\\lbrack 1.,\\ 2.\\rbrack$, the momentum was set to $0.99$, and gradient\nclipping was set to $0.1$ to prevent gradient explosion. Also, weight\nnormalization was used to make training faster as seen in the following\nfigure:\n\n\n    \n\n\nFor model architecture, they selected the number of residual blocks\nbetween $\\left{ 1,\\ …10 \\right}$, the size of the embeddings with\n$\\left{ 128,\\ …256 \\right}$, the number of units between\n$\\left{ 128,\\ …2048 \\right}$, and the kernel width between\n$\\left{ 3,\\ …5 \\right}$ as shown in the following table:\n\n\n    \n\n\nThe following table shows the test perplexity over Google’s Billion word\ndataset; as we can see, GCNN outperforms all LSTMs with the same output\napproximation while only requiring a fraction of the operations:\n\n\n    \n\n\nSame thing happens with WikiText-103 dataset; GCNN outperforms LSTM\nmodels:\n\n\n    \n\n\nThe following figure shows a comparison between GCNN and the\nstate-of-the-art LSTM model back\nin 2016 which uses the full softmax, the adaptive softmax approximation\ngreatly reduces the number of operations required to reach a given\nperplexity:\n\n\n    \n\n\nGating Mechanism (GLU)\n\nGating mechanisms control the path through which information flows in\nthe network. LSTMs\nenable long-term memory via a separate cell controlled by different\ngates (forget, update, output gates). This allows information to flow\nthrough potentially many timesteps and without these gates, information\ncould easily vanish.\n\nIn contrast, convolutional networks do not suffer from the same kind of\nvanishing gradient and we find experimentally that they do not require\nforget gates. Therefore, they considered using only output\ngates, which allow the network to control what information\nshould be propagated through the hierarchy of layers.\n\nIn this paper, they tried four different output gating mechanisms on the\nWikiText-103\nbenchmark:\n\n\n  Tanh (not gating mechanism):\n\n\n\\[h_{l}\\left( X \\right) = \\tanh\\left( X*W + b \\right)\\]\n\n\n  ReLU (not gating mechanism):\n\n\n\\[h_{l}\\left( X \\right) = \\text{ReLU}\\left( X*W + b \\right)\\]\n\n\n  Gated Tanh Unit (GTU):\n\n\n\\[h_{l}\\left( X \\right) = \\tanh\\left( X*W + b \\right) \\otimes \\sigma\\left( X*V + c \\right)\\]\n\n\n  Gated Linear Unit (GLU):\n\n\n\\[h_{l}\\left( X \\right) = \\left( X*W + b \\right) \\otimes \\sigma\\left( X*V + c \\right)\\]\n\nAnd the results show that GLU achieves the best perplexity over the data\nas seen in the following figure. There is a gap of about 5 perplexity\npoints between the GLU and ReLU which is similar to the difference\nbetween the LSTM and RNN models on the same dataset.\n\n\n    \n\n\nNote:\nThe difference between GTU and $\\text{Tanh}$ models shows us the effect\nof gating mechanism since the $\\text{Tanh}$ model can be thought of as a\nGTU network with the sigmoid gating units removed:\n\nThe experiments so far have shown that the GLU benefits from the linear\npath the unit provides compared to other non-linearities such as GTU.\nThat’s why they decided to compare GLU to purely linear networks in\norder to measure the impact of the nonlinear path provided by the gates\nof the GLU. In the paper, they compared GLU to:\n\n\n  Linear (not gating mechanism):\n\n\n\\[h_{l}\\left( X \\right) = \\left( X*W + b \\right)\\]\n\n\n  Bi-linear:\n\n\n\\[h_{l}\\left( X \\right) = \\left( X*W + b \\right) \\otimes \\left( X*V + c \\right)\\]\n\nThe following figure shows the performance of the three mechanisms on\nGoogle’s Billion word\nbenchmark.\nAs we can see, GLU still outperforms other methods\n\n\n    \n\n\nContext Size $k$\n\nThe following figure shows the impact of context size for GCNN on\nGoogle’s Billion word dataset (left graph) and WikiText-103 dataset\n(right graph). Generally, larger contexts improve accuracy but returns\ndrastically diminish with windows larger than 40 words:\n\n\n    \n\n\nThe previous figure Figure 4 also shows that WikiText-103 benefits much\nmore from larger context size than Google Billion Word as the\nperformance degrades more sharply with smaller contexts.\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "GPT",
        "url"       : "/language-modeling/GPT",
        "date"      : "11/06/2018",
        "content": "Transform is a state-of-the-art architecture for machine translation.\nOpenAI tried to use this architecture for the language modeling task in\nthis paper “Improving Language Understanding by Generative\nPre-Training”\nunder the name “Improving Language Understanding by Generative\nPre-Training” which was published in 2018. Pre-training is the process\nof training a model with one task (language modeling in the paper) that\nis able to help it form parameters that can be used to make other tasks\neasier (four other tasks: natural language inference, question\nanswering, semantic similarity, and text classification).\n\nThe Encoder-Decoder structure of the transformer made it perfect for\nmachine translation. But how would you use it to pre-train a language\nmodel that can be fine-tuned for other tasks like sentiment analysis or\ntext classification? The way openAI team did it was pretty smart. It\nturns out that we don’t need the entire transformer architecture to\nadopt a language model. We can do it with just the decoder of the\ntransformer.\n\n\n    \n\n\nThe decoder is a good choice because it’s\na natural choice for language modeling (predicting the next word) since\nit’s built to mask future tokens. Since there is no encoder in this set\nup, the decoder layer would not have the encoder-decoder attention\nsub-layer that vanilla transformer decoder layers have. So, the decoder\narchitecture becomes as shown in the image on the right.\n\nIn the original paper, they stacked twelve decoder layers with a\nfeed-forward neural network at the end with Softmax loss function. With\nthis structure, we can proceed to train the model on the same language\nmodeling task: predict the next word using massive (unlabeled) datasets.\n\n\n    \n\n\nModel Specification\n\nThe language model was trained on the BooksCorpus dataset for training\nthe language model. This dataset contains over 7,000 unique unpublished\nbooks from a variety of genres. Crucially, it contains long stretches of\ncontiguous text, which allows the generative model to learn to condition\non long-range information.\n\nThe model, itself, has the following characteristics:\n\n\n  \n    12-layer decoder-only transformer.\n  \n  \n    Masked self-attention with multi-heads (768 dimensional states and\n12 attention heads).\n  \n  \n    For the position-wise feed-forward networks, they used 3072 neurons.\n  \n  \n    They used the Adam optimization scheme.\n  \n  \n    The learning rate was increased linearly from zero over the first\n2000 updates and annealed to 0 using a cosine schedule with a max\nlearning rate of 2.5e-4.\n  \n  \n    We train for 100 epochs on minibatches of 64 randomly sampled,\ncontiguous sequences of 512 tokens.\n  \n  \n    Since layer normalization is used extensively throughout the model,\na simple weight initialization of $\\mathcal{N}(0,\\ 0.02)$ was\nsufficient.\n  \n  \n    They used a bytepair encoding (BPE) vocabulary with 40,000 merges.\n  \n  \n    Residual, embedding, and attention dropouts with a rate of 0.1 for\nregularization.\n  \n  \n    They also employed a modified version of L2 regularization proposed\nin this paper, with w = 0.01 on all non bias or gain weights.\n  \n  \n    For the activation function, we used the Gaussian Error Linear Unit (GELU).\n  \n  \n    They used learned position embeddings instead of the sinusoidal\nversion proposed in the original work.\n  \n\n\nFine-Tuning\n\nThe OpenAI paper outlines a number of input transformations to handle\nthe inputs for different types of tasks. Since our language model was\ntrained on contiguous sequences of text, we require some modifications\nto apply it to the different NLP tasks.\n\nPrevious work proposed learning\ntask-specific architectures on top of transferred representations. We\nuse a traversal-style approach where we convert structured inputs into\nan ordered sequence that our pre-trained model can process which allows\nus to avoid making extensive changes to the architecture across\ndifferent tasks.\n\nThe following image shows the structures of the model and the input\ntransformations to carry out different tasks. In these transformations,\nwe are using the following special tokens;\n$\\left\\langle s \\right\\rangle$ for start token,\n$\\left\\langle e \\right\\rangle$ for extract token, and\n$\\left\\langle \\$ \\right\\rangle$ for delimiter token:\n\n\n    \n\n\nSo, for example the classification model will look like this:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "BERT",
        "url"       : "/language-modeling/BERT",
        "date"      : "11/10/2018",
        "content": "BERT stands for “Bidirectional Encoder Representations from\nTransformers” which is a model published by researchers at Google in\nthis paper: “BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding” in 2018.\nIt has caused a stir in the NLP community by presenting state-of-the-art\nresults in a wide variety of NLP tasks, including Question Answering\n(SQuAD v1.1), Natural Language Inference (MNLI), and others.\n\nBERT’s key technical innovation is applying the bidirectional training\nto the openAI Transformer. See, the openAI transformer gave us a\nfine-tunable pre-trained language model based on the Transformer. But\nsomething went missing in this transition from LSTMs to Transformers.\nLSTM language model was bi-directional, but the openAI transformer only\ntrains a forward language model. BERT introduced a novel technique to\ntrain the openAI transformer in bi-directional manner which is to train\nBERT using two unsupervised tasks:\n\n\n  \n    Masked LM (MLM): To catch the contextual relation between words.\nVery important for tasks like languge modeling, text\nclassification, ...etc.\n  \n  \n    Next Sentence Prediction (NSP): To catch the contextual relation\nbetween sentences. Very important for tasks like Question\nAnswering, Natural Language Inference... etc.\n  \n\n\nMLM\n\nSince bidirectional conditioning requires each word to indirectly see\nitself and others in a multi-layered context, researchers had to find a\nway to overcome this obstacle. And they did that by introducing\nmasks. Previously, this technique was called “Cloze\nprocedure” where the key idea is to remove words from the input and\npredict them with the remaining input. As you’ve probably guessed, this\nis very similar to the CBOW model for distributed word embeddings.\n\n\n    \n\n\nIn the paper; they said that before feeding word sequences into BERT,\n15% of the words in each sequence are replaced with a [MASK] token;\nand the type of mask will be different according to the following\ndistribution\n\n\n  \n    80% of the time: the mask will be [MASK].\n  \n  \n    10% of the time: the mask will be a random word.\n  \n  \n    10% of the time: The mask will be the original word.\n\n    The advantage of this procedure is that the Transformer-encoder does\nnot know which words it will be asked to predict or which have been\nreplaced by random words, so it is forced to keep a distributional\ncontextual representation of every input token. Additionally,\nbecause random replacement only occurs for 1.5% of all tokens (i.e.,\n10% of 15%), this does not seem to harm the model’s language\nunderstanding capability.\n\n    After that, the model attempts to predict the original value of the\nmasked words, based on the context provided by the other,\nnon-masked, words in the sequence. In technical terms, the\nprediction of the output words requires:\n  \n  \n    Adding a classification layer on top of the encoder output.\n  \n  \n    Multiplying the output vectors by the embedding matrix, transforming\nthem into the vocabulary dimension.\n  \n  \n    Calculating the probability of each word in the vocabulary with softmax.\n  \n\n\n\n    \n\n\nThe BERT loss function takes into consideration only the prediction of\nthe masked values and ignores the prediction of the non-masked words. As\na consequence, the model converges slower than directional models.\n\n\n  Note:\nThere is a new token [CLS] added to the start of the input sentence\nwhen passed to BERT. [CLS] stands for classification and this is just\na way to tell BERT we are using your architecture for classification.\n\n\nNow, let’s ask a very important question: what happens if we increased the\nmasking percentage to more than 15%? Actually, researchers at Princeton tried\nto answer this question in their paper “Should You Mask 15% in Masked Language\nModeling?” published in 2022. And they\nfound out that masking up to 40% of input tokens can outperform the 15%\nbaseline, and even masking 80% can preserve most of the performance,\nas measured by fine-tuning on downstream tasks. You can use this GitHub\nrepository: princeton-nlp/dinkytrain\nto reproduce their results.\n\nNSP\n\nMany important downstream tasks such as Question Answering and Natural\nLanguage Inference are based on understanding the relationship between\ntwo sentences, which is not directly captured by language modeling.\n\nIn order to make BERT better at handling relationships between multiple\nsentences, the pre-training process includes an additional task called\n“Next Sentence Prediction (NSP)” where the model receives pairs of\nsentences as input and learns to predict if the second sentence in the\npair is the subsequent sentence in the original document.\n\nDuring training, 50% of the inputs are a pair in which the second\nsentence is the subsequent sentence in the original document, while in\nthe other 50% a random sentence from the corpus is chosen as the second\nsentence. To help the model distinguish between the two sentences in\ntraining, the input is processed in the following way before entering\nthe model:\n\n\n  \n    A [CLS] token is inserted at the beginning of the first sentence\nand a [SEP] token is inserted at the end of each sentence.\n  \n  \n    A sentence embedding indicating Sentence A or Sentence B is added to\neach token. Sentence embeddings are similar in concept to token\nembeddings with a vocabulary of 2.\n  \n  \n    A positional embedding is added to each token to indicate its\nposition in the sequence. The concept and implementation of\npositional embedding are presented in the Transformer paper.\n  \n\n\nAnd all of this information can be seen in the following figure:\n\n\n    \n\n\nFine-tuning Tasks\n\n\n    \n\n\nFine-tuning in this context means using BERT for a specific task\nsuch as QA, text classification, language inference, ...etc. BERT can be used\nfor a wide variety of language tasks, while only adding a small layer to the\ncore model as explained in:\n\n\n  Text Classification:\nClassification tasks such as sentiment analysis are done\nsimilarly to Next Sentence classification, by adding a\nclassification layer on top of the Transformer output for the\n[CLS] token.\n\n\n\n    \n\n\n\n  Question Answering:\nA question-answering model can be trained by learning two extra\nvectors that mark the beginning and the end of the answer.\n\n\n\n    \n\n\n\n  Named Entity Recognition (NER):\nBERT can be trained by feeding the output vector of each token\ninto a classification layer that predicts the NER label.\n\n\nBERT Linguistic Patterns\n\nAccording to this paper “BERT Rediscovers the Classical NLP\nPipeline” published by Google in\n2019, the authors of this paper found out that different layers of BERT\ncapture different linguistic semantics. For example, they found out that\nlower layers of BERT encode more local syntax while higher layers\ncapture more complex semantics. They used a pre-trained BERT-base &amp;amp;\nBERT-Large on eight different tasks.\n\nThe following table shows the layer-wise metrics on BERT-base (left) and\nBERT-large (right) where blue bars are mixing weights that tell us\nwhich layers are most relevant when a probing classifier at this\nlayer has access to the whole BERT model, while purple\nbars are differential scores normalized for each task which \nmeasures how much better we do on the probing task if we observe\none additional encoder layer:\n\n\n    \n\n\nFrom the past figure, if we have access to the whole BERT model, we can\nsee the following with respect to each task:\n\n\n  \n    Part-of-speech (POS): The purple bars show that the first few\nlayers are the most important; and the blue bars show us that\nprobing the first layers will have the same effect as the last\nlayers.\n  \n  \n    Constituents (Consts.): The purple bars show that the first few\nlayers are the most important; and the blue bars show us that\nprobing the middle layers will have the highest effect.\n  \n  \n    Dependencies (Deps.): The purple bars show that the first few\nlayers of BERT-base are the most important while the middle layers\nof BERT-large are the most important; and the blue bars show us that\nprobing the middle layers will have the highest effect.\n  \n  \n    Entities: The purple bars show that the first few layers are the\nmost important; and the blue bars show us that probing the\nmiddle-last layers will have the highest effect.\n  \n  \n    Semantic role labeling (SRL): The purple bars show that the\nfirst few layers are the most important; and the blue bars show us\nthat probing the middle layers will have the highest effect.\n  \n  \n    Coreference (Coref.): The purple bars show that the last few\nlayers are the least important; and the blue bars show us that\nprobing the middle-last layers will have the highest effect.\n  \n  \n    Semantic proto-roles (SPR): The purple bars show all layers are\nimportant; and the blue bars show us that probing over all layers\nhas the same effect.\n  \n  \n    Relation classification (SemEval): The purple bars show all\nlayers are important; and the blue bars show us that probing over\nall layers has the same effect.\n  \n\n\nBase / Large BERT\n\nThe paper presents two model sizes for BERT:\n\n\n  \n    BERT BASE: Comparable in size to the OpenAI Transformer in order\nto compare performance.\n  \n  \n    BERT LARGE: A ridiculously huge model which achieved the state\nof the art results reported in the paper.\n  \n\n\nThe following summarizes the difference between both models:\n\n\n\n    \n        \n            \n            BERT SMALL\n            BERT BASE\n            BERT LARGE\n        \n    \n    \n        Transformer Blocks\n        4\n        12\n        24\n    \n    \n        Feed-Forward hidden neurons\n        312\n        768\n        1024\n    \n    \n        Attention Heads\n        12\n        12\n        16\n    \n    \n        Input Tokens\n        512\n        512\n        512\n    \n    \n        Parameters\n        14.5 million\n        110 million\n        345 million\n    \n    \n        Hardware for Training\n        -\n        4 TPU + 4 days\n        16 TPU + 4 days\n    \n    \n        Hardware for Inference\n        -\n        1 GPU\n        1 TPU\n    \n\n\n\nBERT Vs GPT\n\nThe most comparable existing pre-training method to BERT is OpenAI GPT,\nwhich trains a left-to-right Transformer LM on a large text corpus. In\nfact, many of the design decisions in BERT were intentionally made to\nmake it as close to GPT as possible so that the two methods could be\nminimally compared. However, there are several other differences:\n\n\n  \n    GPT is trained on the BooksCorpus (800M words); BERT is trained on\nthe BooksCorpus (800M words) and Wikipedia (2,500M words).\n  \n  \n    GPT uses a sentence separator ([SEP]) and classifier token\n([CLS]) which are only introduced at fine-tuning time; BERT\nlearns [SEP], [CLS] and sentence A/B embeddings during\npre-training.\n  \n  \n    GPT was trained for 1M steps with a batch size of 32,000 words; BERT\nwas trained for 1M steps with a batch size of 128,000 words.\n  \n  \n    GPT used the same learning rate of 5e-5 for all fine-tuning\nexperiments; BERT chooses a task-specific fine-tuning learning\nrate which performs the best on the development set.\n  \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "Transformer-XL",
        "url"       : "/language-modeling/Transformer-XL",
        "date"      : "09/01/2019",
        "content": "Transformer-XL, stands for “Transformer Extra Long”, is a language model\npublished in this paper: “Transformer-XL: Attentive Language Models\nBeyond a Fixed-Length Context”\nby Google Brain in 2019.The official code for this paper can be found in the\nfollowing GitHub repository: transformer-xl\n.\n\nIn this paper, the authors are trying to increase the context-dependency scope.\nHence, the name of the paper: Transformer-XL Attentive Language Models Beyond a\nFixed-Length Context. A simple comparison between Transformer-XL and GPT and\nBERT can be summarized in the following figure:\n\n\n    \n\n\nIn the transformer architecture, we split the input paragraph into\nsentences, each sentence can’t exceed a certain length (it’s 512 in\nBERT). After splitting the paragraph into sentences or “segments”, then\nwe train our model as shown in the following image where we assume the\nallowed length is just four:\n\n\n    \n\n\nAs you can see, segment 2 is after segment 1 in the same paragraph. But\naccording to the transformer architecture, they are totally independent\nwhich causes another problem called “context fragmentation” where the\nmodel lacks the necessary contextual information to predict the first\nfew symbols due to the way the context was selected. Transformer-XL\nsolves this problem by providing a segment-level recurrence mechanism.\nAnd since transformer-XL uses larger context-dependency length, the\nauthors decided to use a different positional encoding than the vanilla\ntransformer.\n\nSo, the key innovations behind this paper can be summarized into two\nthings:\n\n\n  \n    Segment-level recurrence mechanism.\n  \n  \n    Relative positional encoding scheme.\n  \n\n\nSegment-level Recurrence\n\nThe goal of the recurrence mechanism is to enable long-term dependencies\nby using information from previous segments. Similarly to the vanilla\nversion, Transformer-XL processes the first segment of tokens but keeps\nthe outputs of the hidden layers. When the following segment is\nprocessed, each hidden layer receives two inputs:\n\n\n  \n    The output of the previous hidden layer of that segment, as in the\nvanilla version (the grey arrows in the chart below).\n  \n  \n    The output of the previous hidden layer from the previous segment\n(the green arrows) that allows the model to create long-term\ndependencies.\n  \n\n\nTechnically, the two inputs are concatenated and then used to calculate\nthe Key and the Value matrices of the (current Head of the current layer\nof the) current segment. This addition provides the network with more\ninformation in regards to the weights (importance) of each token, but it\ndoesn’t change the Value matrix.\n\n\n    \n\n\nIn each segment, each hidden layer receives the output of the previous\nhidden layer and the output of the previous segment. It increases the\nlargest possible dependency by using contextual information from several\nprevious segments.\n\n\n    \n\n\nThis mechanism can be applied at the decoding step with no problem as\nshown in the following figure:\n\n\n    \n\n\nRelative Positional Encoding\n\nNaively applying recurrence introduces another technical challenge. That\nis, the positional information is incoherent, and tokens from different\nsegments have the same positional encoding, which is referred to as\ntemporal confusion. To address this challenge,\nTransformer-XL employs novel relative positional encodings.\n\nIn the vanilla transformer, positional encodings were depending on the\nindex of the tokens. This positional encoding is depending on the\nrelative distance between tokens, hence the name: relative\npositional encoding. In the paper this was done by expanding the simple\nquery-key multiplication of the Attention Head’s Score.\n\nFirst, let’s recap what was the query-key multiplication in the\nattention mechanism of the vanilla transformer:\n\n\n    \n\n\nFollowing the idea of only relying on relative positional information,\nthey proposed to reparameterize the four terms as follows:\n\n\n    \n\n\nWith the following changes:\n\n\n  \n    The first change they made is to replace all appearances of the\nabsolute positional embedding $U_{j}$ for computing key vectors in\nterm $(b)$ and $(d)$ with its relative counterpart $R_{i - j}$ .\nNote that $R$ is a sinusoid encoding matrix without learnable\nparameters.\n  \n  \n    Secondly, we introduce a trainable parameter $u \\in \\mathbb{R}^{d}$\nto replace the query $U_{i}^{T}W_{q}^{T}$ in term $(c)$. In this\ncase, since the query vector is the same for all query positions,\nit suggests that the attentive bias towards different words should\nremain the same regardless of the query position.\n  \n  \n    With a similar reasoning, a trainable parameter\n$v \\in \\mathbb{R}^{d}$ is added to substitute $U_{i}^{T}W_{q}^{T}$\nin term $(d)$.\n  \n  \n    Finally, we deliberately separate the two weight matrices $W_{k,E}$\nand $W_{k,R}$ for producing the content-based key vectors and\nlocation-based key vectors respectively.\n  \n\n\nUnder the new parameterization, each term has an intuitive meaning:\n\n\n  \n    Term (a): represents content-based addressing.\n  \n  \n    Term (b): captures a content-dependent positional bias.\n  \n  \n    Term (c): governs a global content bias\n  \n  \n    Term (d): encodes a global positional bias.\n  \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "Adapter Layers",
        "url"       : "/language-modeling/adapter",
        "date"      : "02/02/2019",
        "content": "At the current moment, the norm in NLP involves downloading and\nfine-tuning pre-trained models consisting of hundreds of millions, or\neven billions of parameters. Modifying these models, no matter how\nsimple the modification is, requires re-training the whole model. And\nre-training these huge models is expensive, slow, and time-consuming,\nwhich impedes the progress in NLP. Adapters are one way to fix this\nproblem.\n\nAdapters, proposed in this paper: Parameter-efficient transfer learning\nfor NLP by Google Research in\n20019, are small learned bottleneck layers inserted within each layer of\na pre-trained models to avoid full fine-tuning of the entire model. To\ndemonstrate adapter’s effectiveness, researchers in the paper have\ntransferred BERT model to 26 diverse text classification tasks achieving\nnear state-of-the-art performance. The official code for this paper can\nbe found in Google’s research official GitHub repository:\nadapter-bert.\n\n\n    \n\n\nAdapter Tuning\n\nAdapter Tuning is considered a new technique for transfer learning.\nBefore that, There are two common transfer learning techniques in NLP:\n\n\n  \n    Feature-based Transfer Learning:\nIt involves pre-training real-valued embeddings vectors. These\nembeddings may be at the word, sentence, or paragraph level. The\nembeddings are then fed to custom downstream models.\n  \n  \n    Fine-tuning:\nFine-tuning involves copying the weights from a pre-trained network\nand tuning them on the downstream task. Recent work shows that\nfine-tuning often enjoys better performance than feature-based\ntransfer.\n  \n\n\nNow, let’s get into adapter tuning. Consider a function (neural network) with\nparameters $\\phi_{w}\\left( x \\right)$, adapter tuning defines a new function\n$\\phi_{w,v}\\left( x \\right)$ where $v$ is anew set of parameters. The initial\nvalue of the parameters $v_{0}$ is set such that the new function resembles\nthe original $\\phi_{w,v_{0}}\\left( x \\right) \\approx \\phi_{w}\\left( x \\right)$.\nDuring training, the $w$ parameters are frozen and only $v$ is tuned.\n\nThe following figure shows the transformer layer on the left and how we are\ngoing to set the adapter tuning to it on the right. As we can see, the\nadapter is always applied directly to the output of the sub-layer, after the\nfeed-forward and before adding the skip connection back:\n\n\n    \n\n\nTo sum up, adapter tuning is a transfer learning technique that attains neat to\nstate-of-the-art performance. During adapter tuning, we only train the adapter\nlayers unlike fine-tuning where we train some of the layers, usually the top\nones. The following figure shows the trade-off between accuracy and number\nparameters, for adapter tuning and fine-tuning. The y-axis represents the\nperformance normalized in comparison with full fine-tuning on nine tasks from\nthe GLUE benchmark.\n\n\n    \n\n\n\n  Note:\nDuring inference, the adapter modules may be ignored if not\nrequired. That is possible because they have near-identity\ninitialization with the parameters in the original neural network.\n\n\n\n    \n\n\nAdapter Layer\n\nHere, we are going to describe the design of the adapter layer. The\nadapter layer first projects the original d-dimensional features into a\nsmaller dimension $m$, apply a non-linearity, then project back to $d$\ndimensions. The adapter module itself has a skip-connection internally.\n\nThe bottleneck dimension, $m$, is the only hyper-parameter which\nprovides a simple means to tradeoff performance with number of added\nparameters. In practice, they use around $0.5:8\\%$ of the parameters of\nthe original model.\n\nThe total number of parameters added per layer, including biases, is\n$md + m$ in the feed-forward down-project and $md + d$ in the\nfeed-forward up-project. So, the total is:\n\n\\[2md + d + m\\]\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "GPT-2",
        "url"       : "/language-modeling/GPT-2",
        "date"      : "14/02/2019",
        "content": "GPT-2 stands for “Generative Pre-trained Transformer” which is a\nlanguage model published in this paper: “Language Models are\nUnsupervised Multitask\nLearners”\nby OpenAI in 2019. In the paper, they tried to demonstrate that language\nmodels can perform down-stream tasks such as (question answering,\nmachine translation, reading comprehension, and summarization) in a\nzero-shot setting – without any parameter or architecture modification.\n\nOne great way to experiment with GPT-2 is using the AllenAI GPT-2\nExplorer. It uses GPT-2 to\ndisplay ten possible predictions for the next word (alongside their\nprobability score). You can select a word then see the next list of\npredictions to continue writing the passage.\n\nWebText\n\nMost prior work trained language models on a single domain of text, such\nas news articles, Wikipedia, or fiction books. Their approach is to\nbuild as large and diverse a dataset as possible in order to collect\nnatural language demonstrations of tasks in as varied of domains and\ncontexts as possible.\n\nA promising source of that kind of data is web scrapes such as Common\nCrawl, but they have significant data quality\nissues which is that a large amount of documents content are mostly\nunintelligible. That’s why in this paper, they created a new dataset\ncalled WebText. This data contains text subset of around 45 million\nlinks from Reddit where each link received at least 3 karma which\nindicates whether other users found the link interesting or not. Also,\nall Wikipedia links were removed since it’s a common data source.\n\nModel\n\nThe model in this paper is the same as the one in GPT with a few\nmodifications:\n\n\n  \n    Layer normalization was moved to the input of each sub-block. And an\nadditional layer normalization was added after the final\nself-attention block.\n  \n  \n    A modified initialization, which accounts for the accumulation on\nthe residual path with model depth, is used. They scaled the\nweights of residual layers at initialization by a factor of\n$\\frac{1}{\\sqrt{N}}$ where $N$ is the number of residual layers.\n  \n  \n    The vocabulary is expanded to 50,257 instead of 40,000.\n  \n  \n    The context size is increased to 1024 instead of 512.\n  \n  \n    The batch size is increased to 512 instead of 64.\n\n    Also, they trained different versions of GPT-2 models: The smallest\nmodel is equivalent to the original GPT, and the second smallest\nequivalent to the largest model from BERT.The learning rate of each\nmodel was manually tuned for the best perplexity on a 5% held-out\nsample of WebText.\n  \n\n\n\n    \n\n\nFine-Tuning\n\nAs we said earlier, the purpose of this paper is to demonstrate that\nlanguage models can perform down-stream tasks such as (question\nanswering, machine translation, reading comprehension, and\nsummarization) in a zero-shot setting – without any parameter or\narchitecture modification.\n\nNow, let’s see how they did that with different tasks:\n\n\n  Machine Translation:\n\n\n\n    \n\n\n\n  Summarization:\n\n\n\n    \n\n\nResult Search\n\nIn order to produce good results when using our model, there are\nmultiple ways that we can search for the best result and they are:\n\n\n  \n    Exhaustive Search: Considering the whole vocabulary\n  \n  \n    Greedy Search: Considering the top option at each time-step.\n  \n  \n    Beam Serach: Considering the top N options at each time-step.\n  \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "MASS",
        "url"       : "/language-modeling/MASS",
        "date"      : "07/05/2019",
        "content": "MASS, stands for “Masked Sequence to Sequence”, is a\npre-training scheme proposed by Microsoft in 2019 and published in this\npaper: “MASS: Masked Sequence to Sequence Pre-training for Language\nGeneration” and the code is\npublicly available on Microsoft’s official account on\nGitHub. Inspired by BERT, MASS\nencoder takes a sentence with a masked fragment as input, and its\ndecoder predicts this masked fragment.\n\n\n    \n\n\nUnlike BERT which pre-trains only the encoder or decoder, MASS is\ncarefully designed to pre-train the encoder and decoder jointly in two\nsteps:\n\n\n  \n    By predicting the fragment of the sentence that is masked on the\nencoder side, MASS can force the encoder to understand the meaning\nof the unmasked tokens, in order to predict the masked tokens in the\ndecoder side.\n  \n  \n    By masking the input tokens of the decoder that are unmasked in the\nencoder side, MASS can force the decoder rely more on the source\nrepresentation other than the previous tokens in the target side for\nnext token prediction, better facilitating the joint training\nbetween encoder and decoder.\n  \n\n\n\n  Note:\nWhile this method works for any neural network based encoder-decoder\nframeworks, they chose Transformer considering that it achieves\nstate-of-theart performances in multiple sequence to sequence learning\ntasks.\n\n\nMasked Sequence\n\nIn the paper, they introduced a novel unsupervised prediction task where\nthey mask $k$ consecutive tokens in the source sentence. Given an\nunpaired source sentence $x \\in \\mathcal{X}$ , they denote $x^{u:v}$ as\na modified version of $x$ where the tokens from position $u$ to $v$ are\nmasked using the special symbol $\\left\\lbrack \\mathbb{M} \\right\\rbrack$\nwhere $0 &amp;lt; u &amp;lt; v &amp;lt; \\text{len}\\left( x \\right)$. They denote the unmasked\npart of $x$ as $x^{\\backslash u:v}$ In this case, the log likelihood is\nused as the objective function:\n\n\\[L\\left( \\theta;\\mathcal{X} \\right) = \\frac{1}{\\left| \\mathcal{X} \\right|}\\sum_{x \\in \\mathcal{X}}^{}{\\log\\left( P\\left( x^{u:v} \\middle| x^{\\backslash u:v};\\theta \\right) \\right)}\\]\n\nFor example in the following figure, we can see that the input sequence\nhas 8 tokens with the fragment\n$x^{3:6} = \\left\\{ x_{3},\\ x_{4},\\ x_{5},\\ x_{6} \\right\\}$ being masked.\nNote that the model only predicts the masked fragment, given only\n$\\left\\{ x_{3},\\ x_{4},\\ x_{5} \\right\\}$ as the decoder input for\nposition $4:6$, and the decoder takes the special mask symbol\n$\\left[ \\mathbb{M} \\right]$ as inputs for the other\npositions (e.g., position $1:3$ and $7:8$.\n\n\n    \n\n\nThe start position $u$ is chosen randomly. The same as BERT, the masked\ntokens in the encoder will be replaced by:\n\n\n  \n    The $\\left\\lbrack \\mathbb{M} \\right\\rbrack$ token about 80% of the\ntime.\n  \n  \n    A random token 10% of the time.\n  \n  \n    Remains unchanged 10% of the time.\n  \n\n\nStudy of Different k\n\nThe length of the masked fragment $k$ is an important hyper-parameter of\nMASS and they explored different values of $k$ from 10% to 90%\npercentage of the sentence length $m$ with a step size of 10%. They\nfound out that the best value for k is around 50% of the sentence length\n$m$ in multiple pre-training and fine-tuning tasks.\n\n\n    \n\n\nActually, the masked language modeling in BERT and the standard language\nmodeling in GPT can be viewed as special cases of MASS. The following\ntable shows how tuning the hyper-parameter $k$ can convert MASS to\neither BERT or OpenAI GPT:\n\n\n    \n\n\nPre-training\n\nWe choose Transformer as the basic model structure, which consists of\n6-layer encoder and 6-layer decoder with 1024 embedding/hidden size and\n4096 feed-forward filter size. Since MASS is a pre-training method\nmainly for language generation, the pre-training method changes based on\nthe fine-tuning task:\n\n\n  \n    For neural machine translation task:\nThey pre-trained MASS on the monolingual data of the source and\ntarget languages. They conducted experiments on three language\npairs: English-French, English-German, and English-Romanian. To\ndistinguish between the source and target languages, they added a\nlanguage embedding to each token of the input sentence for the\nencoder and decoder, which is also learned end-to-end. Also, they\nused a vocabulary of 60,000 sub-word units with Byte-Pair Encoding\nbetween source and target languages\n  \n  \n    For text summarization &amp;amp; conversational response generation:\nThey pre-trained the model with only English monolingual data.\n  \n\n\nAll of the monolingual data used in this pre-training are from WMT News\nCrawl datasets, which covers 190M, 62M and 270M sentences from year 2007\nto 2017 for English, French, German respectively. Also, they used all of\nthe available Romanian sentences from News Crawl dataset and augment it\nwith WMT16 data, which results in 2.9M sentences.\n\nFine-tuning\n\nIn this section, we are going to discuss the performance of MASS over\nvarious tasks such as:\n\n\n  Unsupervised NMT:\nFor unsupervised NMT, we use only monolingual data to train\nMASS with back-translation (no bilingual data). And the following\ntable shows the results of MASS (fine-tuned using Adam optimizer\nwith initial learning rate $10^{- 4}$ and the batch size is set as\n2000 tokens for each GPU) on newstest2014 for English-French, and\nnewstest2016 for English-German and English-Romanian:\n\n\n\n    \n\n\n\n  Low-resource NMT:\nIn the low-resource NMT setting, we respectively sample 10K,\n100K, 1M paired sentence from the bilingual training data of WMT14\nEnglish-French, WMT16 English-German and WMT16 English-Romanian. The\nfollowing table shows the performance of MASS (fine-tuned for 20,000\nsteps with Adam optimizer and the learning rate is set as 10−4) on\nthe same testsets used in the unsupervised setting; The baseline\nmodel here is MASS but without pre-training.\n\n\n\n    \n\n\n\n  Text Summarization:\nText summarization is the task of creating a short and fluent\nsummary of a long text document, which is a typical sequence\ngeneration task. We fine-tune the pre-trained model on text\nsummarization task with different scales (10K, 100K, 1M and 3.8M) of\ntraining data from the Gigaword corpus, which consists of a total of\n3.8M article-title pairs in English. We take the article as the\nencoder input and title as the decoder input for fine-tuning. We\nreport the F1 score of ROUGE-1, ROUGE2 and ROUGE-L on the Gigaword\ntestset during evaluation. We use beam search with a beam size of 5\nfor inference. The baseline here is MASS but without pre-training:\n\n\n\n    \n\n\n\n  Conversational Response Generation:\nConversational response generation generates a flexible response for\nthe conversation. We conduct experiments on the Cornell movie dialog\ncorpus that contains 140K conversation pairs. We randomly sample\n10K/20K pairs as the validation/test set and the remaining data is\nused for training. We adopt the same optimization hyper-parameters\nfrom the pre-training stage for fine-tuning:\n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "XLNet",
        "url"       : "/language-modeling/XLNet",
        "date"      : "19/06/2019",
        "content": "XLNet stands for “Extra-Long Net” which is a model that integrates both\nGPT and BERT introduced in 2019 by Google Brain and published in this\npaper: “XLNet: Generalized Autoregressive Pretraining for Language\nUnderstanding” by the same\nauthors of Transformer-XL. The official code for this paper can be found in\nthe following GitHub repository: xlnet.\n\nIn Transformer-XL, they extended the context-dependency length by\nintroducing the segment-level recurrence mechanism which uses the hidden\nstate of the former segments when predicting the current segment. In\nthis paper, they are trying to make the model uses the hidden state of\nthe former and following segments when predicting the current segment.\n\nAnd that explains the meaning of the paper’s name. An auto-regressive\nlanguage model is is a language model that is able to predict the next\npossible word based on the before-context or predict the previous word\nbased on the after-context. And it’s generalized because it considers\nboth; the before-context and the after-context. The way to do that as\nproposed by the paper is by using “Permutation Language Modeling”.\n\nPLM\n\nPLM stands for “Permutation Language Modeling” which is the idea of\ncapturing bidirectional context by training an auto-regressive model on\nall possible permutation of words in a sentence. Instead of a fixed\nleft-right or a right-left modeling, XLNET maximizes expected log\nlikelihood over all possible permutations of the sequence which means\nthat each position will learn to utilize contextual information from all\npositions thereby capturing bidirectional context.\n\nThis mechanism is better than “MLM (Masked Language Modeling)” used with\nBERT. And that’s because MLM corrupts the input with masks which affects\nreal life applications since we do not have inputs that are masked.\nAlso, MLM ignores the relation between masked tokens. For example, let’s\nconsider this sentence: “She met [mask] and [mask] friends. So, if\nthe first [mask] is “Adam”, then the second [mask] has to be\n“his”. And this will change when the first [mask] is “Sara” for\nexample.\n\nSo, consider a sequence\n$x = \\left[ “This”,\\ “is”,\\ “a”,\\ “sentence” \\right]$ with $T = 4$\ntokens. Now consider the set of all $4!$ permutations\n$Z = \\left\\{ z_{1},\\ z_{2},\\ …z_{4!} \\right\\} =\n{\\lbrack 1,\\ 2,\\ 3,\\ 4\\rbrack,\\ \\lbrack 1,\\ 2,\\ 4,\\ 3\\rbrack,.\\ .\\ .,\\ \\lbrack 4,\\ 3,\\ 2,\\ 1\\rbrack}$.\n\nThe XLNet model calculates the probability of token $x_{t}$ given\npreceding tokens $x_{&amp;lt; t}$ from any order which makes the objective\nfunction as follows:\n\n\\[\\max_{\\theta}\\left( \\mathbb{E}_{z\\sim Z_{T}}\\left\\lbrack \\sum_{t = 1}^{T}{\\log\\left( p_{\\theta}\\left( x_{z_{t}} \\middle| x_{z_{&amp;lt; t}} \\right) \\right)} \\right\\rbrack \\right)\\]\n\nSo, if $t = 3$ and the current permutation is\n$z = \\lbrack 3,\\ 2,\\ 4,\\ 1\\rbrack$, it means XLNet will consider zero\nwords $x_{z_{&amp;lt; t}} = \\lbrack\\rbrack$ when predicting the probability of\nthe third word which corresponds to\n$p_{\\theta}\\left( “a” \\middle| \\varnothing \\right)$. While if the\ncurrent permutation is $z = \\lbrack 2,\\ 4,\\ 3,\\ 1\\rbrack$, it means\nXLNet will consider the second and the fourth words\n$x_{z_{&amp;lt; t}} = \\lbrack 2,\\ 4\\rbrack$ when predicting the probability of\nthe third word which corresponds to\n$p_{\\theta}\\left( “a” \\middle| “is”,\\ “sentence” \\right)$.\n\nAs you have probably figured out, there is something missing from the\nway the model has been presented so far: how does the model know about\nword order? The model can compute\n$p_{\\theta}\\left( “This” \\middle| “a” \\right)$ as well as\n$p_{\\theta}\\left( “This” \\middle| “is” \\right)$. Ideally it should know\nsomething about the relative position of “This” and “is” and also of\n“a”. Otherwise it would just think all tokens in the sequence are\nequally likely to be next to one-another. And that’s what the attention\nmask does!\n\nAttention Mask\n\nThe transformer architecture addresses this problem by adding\nmasking/zeroing the words that are not in the provided context. As a\nconcrete example, consider the following permutation\n$z = \\lbrack 3,\\ 2,\\ 4,\\ 1\\rbrack$. When calculating the probability of\nthe $1^{st}$ element in that order, the model has no context as the other\ntokens have not yet been seen. So the mask would be\n$\\lbrack 0,\\ 0,\\ 0,\\ 0\\rbrack$ as shown below:\n\n\n    \n\n\nFor the 2nd element (token 2), the mask is\n$\\lbrack 0,\\ 0,\\ 1,\\ 0\\rbrack$ as its only context is token 3. Following\nthat logic, the $3^{rd}$ and $4^{th}$ elements (tokens 4 and 1) have masks\n$\\lbrack 0,\\ 1,\\ 1,\\ 0\\rbrack$ and $\\lbrack 0,\\ 1,\\ 1,\\ 1\\rbrack$\nrespectively as shown in the following figures:\n\n\n    \n\n\nAnother way to look at this is that the training objective will contain\nthe following terms in case of the $z = \\lbrack 3,\\ 2,\\ 4,\\ 1\\rbrack$\npermutation; where underscores represent what has been masked:\n\n\\[p_{\\theta}\\left( &quot;a&quot; \\middle| \\_\\_,\\ \\_\\_,\\ \\_\\_,\\ \\_\\_ \\right)\\]\n\n\\[p_{\\theta}\\left( &quot;is&quot; \\middle| \\_\\_,\\ \\_\\_,\\ &quot;a&quot;,\\ \\_\\_ \\right)\\]\n\n\\[p_{\\theta}\\left( &quot;sentence&quot; \\middle| \\_\\_,\\ &quot;is&quot;,\\ &quot;a&quot;,\\ \\_\\_ \\right)\\]\n\n\\[p_{\\theta}\\left( &quot;This&quot; \\middle| \\_\\_,\\ &quot;is&quot;,\\ &quot;a&quot;,\\ &quot;sentence&quot; \\right)\\]\n\nBut wait a minute! There remains one oversight to address: As you can\nsee, the probability of “sentence” in $4^{th}$ position the previous\npermutation should be different than when “sentence” is in the $1^{st}\nposition. In other words, we need to use the current word position when\ncalculating the probability; like so:\n\n\\[p_{\\theta}\\left( &quot;sentence&quot; \\middle| \\_\\_,\\ &quot;is&quot;,\\ &quot;a&quot;,\\ 4 \\right)\\]\n\nAnd this paper deals with problem by providing a “two-stream” self\nattention mechanism.\n\nTwo-stream Self-Attention\n\nThe solution to this problem is a two-stream self-attention mechanism;\nwhere the standard self-attention is divided into two parts or streams:\n\n\n  \n    Content Stream: The content stream (denoted by $h$) cares about\nthe context of the preceding tokens including the current token,\nin other words the “content’.\n  \n  \n    Query Stream: The query stream (denoted by $g$) cares about the\ncontext of the preceding tokens including the position of the\ncurrent token.\n  \n\n\nThe following figure shows just two layers of this two-stream self-attention:\n\n\n    \n\n\nContent Stream\n\nThe content vector of a token at position $i$ and at self-attention\nlayer $m$ is denoted by $h_{i}^{m}$. All content stream vectors are\ninitialized with token embeddings. It’s calculated according to the\nfollowing formula:\n\n\\[p_{\\theta}\\left( x \\middle| x_{z_{&amp;lt; t}} \\right) = \\frac{\\exp\\left( {e(x)}^{T}.h_{\\theta}\\left( x_{z_{&amp;lt; t}} \\right) \\right)}{\\sum_{x&#39;}^{}{\\exp\\left( {e(x&#39;)}^{T} \\right).h_{\\theta}\\left( x_{z_{&amp;lt; t}} \\right)}}\\]\n\nWhere:\n\n\n  \n    $x$: is the current token at position $t$ in the current permutation$z$.\n  \n  \n    $e(x)$: is the word embedding of the current token.\n  \n  \n    $x_{z_{&amp;lt; t}}$: is the preceding tokens to the current one.\n  \n  \n    $h_{\\theta}\\left( x_{z_{&amp;lt; t}} \\right)$: denotes the hidden\nrepresentation of $x_{z_{&amp;lt; t}}$ produced by the shared Transformer\nnetwork after proper masking.\n  \n\n\nConsidering the $z = \\lbrack 3,\\ 2,\\ 4,\\ 1\\rbrack$ permutation, at each\nlayer, the content vector $h_{i}$ is updated using the other context\nvectors that remained unmasked and itself. Thus, $h_{1}$ is\nupdated with the knowledge of $x_{3}$, $x_{2}$ , $x_{4}$ and $x_{1}$ as\nshown in the following figure:\n\n\n    \n\n\nAnd the following figure shows that all content vectors are contributing\nin calculating the key-value pair in the attention mechanism, while the\ncurrent content vector is used for the query vector of the attention\nmechanism.\n\n\n    \n\n\n\n  Note:\nThe content stream is the same as the standard self-attention found in\nthe vanilla transformer architecture.\n\n\nQuery Stream\n\nThe query vector of a token at position $i$ and at self-attention layer\n$m$ is denoted by $g_{i}^{m}$. All query stream vectors are initialized\nwith a generic embedding vector $w$ added to positional embeddings. Note\nthat $w$ is the same no matter the token. It’s calculated according to\nthe following formula:\n\n\\[p_{\\theta}\\left( x \\middle| x_{z_{&amp;lt; t}} \\right) = \\frac{\\exp\\left( {e(x)}^{T}.g_{\\theta}\\left( x_{z_{&amp;lt; t}},\\ z_{t} \\right) \\right)}{\\sum_{x&#39;}^{}{\\exp\\left( {e(x&#39;)}^{T} \\right).g_{\\theta}\\left( x_{z_{&amp;lt; t}},\\ z_{t} \\right)}}\\]\n\nWhere:\n\n\n  \n    $x$: is the current token at position $t$ in the current permutation $z$.\n  \n  \n    $e(x)$: is the word embedding of the current token.\n  \n  \n    $x_{z_{&amp;lt; t}}$: is the preceding tokens to the current one.\n  \n  \n    $g_{\\theta}\\left( x_{z_{&amp;lt; t}},\\ z_{t} \\right)$: denotes the hidden\nrepresentation of $x_{z_{&amp;lt; t}}$ produced by the shared Transformer\nnetwork after proper masking which additionally take the target\nposition $z_{t}$ as input.\n  \n\n\nConsidering the same permutation in the content stream\n$z = \\lbrack 3,\\ 2,\\ 4,\\ 1\\rbrack$, at each layer, the query\nvector $g_{i}$ is updated using the other context vectors that remained\nunmasked and itself. Thus, $h_{1}$ is updated with the\nknowledge of $x_{3}$, $x_{2}$ , $x_{4}$ and $w_{1}$ without considering\n$x_{1}$ as shown in the following figure:\n\n\n    \n\n\nAnd the following figure shows that all content vectors (except the\ncurrent one) are contributing in calculating the key-value pair in the\nattention mechanism, while the current query vector is used for the\nquery vector of the attention mechanism.\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "SpanBERT",
        "url"       : "/language-modeling/SpanBERT",
        "date"      : "24/07/2019",
        "content": "SpanBERT is a model created by Facebook AI and Allen Institute in\nJanuary 2019 and published in this paper “SpanBERT: Improving\nPre-training by Representing and Predicting\nSpans”. SpanBERT is just an\nextension to BERT where it better represents and predict continuous\nrandom spans of text, rather than random tokens. This is crucial since\nmany NLP tasks involve spans of text rather than single tokens. SpanBERT\nis different from BERT in both the masking scheme and the training\nobjectives:\n\n\n  \n    Span Masking:\nSpanBERT masks random contiguous spans, rather than random\nindividual tokens which forces the model to predict entire spans\nsolely using the context in which they appear.\n  \n  \n    SBO:\nSpanBERT uses a novel span-boundary objective (SBO) so the\nmodel learns to predict the entire masked span from the observed\ntokens at its boundary which encourages the model to store this\nspan-level information at the boundary tokens, which can be easily\naccessed during the fine-tuning stage.\n  \n  \n    No NSP:\nSpanBERT doesn’t use the NSP objective unlike BERT.\n  \n\n\n\n    \n\n\nSpan Masking\n\nGiven a sequence of tokens X, they selected a subset of tokens by\niteratively sampling spans of text until the masking budget (e.g. 15% of\nX) has been spent. And they following the following steps when masking a\nsubset of tokens:\n\n\n  They randomly sample a span length (number of words) from a\ngeometric distribution\n$\\ell \\sim Geo(p) = p.\\left( 1 - p \\right)^{n - 1}$ where\n$p = 0.2$ and $\\ell_{\\max} = 10$ which is skewed towards shorter\nspans as shown in the following figure. :\n\n\n\n    \n\n\n\n  \n    Then, they randomly select the starting point for the span to be\nmasked from a uniform distribution. They always sample a sequence\nof complete words (instead of subword tokens) and the starting\npoint must be the beginning of one word.\n  \n  \n    As in BERT, they also masked 15% of the tokens in total: replacing\n80% of the masked tokens with [MASK], 10% with random tokens and\n10% with the original tokens.\n  \n\n\nSBO\n\nSpan selection models typically create a fixed-length representation of\na span using its boundary tokens (start and end). To support such\nmodels, we would ideally like the representations for the end of the\nspan to summarize as much of the internal span content as possible. We\ndo so by introducing a Span Boundary\nObjective (SBO) that involves predicting each token of a\nmasked span using only the representations of the observed tokens at the\nboundaries.\n\nFormally, they calculated the SBO loss function by following these\nsteps:\n\n\n  \n    Given an input sequence of $X = x_{1},\\ …,\\ x_{n}$ and a masked\nspan of tokens $\\left( x_{s},…,\\ x_{e} \\right) \\in Y\\ $, where\n$\\left( s,\\ e \\right)$ indicates its start and end positions\nrespectively.\n  \n  \n    They represented each token $x_{i}$ in the span using the output\nencodings of the external boundary tokens $x_{s - 1}$ and\n$x_{e + 1}$, as well as the position embedding of the target token\n$p_{i - s + 1}$:\n  \n\n\n\\[h_{0} = \\left\\lbrack x_{s - 1};x_{e + 1};p_{i - s + 1} \\right\\rbrack\\]\n\n\n  Then, they implemented the representation function as a 2-layer\nfeed-forward network with GeLU activations and layer normalization\n\n\n\\[h_{1} = \\text{LayerNorm}\\left( \\text{GeLU}\\left( W_{1}.h_{0} \\right) \\right)\\]\n\n\\[y_{i} = \\text{LayerNorm}\\left( \\text{GeLU}\\left( W_{2}.h_{1} \\right) \\right)\\]\n\n\n  Finally, they used the vector representation $y_{i}$ to predict the\ntoken $x_{i}$ and compute the cross-entropy loss exactly like the\nMLM objective.\n\n\n\\[\\mathcal{L}_{\\text{SBO}}\\left( x_{i} \\right) = - log\\ P\\left( x_{i}\\  \\middle| \\ y_{i} \\right)\\]\n\nFor example, given the following sequence “Super Bowl 50 was an American\nfootball game to determine the champion” where the span “an American\nfootball game” is masked. The span boundary objective (SBO) uses the\noutput representations of the boundary tokens, x4 and x9 (in blue), to\npredict each token in the masked span.\n\n\n    \n\n\nThe equation shows the MLM and SBO loss terms for predicting the token,\nfootball (in pink), which as marked by the position embedding $p_{3}$,\nis the third token from $x_{4}$.\n\n\\[\\mathcal{L}\\left( \\text{football} \\right) = \\mathcal{L}_{\\text{MLM}}\\left( \\text{football} \\right) + \\mathcal{L}_{\\text{SBO}}\\left( \\text{football} \\right) = - log\\ P\\left( \\text{football} \\middle| \\ x_{7} \\right) - log\\ P\\left( \\text{football} \\middle| \\ x_{4},x_{9},p_{3} \\right)\\]\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "RoBERTa",
        "url"       : "/language-modeling/RoBERTa",
        "date"      : "26/07/2019",
        "content": "RoBERTa, stands for “Robustly optimized BERT approach”,\nis an approach to train BERT created by Facebook AI in 2019 and\npublished in this paper: “RoBERTa: A Robustly Optimized BERT\nPretraining Approach”. The official code\nfor this paper can be found on Facebook’s FairSeq official GitHub repository:\nfairseq/roberta.\n\n\n    \n\n\nThe authors of this paper has found out that BERT, when published, was\nsignificantly under-trained. So, they wrote this paper introducing an\napproach with the following:\n\n\n  \n    Training the BERT model longer, on longer sequences, with bigger\nbatches, over more data which increased the character-level BPE\nvocabulary size from 30K to 50K.\n  \n  \n    Dynamically changing the masking pattern applied to the training data.\n  \n  \n    Removing the next sentence prediction (NSP) objective.\n  \n\n\nAnd just by doing these simple modifications, RoBERTa was able to\nexceed BERT and XLNet in almost all tasks on GLUE:\n\n\n    \n\n\nIn the following sections, we are going to discuss each modification out\nof the proposed three modifications.\n\nData\n\nBERT is trained on a combination of BooksCorpus and English Wikipedia\nwhich totals 16GB of uncompressed text. On the other hand, RoBERTa is\ntrained on a combination of the following data which totals 160 GB of\nuncompressed text:\n\n\n  \n    BooksCorpus and English\nWikipedia (This is the original data used to train BERT).\n  \n  \n    CC-NEWS:\ncollected from the English portion of the CommonCrawl News\ndataset. The data contains 63 million English news articles\ncrawled between September 2016 and February 2019. (76GB after\nfiltering).\n  \n  \n    OpenWebText: it’s\nthe web content extracted from URLs shared on Reddit with at least\nthree upvotes. (38GB).\n  \n  \n    Stories: a dataset containing a subset of CommonCrawl data filtered\nto match the story-like style of Winograd schemas. (31GB).\n  \n\n\nThe following table contains a simple comparison between RoBERTa and\nBERT using different amount of data:\n\n\n    \n\n\nBatch Size\n\nPast work in NMT has shown that training with very large mini-batches\ncan both improve optimization speed and end-task performance when the\nlearning rate is increased appropriately. BERT is also amenable to large\nbatch training.\n\nThe original BERT model was trained for 1 million steps with batch size\nof 256 sequences. So, in this paper the publishers increased the batch\nsize and compared the performance of BERT on the development set of\nBooksCorpus and English Wikipedia as shown in the following table:\n\n\n    \n\n\nWe can see that training with large batches improves perplexity for the\nmasked language modeling objective, as well as end-task accuracy. Large\nbatches are also easier to parallelize via distributed data parallel\ntraining.\n\nAnd that was the first modification in the paper which is increasing the\namount of data used for training BERT with bigger batch sizes. Now,\nlet’s get to the second one.\n\nStatic Vs. Dynamic Masking\n\nAs discussed before, BERT relies on randomly masking and predicting\ntokens. The original BERT implementation performed masking once during\ndata preprocessing, resulting in a single static mask.\n\nIn the paper, they proposed two different techniques for masking:\n\n\n  \n    Static Masking: Where they duplicate training data\n10 times and mask each time with different mask pattern.\n  \n  \n    Dynamic Masking: Where they generated the masking\npattern every time a sequence is fed to the model.\n  \n\n\nAnd the following table shows the result in comparison with the official\nresults from BERT where we can see clearly that both proposed methods before\nbetter than the single masking:\n\n\n    \n\n\nNSP\n\nIn the original BERT paper, the model is trained to predict whether the\nobserved sentence is next to the previous sentence or not via an\nauxiliary Next Sentence Prediction (NSP) loss. The NSP loss was\nhypothesized to be an important factor in training the original BERT\nmodel and removing it hurts the performance. However, some recent work\nhas questioned the necessity of the NSP loss.\n\nSo, to put this issue to rest, the publishers of the paper compared\nseveral alternative training formats:\n\n\n  \n    Segment-pair + NSP:\nThis follows the original input format used in BERT. Each input\ncontains a pair of segments; each segment could contain multiple\nsentences.\n  \n  \n    Sentence-pair + NSP:\nEach input contains a pair of sentences.\n  \n  \n    Full-sentences:\nEach input is packed with full sentences, such that the\ntotal length is at most 512 tokens. Inputs may cross document\nboundaries. When we reach the end of one document, we begin\nsampling sentences from the next document and add an extra\nseparator token between documents. The NSP loss is removed.\n  \n  \n    Doc-sentences:\nEach input is packed with sentences of the same document, such\nthat the total length is at most 512. The NSP loss is also\nremoved.\n  \n\n\nAnd the following table contains a comparison of these different\ntraining format on four different tasks.\n\n\n    \n\n\nAnd in the table, we can see the following:\n\n\n  \n    We find that using individual sentences hurts performance, which\nthey hypothesized is because the model is not able to learn\nlong-range dependencies.\n  \n  \n    Removing the NSP loss matches or slightly improves the performance.\n  \n  \n    Restricting sequences to come from a single document\n(Doc-sentences) performs slightly better than packing\nsequences from multiple documents (full-sentences).\n  \n\n\nBase Vs Large RoBERTa\n\nThe same as BERT, the published or RoBERTa created two models sizes for\nit in order to compare performance:\n\n\n  \n    RoBERTa BASE: Comparable in size to the BERT-base.\n  \n  \n    RoBERTa LARGE: Comparable in size to BERT-large.\n  \n\n\nThe following summarizes the difference between both models:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "Big Models pollute Earth",
        "url"       : "/language-modeling/pollution",
        "date"      : "02/08/2019",
        "content": "Recent progress in hardware and methodology for training neural networks\nhas ushered in a new generation of large networks. These models have\nobtained notable gains in accuracy across many NLP tasks. However, these\naccuracy improvements depend on the availability of exceptionally large\ncomputational resources that necessitate similarly substantial energy\nconsumption. As a result, these models are costly to train both\nfinancially and environmentally.\n\nThis paper: Energy and Policy Considerations for Deep Learning in\nNLP, published in 2019 by the\nUniversity of Massachusetts Amherst, is explaining that these big models\nare costly environmentally due to the carbon footprint required to\ngenerate electricity as it’s the main source of electricity in the top\nthree cloud service providers according to the following table:\n\n\n    \n\n\nTo heighten the awareness of the NLP community to this issue,\nresearchers of this paper have characterized the dollar cost and carbon\nemissions that result from training popular off-the-shelf NLP models.\nThey have done that by estimating the kilowatts of electrical energy\ngenerated to power the required hardware which can be calculated via the\nfollowing formula:\n\n\\[p_{t} = \\frac{1.58t\\left( p_{c} + p_{r} + gp_{g} \\right)}{1000}\\]\n\nWhere:\n\n\n  \n    $p_{t}$: total power consumption (in watts).\n  \n  \n    $p_{c}$: the average power draw (in watts) from all CPU sockets\nduring training.\n  \n  \n    $p_{r}$: the average power draw from all DRAM (main memory) sockets\nduring training.\n  \n  \n    $p_{g}$: the average power draw of one GPU during training. For $g$\nGPUs, the average power draw will be $gp_{g}$.\n  \n  \n    $1.58t$: the Power Usage Effectiveness (PUE), which accounts for the\nadditional energy required to support the compute infrastructure\n(mainly cooling). In 2018 (one year before the publish date of this\npaper), the global average PUE for data centers was 1.58 per hour.\n  \n  \n    $\\frac{1}{1000}$: to convert the power to kilo watt.\n  \n\n\nAccording to EPA (Environmental Protection Agency) in 2018, each one kilowatt\nproduces 0.95 pound of CO~2~. Then, the average CO~2~emissions will be:\n\n\\[Co_{2}e = 0.95\\ p_{t}\\]\n\nIn the paper, they analyzed five different models as shown in the\nfollowing table. These four models were BERT, ELMO, standard Transformer\n(T2T), Evolved Transformer (NAS), and finally GPT-2. The following table\nshows the estimated cost of training a model in terms of CO~2~ emissions\n(lbs) and cloud compute cost (USD) according to the previous equations:\n\n\n    \n\n\nBased on the previous table and knowing that an air travel from New York\nto San Francisco emits around 1984 lbs of CO~2~, we can clearly see that\ntraining BERT~base~ for around 90 hours will have the same effect. And\ndon’t get me started on the fact that training NAS on 8 Tesla P100 GPUs\nemits the same CO~2~ as approximately 315 flights from New York to San\nFrancisco.\n\nYou can use this tool:\nexperiment-impact-tracker\nto track energy usage, carbon emissions, and compute utilization of your\nsystem.\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "StructBERT",
        "url"       : "/language-modeling/StructBERT",
        "date"      : "13/08/2019",
        "content": "StructBERT stands for “Structural BERT” which is an extension of\nBERT created by\nincorporating language structures into pre-training. StructBERT was\nproposed in 2019 by Alibaba Group and published in their “StructBERT:\nIncorporating Language Structures Into Pre-Training For Deep Language\nUnderstanding” paper. The\nofficial code for this paper can be found in the following GitHub\nrepository:\nalibaba/StructBERT.\n\nStructBERT incorporates the language structures into BERT by\npre-training BERT with two tasks/objectives that make the most use of\nthe sequential order of words and sentences along with the Masked\nLanguage Modeling (MLM) objective. These two objectives are:\n\n\n  \n    Word Structural Objective.\n  \n  \n    Sentence Structural Objective.\n  \n\n\nMLM Recap\n\nBefore getting into more details about StructBERT, let’s first recap how\nMLM objective worked in\nBERT. Given an\ninput sequence, 15% of the tokens in that sequence are replaced with a\n[MASK] token; and the type of mask will be different according to the\nfollowing distribution\n\n\n  \n    80% of the time: the mask will be [MASK].\n  \n  \n    10% of the time: the mask will be a random word.\n  \n  \n    10% of the time: The mask will be the original word.\n  \n\n\nThen, BERT will have to learn to predict these masked tokens correctly while\npre-training.\n\n\n    \n\n\nWord Structural Objective\n\nThis new word objective is jointly trained together with the original\nMLM objective; the new objective takes the word order into\nconsideration. The way this objective works is that they randomly choose\n$5\\%$ of the unmasked trigrams to be shuffled and then StructBERT is\npre-trained to predict the original order of the tokens. As shown in the\nfollowing figure, the trigram (t2, t3, and t4) was shuffled to (t3, t4,\nand t2).\n\n\n    \n\n\nGiven a randomly shuffled span of $K$ tokens (trigram means $K = 3$),\nthe word structural objective is equivalent to maximizing the likelihood\nof placing every shuffled token in its correct position. So, given a set\nof trainable parameters $\\theta$, this objective can be defined as:\n\n\\[\\underset{\\theta}{\\text{arg}\\max}{\\sum_{}^{}{\\log\\left( P\\left( \\text{pos}_{1} = t_{1},\\ \\text{... pos}_{K} = t_{K} \\middle| t_{1},\\ ...t_{K}; \\theta \\right) \\right)}}\\]\n\nThey studied the effect of this objective in comparison with MLM in BERT\nduring self-supervised pre-training. The following figure illustrates\nthe loss (left) and accuracy (right) of word and sentence prediction\nover the number of pre-training steps for StructBERT-Base and BERT-Base:\n\n\n    \n\n\nWe can see that the the shuffled token prediction objective (blue line)\nled to lower loss and higher accuracy more than MLM (red line).\n\nSentence Structural Objective\n\nThe Next Sentence Prediction (NSP) task is\nconsidered easy for the original BERT model (the prediction accuracy of\nBERT can easily achieve 97%-98% in this task). Therefore, they extended\nthe NSP task by considering both the next sentence and the previous\nsentence to make the pre-trained language model aware of the sequential\norder of the sentences in a bidirectional manner.\n\nAs illustrated in the following figure, they sampled the data for this\ntask in pairs $\\left( S_{1},\\ S_{2} \\right)$ where the two sentences are\nconcatenated together with the separator token [SEP] in between, as\ndone in BERT. Given a first sentence $S_{1}$, they sampled $S_{2}$ to\nbe:\n\n\n  \n    The next sentence $\\frac{1}{3}$ of the time.\n  \n  \n    The previous sentence $\\frac{1}{3}$ of the time.\n  \n  \n    A random sentence from the data $\\frac{1}{3}$ of the time.\n  \n\n\n\n    \n\n\nAnd given the input sequence, the model has to predict whether $S_{2}$\nis the next sentence that follows $S_{1}$ (class label=1), or the\nprevious sentence that precedes $S_{1}$ (class label=2), or a random\nsentence from a different document (class label=0).\n\nThey studied the effect of this objective during self-supervised\npre-training. The following figure illustrates the loss (left) and\naccuracy (right) of word and sentence prediction over the number of\npre-training steps for StructBERT-Base and BERT-Base:\n\n\n    \n\n\nAs we can see, the new sentence structural objective in StructBERT leads\nto a more challenging prediction task than that in BERT enabling\nStructBERT to exploit inter-sentence structures, which benefits\nsentence-pair downstream tasks.\n\nExperiments\n\nIn this paper, they pre-trained StructBERT on documents from English\nWikipedia (2,500M words) and BookCorpus using WordPiece models. The\nmaximum length of input sequence was set to 512. They used Adam\noptimizer ($\\beta_{1} = 0.9,\\ \\beta_{2} = 0.999$). They used L2 weight\ndecay regularization of $0.01$. The learning rate was set to $1e^{- 4}$\nwith warm-up over the first 10% of the total steps, and linear decay at\nthe rest. The dropout probability was set to $0.1$ for every layer. They\nused GeLU activation. They pre-trained two model sizes:\n\n\n\n    \n        \n            \n            $$N$$\n            $$d_{\\text{ff}}$$\n            $$h$$\n            # parameters\n            Hardware\n        \n    \n    \n        Base\n        12\n        768\n        12\n        110 M\n        64 GPU V100 + 38 hours\n    \n    \n        Large\n        24\n        1024\n        16\n        340 M\n        64 GPU V100 + 7 days\n    \n\n\n\nNow, we are going to see the performance of StructBERT over multiple\ndownstream tasks:\n\n\n  Natural Language Understanding:\nThe following table shows the results of StructBERT on the\nGLUE test set, which are scored by the GLUE evaluation server. The\nnumber below each task denotes the number of training examples. The\nstate-of-the-art results are in bold. All the results are obtained\nfrom the leaderboard\n(StructBERT submitted under a different model name ALICE):\n\n\n\n    \n\n\n\n  Natural Language Inference:\nThe following table shows the accuracy of multiple models on SNLI\ndataset. As seen from the table, StructBERT outperformed all\nexisting systems on SNLI, creating new state-of-the-art results\n91.7%, which amounts to 0.4% absolute improvement over the previous\nstate-of-the-art model SJRC and 0.9% absolute improvement over BERT.\n\n\n\n    \n\n\n\n  Question Answering:\nThe following table shows the results of SQuAD dataset where\nwe can see that StructBERT model is superior to all other models\nexcept\nXLNet+DA. It\ndemonstrates the effectiveness of StructBERT in modeling the\nquestion-paragraph relationship.\n\n\n\n    \n\n\nTo study the effect of the new pre-training tasks over fine-tuning, they\nperformed ablation study using StructBERT-Base architecture on six\ndifferent downstream tasks as shown in the following table:\n\n\n    \n\n\nBased on this study, they found out the following:\n\n\n  \n    The two structural objectives were both critical to most of the\ndownstream tasks, except for the word structural objective in the\nSNLI task.\n  \n  \n    The StructBERT model with structural pre-training consistently\noutperformed the original BERT model, which shows the effectiveness\nof the proposed structural objectives.\n  \n  \n    For the sentence-pair tasks such as MNLI, SNLI, QQP and SQuAD,\nincorporating the sentence structural objective significantly\nimproved the performance.\n  \n  \n    For the single-sentence tasks such as CoLA and SST-2, the word\nstructural objective played the most important role. Especially in\nthe CoLA task, which is related to the grammatical error correction,\nthe improvement was over 5%. The ability of reconstructing the order\nof words in pre-training helped the model better judge the\nacceptability of a single sentence.\n  \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "TinyBERT",
        "url"       : "/language-modeling/TinyBERT",
        "date"      : "23/09/2019",
        "content": "TinyBERT is a distilled version of BERT using a novel knowledge\ndistillation method called “Transformer distillation” that was specially\ndesigned for Transformer models such as\nBERT. TinyBERT was\nproposed in 2019 by Huawei Noah’s Ark Lab and published in this paper\nunder the same name “TinyBERT: Distilling Bert For Natural Language\nUnderstanding”. The official code\nfor this paper can be found in the following GitHub repository:\nTinyBERT.\n\n\n    \n\n\nKnowledge distillation (KD) is a commonly used technique for reducing\nthe size of big deep learning models. This technique was proposed by\nGeoffrey Hinton back in 2015 in this paper: Distilling the knowledge in\na neural network. KD aims to\ntransfer the knowledge embedded in a large network (called “teacher”) to\na small network (called “student”) where the student network is trained\nto reproduce the behaviors of the teacher network.\n\nThe proposed Transformer Distillation method works differently based on\nthe type of layer to be distilled. Additionally, this method performs\nthe knowledge distillation at both the pre-training and fine-tuning\nstages which ensures that TinyBERT can capture both the general-domain\nand task-specific knowledge of the teacher BERT.\n\nThe following table shows a comparison between BERT-base,the proposed TinyBERT and other distilled variations:\n\n\n    \n\n\nTransformer Distillation\n\nAs said earlier, Transformer Distillation is a novel Knowledge\nDistillation technique for compressing transformer-based models and\nconsidering all types of layers including transformer layers, the\nembedding layer, and the prediction layer. Each layer will be distilled\ndifferently as we are going to see later.\n\nAssuming that the student model has M Transformer layers and teacher\nmodel has N Transformer layers, the student can acquire knowledge from\nthe teacher by minimizing the following objective:\n\n\\[\\mathcal{L}_{\\text{model}} = \\sum_{m = 0}^{M + 1}{\\lambda_{m}\\mathcal{L}_{\\text{layer}}\\left( S_{m},\\ T_{g\\left( m \\right)} \\right)}\\]\n\nWhere:\n\n\n  \n    $S$ and $T$ refer to the Student model and the Teacher model\nrespectively.\n  \n  \n    $\\lambda_{m}$ is the hyper-parameter that represents the importance\nof the m-th layer’s distillation. In the paper, they used\n$\\lambda = 1$.\n  \n  \n    $g\\left( m \\right)$ is a mapping function that maps m-th student\nlayer to a certain Teacher layer. In the paper, they used\n$g\\left( m \\right) = 3m$; knowing that $g\\left( 0 \\right) = 0$ which\nis the mapping of the embedding layer and\n$g\\left( M + 1 \\right) = N + 1$ which is the mapping of the\nprediction layer.\n  \n  \n    $\\mathcal{L}_{\\text{layer}}$ refers to the loss function of a given\nmodel layer which changes based on the layer type; it can be\ndescribed in the following formula:\n  \n\n\n\\[\\mathcal{L}_{\\text{layer}}\\left( S_{m},\\ T_{g\\left( m \\right)} \\right) = \\left\\{ \\begin{matrix}\n\\mathcal{L}_{\\text{embd}}\\left( S_{0},\\ T_{0} \\right),\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ m = 0 \\\\\n\\mathcal{L}_{\\text{hid}}\\left( S_{m},\\ T_{g\\left( m \\right)} \\right) + \\mathcal{L}_{\\text{attn}}\\left( S_{m},\\ T_{g\\left( m \\right)} \\right),\\ \\ \\ M \\geq m &amp;gt; 0 \\\\\n\\mathcal{L}_{\\text{pred}}\\left( S_{M + 1},\\ T_{N + 1} \\right),\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ m = M + 1 \\\\\n\\end{matrix} \\right.\\]\n\nEmbedding-Layer Distillation\n\nHere, the student embedding layer $E^{S} \\in \\mathbb{R}^{l \\times d}$\nhas the same size as the teacher embedding layer\n$E^{T} \\in \\mathbb{R}^{l \\times d}$ and it acquires knowledge by\nminimizing the following mean-squared error function where the matrix\n$W_{e} \\in \\mathbb{R}^{d \\times d}$ is a learnable linear transformation\nmatrix, $l$ is the length of the input text and $d$ is the model\ndimension:\n\n\\[\\mathcal{L}_{\\text{embd}} = \\text{MSE}\\left( E^{S}W_{e},\\ E^{T} \\right)\\]\n\nTransformer-Layer Distillation\n\nAs show in the following figure, the transformer-layer distillation\nconsists of two parts:\n\n\n    \n\n\n\n  Attention-based distillation $\\mathcal{L}_{\\text{attn}}$:\nThat ensures that the linguistic knowledge is transferred from\nteacher BERT to student TinyBERT where the student learns to fit the\nattention matrices of multi-head attention from the teacher, and the\nobjective is defined as:\n\n\n\\[\\mathcal{L}_{\\text{attn}} = \\frac{1}{h}\\sum_{i = 1}^{h}{\\text{MSE}\\left( A_{i}^{S},\\ A_{i}^{T} \\right)}\\]\n\n    Where $h$ is the number of attention heads and\n$A_{i}^{S} \\in \\mathbb{R}^{l \\times 1}$ refers to the attention matrix\ncorresponding to the i-th head of the student model and $l$ is the input\ntext.\n\n\n  Hidden states based distillation $\\mathcal{L}_{\\text{hid}}$:\nHere, the student hidden layer $H^{S} \\in \\mathbb{R}^{l \\times d’}$\nhas the smaller size than the teacher embedding layer\n$H^{T} \\in \\mathbb{R}^{l \\times d}$ and it acquires knowledge by\nminimizing the following mean-squared error function where the\nmatrix $W_{f} \\in \\mathbb{R}^{d’ \\times d}$ is a learnable linear\ntransformation matrix:\n\n\n\\[\\mathcal{L}_{\\text{hid}} = \\text{MSE}\\left( H^{S}W_{h},\\ H^{T} \\right)\\]\n\nPrediction-Layer Distillation\n\nIn addition to imitating the behaviors of intermediate layers, they also\nused the knowledge distillation to fit the predictions of teacher model\naccording to the following loss function\n\n\\[\\mathcal{L}_{\\text{pred}} = \\text{softmax}\\left( z^{T} \\right).\\text{log\\_softmax}\\left( \\frac{z^{S}}{t} \\right)\\]\n\nWhere $z^{S}$ and $z^{T}$ are the logits vectors predicted by the\nstudent and teacher respectively, $\\text{log_softmax}$ means the log\nlikelihood, and $t$ means the temperature value. According to the paper,\n$t = 1$ performs well.\n\nTinyBERT Learning\n\nAs shown in the following figure, TinyBERT learning consist of two stages that\nare complementary to each other which are General\nDistillation and Task-specific Distillation.\nAlthough there is a big gap between BERT and TinyBERT in model size, by\nperforming the proposed two-stage distillation, the TinyBERT can achieve\ncomparable performances as large BERT in various NLP tasks.\n\n\n    \n\n\nGeneral Distillation\n\nIn general distillation, the original BERT without fine-tuning is used\nas the teacher model along with a large-scale text corpus as the basic\nlearning material. By performing the proposed Transformer distillation\non the text from general domain, we obtain a general TinyBERT that can\nbe fine-tuned for downstream tasks later.\n\n\n  Note:\nDue to the big reductions in the hidden/embedding size and the layer\nnumber, general TinyBERT performs relatively worse than the original\nBERT\n\n\nTask-specific Distillation:\n\nIn the task-specific distillation, the fine-tuned BERT is used as the\nteacher model along with an augmented data to extend the task-specific\ntraining set. With learning more task-related materials, the\ngeneralization capabilities of student model can be further improved.\n\nTo augment the task-specific training set, they used a pre-trained\nlanguage model BERT and GloVe word embeddings to do word-level\nreplacement as shown in the following algorithm:\n\n\n    \n\n\nWhich can be explained in the following steps:\n\n\n  \n    First, mask each word piece in a sentence.\n  \n  \n    If the selected word is a word-piece, then use BERT as a language\nmodel to predict $N$ most-likely words. Otherwise, use GloVe\nembeddings to get the top $N$ similar words.\n  \n  \n    Then, choose a random number uniformally. If the number is less than\na certain threshold number $p_{t}$, then choose a random candidate\nfrom the suggested one. Otherwise, keep the word as it is.\n  \n  \n    In the paper, they applied this data augmentation method $N = 20$\ntimes to all the sentences of a downstream task while setting\n$p_{t} = 0.4$ for all our experiments.\n  \n\n\nExperiments &amp;amp; Results\n\nThey evaluated TinyBERT on the General Language Understanding Evaluation\n(GLUE) benchmark, which is a collection of diverse natural language\nunderstanding tasks.The evaluation results are presented in the\nfollowing table which shows that TinyBERT is consistently better than\nBERT-SMALL in all the GLUE tasks despite being same in size:\n\n\n    \n\n\nThe following table shows a comparison among three wider and deeper\nvariants of TinyBERT and their evaluation results on different\ndevelopment sets.\n\n\n    \n\n\nWe can clearly see that:\n\n\n  \n    All the three TinyBERT variants can consistently outperform the\noriginal smallest TinyBERT, which indicates that the proposed KD\nmethod works for the student models of various model sizes.\n  \n  \n    For the CoLA task, the improvement is slight when only increasing\nthe number of layers (from 49.7 to 50.6) or hidden size (from 49.7\nto 50.5). To achieve more dramatic improvements, the student model\nshould become deeper and wider (from 49.7 to 54.0).\n  \n  \n    Another interesting observation is that the smallest 4-layer\nTinyBERT can even outperform the 6-layers baselines, which further\nconfirms the effectiveness of the proposed KD method.\n  \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "ALBERT",
        "url"       : "/language-modeling/ALBERT",
        "date"      : "26/09/2019",
        "content": "ALBERT, stands for “A Lite BERT”, reduced version of BERT which is a\nsmaller, faster, cheaper and easier to scale. ALBERT was created by\nGoogle &amp;amp; Toyota Technical Institute in February 2019 and published in\nthis paper: “ALBERT: A Lite Bert For Self-Supervised Learning Of\nLanguage Representations” and you can\nfine the official code for this paper in Google Research’s official GitHub\nrepository: google-research/ALBERT.\n\n\n    \n\n\nALBERT incorporates two parameter reduction techniques that act as a form of\nregularization that stabilizes the training and helps with generalization; and\none new loss function:\n\n\n  \n    Factorized Embedding Parameterization\n  \n  \n    Cross-layer Parameter Sharing\n  \n  \n    Sentence-Order Prediction Loss\n  \n\n\nAlso, The parameter reduction techniques also act as a form of\nregularization that stabilizes the training and helps with\ngeneralization.\n\nThese three modifications significantly reduce the number of\nparameters for BERT without seriously hurting performance. An ALBERT\nconfiguration similar to BERT-large has 18x fewer parameters and can\nbe trained about 1.7x faster as shown in the following table:\n\n\n    \n\n\nAnd since longer training usually leads to better performance, the\nfollowing table shows a comparison between the performance BERT and\nALBERT after the same training time which shows that ALBERT still\nperforms better:\n\n\n    \n\n\nFactorized Embedding Parameterization\n\nIn BERT, the Word embedding size $E$ is tied with the hidden layer size\n$H$, i.e., $E\\  \\equiv \\ H$. This decision appears sub-optimal for the\nfollowing reasons:\n\n\n  \n    Word embeddings are meant to learn context-independent\nrepresentations, whereas hidden-layer embeddings are meant to\nlearn context-dependent representations. And that dictates that\n$H \\gg E$.\n  \n  \n    NLP usually requires the vocabulary size $V$ to be large. If\n$E \\equiv H$, then increasing $H$ increases the size of the\nembedding matrix, which has size $V \\times E$. This can easily\nresult in a model with billions of parameters, most of which are\nonly updated sparsely during training.\n  \n\n\nSo, instead of projecting the one-hot vectors directly into the hidden\nspace of size H, they first projected them into a lower dimensional\nembedding space of size E, and then projected it to the hidden space. By\nusing this decomposition, we reduce the embedding parameters from\n$O(V \\times H)$ to\n$O\\left( \\left( V \\times E \\right) + \\left( E \\times H \\right) \\right)$.\nThis parameter reduction is significant when $H \\gg E$.\n\nCross-layer Parameter Sharing\n\nThe default decision for ALBERT is to share all parameters across\nlayers. The following figure shows the L2 distances and cosine\nsimilarity of the input and output embeddings for each layer. We observe\nthat the transitions from layer to layer are much smoother for ALBERT\nthan for BERT.\n\nThese results show that weight-sharing has an effect on stabilizing\nnetwork parameters. Although there is a drop for both metrics compared\nto BERT, they nevertheless do not converge to 0 even after 24 layers.\n\n\n    \n\n\nSentence-Order Prediction (SOP)\n\nIn this paper, they proposed a new loss function called sentence-order\nprediction (SOP) loss as a replacement for next sentence prediction\n(NSP) used with BERT. The same as NSP, SOP loss uses two consecutive\nsegments from the same document as a positive example. And as negative\nexamples, it uses the same two consecutive segments but with their order\nswapped.\n\nThis forces the model to learn finer-grained distinctions about\ndiscourse-level coherence properties. As shown in the following table,\nit turns out that NSP cannot solve the SOP task at all, while SOP can\nsolve the NSP task to a reasonable degree.\n\n\n    \n\n\nHyper-parameters\n\nIn this part, we are going to talk about the effect of some\nhyper-parameters on ALBERT:\n\n\n  Network Depth (number of layers):\nIf we compare a 3-layer ALBERT model with a 1-layer ALBERT model,\nalthough they have the same number of parameters, the performance\nincreases significantly. However, there are diminishing returns\nwhen continuing to increase the number of layers: the results of a\n12-layer network are relatively close to the results of a 24-layer\nnetwork, and the performance of a 48-layer network appears to\ndecline.\n\n\n\n    \n\n\n\n  Network Width (hidden size):\nUsing a 3-layers ALBERT, we can see that as we increase the hidden\nsize, we get an increase in performance with diminishing returns.\nAt a hidden size of 6144, the performance appears to decline\nsignificantly:\n\n\n\n    \n\n\n\n  Dropout:\nUsing ALBERT-xxlarge at around 1M training steps states that\nremoving dropout helps the downstream tasks which indicates that\nALBERT models didn’t overfit the data.\n\n\n\n    \n\n\n\n  Data:\nAdding more data improvements on the downstream tasks, except\nfor the SQuAD benchmarks (which are Wikipedia-based, and therefore\nare negatively affected by out-of-domain training material).\n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "Google&#39;s T5",
        "url"       : "/language-modeling/T5",
        "date"      : "23/10/2019",
        "content": "T5 stands for “Text-to-Text Transfer Transformer” which is a\ntext-to-text framework proposed by Google in 2019 and published in this\npaper: “Exploring the Limits of Transfer Learning with a Unified\nText-to-Text Transformer”. The\nofficial code for this paper can be found on Google Research’s official\nGitHub repository:\ngoogle-research/text-to-text-transfer-transformer.\n\nFrom the name of the paper, we can see that this 64-pages research paper\ndiscusses transfer learning and its limitations. Transfer learning is\nwhere a model is first pre-trained on a data-rich task using in an\nunsupervised-fashion before being fine-tuned on a downstream task.\nDownstream tasks like machine translation, text classification, ..etc.\nThis has turned out to be as a powerful technique in natural language\nprocessing.\n\nAnd also from the name, we can see it focuses on text-to-text tasks\nwhich are tasks that take text as an input and return text as an output\nlike machine translation, language generation, text summarization,\nquestion answering ...etc. This allows us to use the same model, loss\nfunction, hyper-parameters across diverse set of tasks.\n\nIn order to train a single model on the diverse set of tasks, they fed\nthe model some text for context before feeding the input text and then\nasked to produce some output text. For example, to ask the model to\ntranslate the sentence “That is good.” from English to German, the model\nwould be fed the sequence “translate English to German: That is good.”\nand would be trained to output “Das ist gut.” as shown below:\n\n\n    \n\n\nAgain, this paper does not propose new methods but instead provides a\ncomprehensive perspective on text-to-text tasks. And to perform\nexperiments, it introduces the\nC4 (Colossal Clean\nCrawled Corpus) dataset consisting of hundreds of gigabytes of clean\nEnglish text scraped from the web.\n\nC4\n\nC4 stands for “Colossal Clean Crawled Corpus” which is a cleaned-up\nversion of the publicly-available Common\nCrawl dataset. Common Crawl provides web\nextracted text by crawling the internet producing around 20TB of scraped\ntext data each month.\n\nTo assemble the C4 dataset, they downloaded the web extracted text from\nCommon Crawl that was posted on April 2019, which is about 750 GB. And\nthen they applied the following filtering steps:\n\n\n  \n    They only retained lines that ended in a terminal punctuation mark\n(i.e. a period, exclamation mark, question mark, or end quotation\nmark).\n  \n  \n    They discarded any page with fewer than 5 sentences and only\nretained lines that contained at least 3 words.\n  \n  \n    They removed any page that contained any word on the “List of\nDirty, Naughty, Obscene or Otherwise Bad\nWords”.\n  \n  \n    They removed any line with the word JavaScript since it usually\nindicates an error on the web.\n  \n  \n    Some pages had placeholder “lorem ipsum” text; they removed any page\nwhere the phrase “lorem ipsum” appeared.\n  \n  \n    Some pages inadvertently contained code. Since the curly bracket “{“\nappears in many programming languages (such as Javascript, widely\nused on the web) but not in natural text, they removed any pages\nthat contained a curly bracket.\n  \n  \n    To de-duplicate the dataset, they discarded all but one of any\nthree-sentence span occurring more than once in the data set.\n  \n  \n    Since most of downstream tasks are focused on English-language text,\nthey used langdetect tool\nto filter out any pages that were not classified as English with a\nprobability of at least $0.99$.\n  \n\n\n\n  Note:\nThe C4 dataset is released as part of TensorFlow Datasets. You can\nfind it in this\nlink.\n\n\nModels\n\nAll the models mentioned in this paper are based on the\nTransformer\narchitecture. The implemented version of the transformer used in this\npaper follows closely the originally-proposed form with a few changes:\n\n\n  \n    They used a simplified version of layer normalization where the\nactivations are only re-scaled and no additive bias is applied.\n  \n  \n    While the original Transformer used a sinusoidal position signal or\nlearned position embeddings, they used a simplified form of\nrelative-position embeddings where each “embedding” is simply a\nscalar that represents the offset between the “key” and “query”\nbeing compared. This scalar is a number from 1 to 32 increasing\nlogarithmically. So, if the offset is 4, the embedding is 1. If the\noffset is 32, the embedding is 4. If the offset is 128, the\nembedding is 32. Beyond 128, all relative positions to the same\nembedding which is 32.\n  \n  \n    Also, they shared the position embedding parameters across all\nlayers knowing that each attention head uses a different learned\nposition embedding.\n\n    In this paper, the researchers pre-trained different variations of\nthe Transformer architecture and compared their performance in\ncomparison to the baseline models. First, let’s talk about the\nbaseline model first.\n  \n\n\nBaseline Model\n\nThe baseline model used in this paper is a standard encoder-decoder\nTransformer architecture. It is designed so that the encoder and decoder\nare each similar in size and both are configured to a BERT~BASE~\nconfiguration which means:\n\n\n  \n    Both the encoder and decoder consist of 12 layers (each layer\ncomprising self-attention, optional encoder-decoder attention, and a\nfeed-forward network).\n  \n  \n    The feed-forward networks is a dense layer with an output\ndimensionality of $d_{\\text{ff}} = 3072$ followed by a ReLU\nactivation and another dense layer.\n  \n  \n    The “key” and “value” matrices of all attention mechanisms have an\ninner dimensionality of $d_{\\text{kv}} = 64$ and all attention\nmechanisms have $12$ heads. All other sub-layers and embeddings have\na dimensionality of $d_{\\text{model}} = 768$.\n  \n  \n    For regularization, they used a dropout probability of 0.1\neverywhere in the model.\n\n    In total, this results in a model with about 220 million parameters\nwhich is roughly twice the number of parameters of BERT~BASE~ since\nthis baseline model contains an encoder and a decoder.\n  \n\n\nArchitectural Variants\n\nNow that we have discussed the baseline model used in this paper, let’s\ndiscuss the other variations they tried. The only thing at which these\nvariations differ is the self-attention mechanism.\n\nRecall that the self-attention operation in a Transformer takes a\nsequence as input $X$ and outputs a new sequence of the same length $Y$.\nEach entry of the output sequence $y_{i}$ is produced by computing a\nweighted average of entries of the input sequence\n$\\sum_{j}^{}w_{i,j}x_{j}$ where $w_{i,j}$ is the scalar weight produced\nby the self-attention mechanism as a function of $x_{i}$ and $x_{i}$.\nThe attention mask is then used to zero out certain weights\n$w_{i,j} = 0$ in order to constrain which entries of the input can be\nattended to at a given output timestep.\n\nThe different variations of masking in self-attention mechanism can be\nseen in the following figure:\n\n\n    \n\n\n\n  \n    Fully-visible:\nThis allows a self-attention mechanism to attend to any\nentry of the input when producing each entry of its output.\n  \n  \n    Causal:\nThis allows a self-attention mechanism to attend to only previous\ntokens. So, when producing the $i^{\\text{th}}$ entry of the output\nsequence, causal masking prevents the model from attending to the\n$j^{\\text{th}}$ entry of the input sequence for $j &amp;gt; i$. This\nmasking can be used as a language model (LM), i.e. a model trained\nsolely for next-step prediction\n  \n  \n    Causal with prefix:\nThis is a special case of the causal masking where a fully-visible\nmasking will be used during the prefix portion of the sequence and\ncausal masking for the rest.\n  \n\n\nUsing these three different masking techniques with the standard\nencoder-decoder transformer, we will get three different variations that\nwill be used:\n\n\n    \n\n\n\n  \n    Encoder-decoder (left):\nThis is the same as the baseline; the encoder has no masking\n(fully-visible) while the decoder has a “causal masking”.\n  \n  \n    Language Model (middle):\nThis architecture consists of a single Transformer layer stack and\nis fed the concatenation of the input and target using a causal mask\nthroughout.\n  \n  \n    Prefix LM (right):\nThis architecture is similar to language model with\nprefix-parameters (red rectangle) use fully-visible masking and the\nrest (green rectangle) use causal masking. This closely resembles\nBERT\n  \n\n\n\n  Note:\nThe Prefix LM is similar to an encoder-decoder model with parameters\nshared across the encoder and decoder and with the encoder-decoder\nattention replaced with full attention across the input and target\nsequence.\n\n\nPre-training\n\nIn this paper, pre-training is done by running the model for\n$2^{19} = 524,288$ steps on C4 before fine-tuning. A maximum sequence\nlength of $512$ is used and a batch size of $128$ sequences and they\npacked multiple sequences into one entry of the batch whenever\npossible. Roughly speaking, models in this paper are pre-trained on\n$2^{35} \\approx 34B$ tokens which is considerably less than BERT\n($137B\\ $tokens) or RoBERTa ($2.2T$ tokens).\n\n\n  Note:\nThat $2^{35}$ tokens only covers a fraction of the entire C4 dataset\nwhich means they never repeated any data during pre-training.\n\n\nAlso, they used an “inverse square root” learning rate schedule:\n$\\frac{1}{\\sqrt{\\max\\left( n,k \\right)}}$ where $n$ is the current\ntraining iteration and $k$ is the number of warm-up steps which was set\nto $10^{4}$. This sets a constant learning rate of $0.01$ for the first\n$10^{4}$ steps, then exponentially decays the learning rate until\npre-training is over.\n\nSince fine-tuning will be done on other languages other than English\nlike (German, French, and Romanian), they classified the pages in C4\nthat are either German, or French, or Romanian. Then, they trained the\nSentencePiece model on a mixture of 10 parts of English C4 data with 1\npart for each language forming a vocabulary of $32,000$ word-pieces.\n\n\n  Very Important Note:\nBased on this vocabulary setup, T5 models can only process a\npredetermined, fixed set of languages which are English, German, French,\nand Romanian.\n\n\nObjectives\n\nThe choice of unsupervised objective is of central importance as it\nprovides the mechanism through which the model gains general-purpose\nknowledge to apply to downstream tasks. All objectives mask one or\nmultiple tokens from the input to produce a (corrupted) input that the\nmodel will learn to predict the target sequence with the maximum\nlikelihood. The following table summarizes all pre-training objectives\ndiscussed in this paper and they are:\n\n\n    \n\n\n\n  \n    Prefix Language Modeling:\nThis technique splits a span of text into two components, one to use\nas inputs to the encoder and the other to use as a target sequence\nto be predicted by the decoder.\n  \n  \n    BERT-style:\nBERT-MLM objective takes a span of text and corrupts $15\\%$ of the\ntokens. BERT had only an encoder without a decoder. So, in this\nencoder-decoder setup, they adapted MLM from BERT by simply using\nthe entire uncorrupted sequence as the target.\n  \n  \n    De-shuffling:\nThis approach takes a sequence of tokens, shuffles it, and\nthen uses the original de-shuffled sequence as a target.\n  \n  \n    MASS-style:\nThis approach masks a consecutive sequence of tokens in the input\nand passes it to the encoder, then use the uncorrupted sequence as\nthe target. This looks like the Masked-Sequence objective discussed\nin MASS with a\nfew changes.\n  \n  \n    i.i.d. noise, replace spans:\nThis approach avoid predicting the whole uncorrupted text by\nmasking a few tokens in the encoder. Then, the target sequence\nbecomes the concatenation of the “corrupted” spans, each prefixed by\nthe mask token used to replace it in the input.\n  \n  \n    i.i.d. noise, drop tokens:\nIt’s the same as I.i.d. noise, replace spans but\nwith dropping the corrupted tokens from the input sequence, then use\nthese dropped tokens (in order) as the target.\n  \n\n\n\n  Notes:\n\n  \n    \n      $\\left\\langle M \\right\\rangle$ denotes a shared mask token (with\n  same ID) while $\\left\\langle X \\right\\rangle$,\n  $\\left\\langle Y \\right\\rangle$, and $\\left\\langle Z \\right\\rangle$\n  denote sentinel tokens that are assigned unique token IDs.\n    \n    \n      The greyed-out word “apple” in the previous table shows that this\n  token is a random token used as a replacement.\n    \n    \n      All objectives mentioned earlier except the prefix LM are called\n  “denoising” objectives, since they add a noise to the input and the\n  model has to de-noise it.\n    \n    \n      The replace spans approach later will be called “Span\n  Corruption”.\n    \n    \n      The “i.i.d” written before the objective name of the last two\n  objectives indicates that for each input token, a decision will be\n  made to either corrupt the token or leave it as it is.\n    \n  \n\n\nFine-Tuning\n\nAll models in this paper were fine-tuned for $2^{18} = 262,144$ steps on\nall tasks. This value was chosen as a trade-off between the\nhigh-resource tasks (i.e. those with large data sets), which benefit\nfrom additional fine-tuning, and low-resource tasks (smaller data sets),\nwhich overfit quickly. Like pre-training, they used batches with $128$\nlength-$512$ sequences. And unlike pre-training, they used a constant\nlearning rate of $0.001$.\n\nThey saved a checkpoint every $5,000$ steps and report results on the\ncheckpoint that got the highest validation performance. For models\nfine-tuned on multiple tasks, they chose the best checkpoint for each\ntask independently.\n\nBenchmarks\n\nThey used a diverse set of benchmark (all sourced from TensorFlow\ndatasets) that is able to measure\nthe general language learning; these datasets are:\n\n\n  \n    GLUE and SuperGLUE for text classification.\n  \n  \n    CNN/Daily Mail for abstractive summarization.\n  \n  \n    SQuAD for question answering.\n  \n  \n    WMT (English → German), (English → French), and (English → Romanian) for\nmachine translation.\n  \n\n\nResults\n\nIn this part, we are going to discuss all the experiments they tried in\nthe paper and what we can learn from them. The results tables are all\nformatted so that each row corresponds to a particular experimental\nconfiguration with columns giving the scores for each benchmark. The\nbaseline configuration is marked with ★. Any score that is within two\nstandard deviation of the best score in a given experiment will be\nbold-faced. Also, all results are reported on the validation set of\neach benchmark dataset.\n\n\n  Baseline (with/without pre-training):\nThe following table shows the average and standard deviation\nof the baseline model with and without pre-training for the same\nnumber of steps:\n\n\n\n    \n\n\n\n  Architectural Variants:\nThe following table shows the performance when trying different\narchitectural variants pre-trained on a certain objective and\nfine-tuned on the benchmark. To provide a reasonable means of\ncomparison, they referred to the number of layers in\nBERT~BASE~-sized layer stack as $L$ and the number of parameters as\n$P$ and the number of FLOPs (Floating-point Operations) required for\nan $L + L$-layer encoder-decoder model or $L$-layer decoder-only\nmodel as $M$.\n\n\n\n    \n\n\n\n  Objective Functions:\nThe following table shows the performance of the baseline model\nusing different objective functions; from the table we can see that\nall BERT-style variants (BERT-style + the last three) perform\nsimilarly.\n\n\n\n    \n\n\n\n  BERT-style Corruption Rate:\nAs you remember, BERT-style corruption rate masks 15% of the input\nsequence; 80% with the $\\left\\langle M \\right\\rangle$ token; 10%\nwith a random token and 10% with the original token. In the\nfollowing table, they tried different corruption rate. From the\ntable, we can see that the corruption rate had a limited effect on\nthe model’s performance. The only exception is (50%), it results in\na significant degradation of performance on GLUE and SQuAD.\n\n\n\n    \n\n\n\n  Span Length:\nSpan length is the number of consecutive tokens that will be masked\nwhen applying the denoising objective. Using a corruption rate of\n$15\\%$ in all cases, the following table compares average span\nlengths of 2, 3, 5 and 10. The baseline (i.i.d) means that for each\ninput token, we will have to make a decision whether to corrupt it\nor not. Again, this shows limited difference except with an average\nspan length of 10 which slightly under-performs the other values in\nsome cases.\n\n\n\n    \n\n\n\n  \n    Pre-training datasets:\nThe following table measures the effect of different C4 filtration\nmethods (first four entries) alongside with common pre-training\ndatasets (last two) on downstream tasks performance:\n\n    \n      \n        Unfiltered C4: ignoring all C4 filtration steps mentioned\nearlier except the one using the\nlangdetect tool to extract\nEnglish data.\n      \n      \n        RealNews-like: using standard filtration on C4 and to only\ninclude content from one of the domains used in the “RealNews”\ndataset.\n      \n      \n        WebText-like: using standard filtration on C4 and to only\nuse content originated from a URL that appeared in the list\nprepared by the\nOpenWebText. This\nwas relatively small (around 2GB). Therefore, they used 12\nmonths (August 2018 to July 2019) from CommonCrawl instead of\njust one months as the original C4.\n      \n      \n        Wikipedia: using the English Wikipedia text data from\nTensorFlow\nDatasets,\nwhich omits any markup or reference sections from the\narticles.\n      \n      \n        Wikipedia + Toronto Books Corpus: A drawback of using\npre-training data from Wikipedia is that it represents only\none possible domain of natural text (encyclopedia articles).\nTo mitigate this, they combined the Wikipedia data with the\nToronto Books Corpus (TBC). TBC contains text extracted from\neBooks, which represents a different domain of natural\nlanguage. This is the same pre-training data used with BERT.\n      \n    \n  \n\n\n\n    \n\n\n\n  Pre-training data size:\nThe following table measures the effect of limited unlabeled dataset\nsizes. These results were obtained by truncating the first $2^{29}$,\n$2^{27}$, $2^{25}$, and $2^{23}$ tokens of the C4 dataset. Knowing\nthat all models in this paper were trained using $2^{35}$ tokens,\nthese data sizes had to be repeated to match that. From the table,\nthe performance degrades as the data set size shrinks which is\ntotally expected:\n\n\n\n    \n\n\n\n  Alternative fine-tuning Methods:\nThe following table compares different alternative fine-tuning\nmethods that only update a subset of the model’s parameters. For\nadapter layers, $d$ refers to the inner dimensionality of the\nadapters. As we can see, lower-resource tasks like SQuAD work well\nwith a small value of $d$ whereas higher resource tasks require a\nlarge dimensionality to achieve reasonable performance.\n\n\n\n    \n\n\n\n  Note:\nThe past experiment suggests that adapter layers could be a\npromising technique for fine-tuning on fewer parameters as long as\nthe dimensionality is scaled appropriately to the task size.\n\n\nMulti-task Learning\n\nSo far, we have been pre-training our model on a single unsupervised\nlearning task before fine-tuning it individually on each downstream\ntask. An alternative approach, called “multitask learning”, is to train\nthe model on multiple tasks at once. In this paper, they relaxed that\ngoal somewhat and instead investigated methods for training on multiple\ntasks at once in order to eventually produce separate parameter settings\nthat perform well on each individual task which makes it comparable to\nthe pre-train-then-fine-tune approach.\n\nMulti-task learning is performed on the same datasets as the fine-tuning\nAn extremely important factor in multi-task learning is how much data\nfrom each task the model should be trained on. In this paper, they have\ntried three different mixing methods to make sure each task gets enough\ndata:\n\n\n  \n    Equal mixing:\nIn this case, each example in each batch is sampled uniformly at\nrandom from one of the datasets.\n  \n  \n    Example-proportional Mixing:\nIf the number of examples in each of the $N$ task’s data sets is\n$e_{n}$ where $n \\in \\left\\{ 1,\\ …N \\right\\}$, then the\nprobability of sampling an example from the $m^{\\text{th}}$ task\nduring training is\n$r_{m} = \\frac{\\min\\left( e_{n},\\ K \\right)}{\\sum_{i = 1}^{N}{\\min\\left( e_{i},\\ K \\right)}}$\nwhere $K$ is the artificial data set size limit.\n  \n  \n    Temperature-scaled mixing:\nTemperature up-samples the relatively low-resource tasks. This is\ndone by raising each task’s mixing rate $r_{m}$ to the power of\n$\\frac{1}{T}$.\n  \n\n\nTo compare these mixing strategies on equal footing with the\npre-train-then-fine-tune results, they trained multi-task models for\nthe same total number of steps: $2^{19} + 2^{18} = 786,432$. The\nresults are shown in the following table:\n\n\n    \n\n\nFrom the table, we can find out that multi-task training\nunder-performs pre-training followed by fine-tuning on most tasks.\nThe “equal” mixing strategy in particular results in dramatically\ndegraded performance.\n\nSince they relaxed the multi-task learning to make it look similar\nto the pre-train-then-fine-tune-approach, they decided to combine\nfine-tuning with multi-task learning once and combine multi-task\nlearning with pre-training another time. The following table shows\nthat the pre-train and then fine-tune technique is still better:\n\n\n    \n\n\nAll multi-task learning performed in the previous table were mixed\nusing examples-proportional method (with $K = 2^{19}$).\nLeave-one-out multi-task training was done by pre-training the model\non all tasks except one and then fine-tune it on the task that was\nleft out during pre-training.\n\nScaling\n\nIn deep learning, there is an argument that says that scaling up models\nproduces improved performance. However, there are a variety of possible\nways to scale (to make model bigger) like using more\nparameters, training the model for more steps,\nand ensembling. In the paper, they compared these\ndifferent approaches with the baseline model.\n\nTo increase the number of parameters, they experimented with the\nBERT~LARGE~ setup with $d_{\\text{ff}} = 4096$,\n$d_{\\text{model}} = 1024$, $d_{\\text{kv}} = 64$, and $16$-head attention\nmechanism and $16$-layers encoder and $16$-layers decoder. This setup\nproduces twice ($2 \\times$) the number of parameters as the baseline\nmodel. Using $32$-layers encoder and $32$-layers decoder will produce\nroughly four times ($4 \\times$) the number of parameters as the baseline\nmodel. Also, created an ensemble model by training the baseline 4\ndifferent times and averaging their results.\n\nThe performance achieved after applying these various scaling methods is\nshown in the following table which shows that increasing the model size\nresulted in an additional bump in performance compared to solely\nincreasing the training time or batch size:\n\n\n    \n\n\n\n  Important Notes:\n\n  \n    \n      Different scaling methods have different trade-offs that are\n  separate from their performance. For example, using a larger model\n  can make downstream fine-tuning and inference more expensive. In\n  contrast, the cost of pre-training a small model for longer is\n  effectively amortized if it is applied to many downstream tasks.\n    \n    \n      Ensembling $N$ separate models has a similar cost to using a model\n  that has an $N \\times$ higher computational cost.\n    \n  \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "BART",
        "url"       : "/language-modeling/BART",
        "date"      : "29/10/2019",
        "content": "BART stands for “Bidirectional Auto-regressive Transformer” which is a\npre-training scheme for models created by Facebook AI in 2019 and\npublished in this paper: “BART: Denoising Sequence-to-Sequence\nPre-training for Natural Language Generation, Translation, and\nComprehension”. Pre-training is\nthe process of training a model with one task that is able to help it\nform parameters that can be used to make other tasks easier. And this is\nwhat we, human beings, do. We use our old knowledge of what we have\nlearned in the past to understand new knowledge and handle a variety of\nnew tasks.\n\nBART was published in a paper under the name: “Denoising\nSequence-to-Sequence Pre-training for Natural Language Generation,\nTranslation, and Comprehension”. From, the name of the paper, we can\nconclude the following:\n\n\n  \n    The pre-training method in BART is denoising; denoising is when\nyou corrupt the input text with an arbitrary noising function, and\ntry to reconstruct the original text.\n  \n  \n    BART is a sequence-to-sequence model or encoder-decoder architecture.\n  \n  \n    BART is for any sequence-to-sequence data such as natural language\ngeneration, machine translation, and machine comprehension.\n  \n\n\nPut in simple words, BART is the same as the standard transformer that\nwe discussed with NMT with a few differences:\n\n\n  \n    BART uses Bi-directional encoder (same as BERT) and a\nuni-directional decoder from left to right (same as GPT).\n  \n  \n    BART uses GELU as an activation function instead of ReLU.\n  \n  \n    The parameters are initialized from a $\\mathcal{N}\\left( 0,\\ 0.02 \\right)$\ndistribution.\n  \n  \n    BART is pre-trained.\n  \n\n\n    \n\n\nPre-training\n\nThe encoder and the decoder are pre-trained in BART using different\ntechniques. The encoder is pre-trained by masking some of the input\ntokens and trying to predict these masked tokens (same as BERT). While\nthe decoder is trained by giving the preceding tokens and trying to\npredict the next token (same as GPT).\n\n\n    \n\n\nUnlike any other model, BART allows us to apply any type of document\ncorruption. In the paper, they experimented with several previously\nproposed and novel transformations which are summarized below:\n\n\n  \n    Token Masking: Following BERT random tokens are sampled and\nreplaced with [MASK] token.\n  \n  \n    Token Deletion: Random tokens are deleted from the input. In\ncontrast to token masking, the model must decide which positions\nare missing inputs.\n  \n  \n    Sentence Permutation: A document is divided into sentences based\non full stops, and these sentences are shuffled in a random order.\n  \n  \n    Document Rotation: A token is chosen uniformly at random, and\nthe document is rotated so that it begins with that token. This\ntask trains the model to identify the start of the document.\n  \n  \n    Text Infilling: Following SpanBERT, a number of text spans are\nsampled, with span lengths drawn from a Poisson distribution\n($\\lambda = 3$). Each span is replaced with a single [MASK]\ntoken. 0-length spans correspond to the insertion of [MASK]\ntokens. Text infilling teaches the model to predict how many\ntokens are missing from a span.\n  \n\n\nAnd the following table summarizes which noising function perform\nbest at which task. All models are of comparable size and are\ntrained for 1M steps on a combination of books and Wikipedia data.\nPerformance varies considerably across tasks, but the BART models\nwith text infilling demonstrate the most consistently strong\nperformance:\n\n\n    \n\n\n\n  Note:\nThese noising functions can be combined as seen in the last entry of\nthe above table.\n\n\nFine-tuning Tasks\n\nSame as BERT, The representations produced by BART can be used in\nseveral ways for downstream applications.\n\n\n  Text Classification:\nThe same input is fed into the encoder and decoder, and the\nfinal hidden state of the final decoder token is fed into new\nmulti-class linear classifier. This approach is related to the CLS\ntoken in BERT.\n\n\n\n    \n\n\n\n  \n    Sequence Generation:\nBecause BART has an autoregressive decoder, it can be directly\nfine tuned for sequence generation tasks such as abstractive\nquestion answering and summarization.\n  \n  \n    Machine Translation:\nWe replace BART’s encoder embedding layer with a new randomly\ninitialized encoder. The model is trained end-to-end, which trains\nthe new encoder to map foreign words into an input that BART can\nde-noise to English. The encoder part is trained in two steps, in\nboth cases backpropagating the cross-entropy loss from the output\nof the BART model:\n\n    \n      \n        In the first step, we freeze BART parameters and only update the\nrandomly initialized source encoder, the BART positional\nembeddings, and the self-attention input projection matrix of\nBART’s encoder first layer.\n      \n      \n        In the second step, we train all model parameters for a small\nnumber of iterations.\n      \n    \n  \n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "DistilBERT",
        "url"       : "/language-modeling/DistilBERT",
        "date"      : "01/03/2020",
        "content": "DistilBERT is a smaller, faster, cheaper and lighter version of BERT\ncreated by Hugging Face in March 2020 and published in this paper:\n“DistilBERT, a distilled version of BERT: smaller, faster, cheaper and\nlighter”. In this paper, they\nused knowledge distillation to reduce the size of a BERT by 40%, while\nretaining 97% of its language understanding capabilities and being 60%\nfaster. This was possible by using a triple loss function that combines\nlanguage modeling, distillation and cosine-distance losses.\n\nThe following figure shows a simple comparison between DistilBERT and\nother models with respect to the number of parameters; which shows that\nDistilBERT is small enough to run on the edge, e.g. on mobile devices.\n\n\n    \n\n\nKnowledge Distillation\n\nKnowledge distillation is a compression technique in which a compact\nmodel (the student) is trained to reproduce the behaviour of a larger\nmodel (the teacher). The teacher is trained on the hard labels (labels\nthat belongs to one class) while the student is trained on soft labels\n(the class probabilities) which resulted from the teacher. In other\nwords, the teacher is trained on the true labels from the training data\nand results probabilities that will be used to teach the student.\n\n\n    \n\n\nTriple Loss\n\nIn this paper, the student (DistilBERT) is trained with a triple loss\nwhich is a linear combination of three different losses which are:\n\n\n  $\\mathcal{L}_{\\text{ce}}$:\na distillation loss over the soft target probabilities of the teacher\n(BERT) where $t_{i}$ is the probability estimated by the teacher and\n$s_{i}$ is the probability estimated by the student.\n\n\n\\[\\mathcal{L}_{\\text{ce}} = \\sum_{i}^{}{t_{i}\\text{.log}\\left( s_{i} \\right)}\\]\n\n   Calculating the probability ($t_{i}$ and $s_{i}$) is done using the\nsoftmax-temperature function where T is the temperature that controls\nthe smoothness of the output distribution knowing that the same\ntemperature will be used for the student and the teacher:\n\n\\[p_{i} = \\frac{\\exp\\left( \\frac{z_{i}}{T} \\right)}{\\sum_{j}^{}{\\exp\\left( \\frac{z_{j}}{T} \\right)}}\\]\n\n\n  $\\mathcal{L}_{\\text{MLM}}$:\na masked language model loss; the same as BERT where ${\\widehat{x}}{i}$ is\nthe predicted word and $x{i}$ is the true word.\n\n\n\\[\\mathcal{L}_{\\text{MLM}}\\left( x_{i} \\right) = - \\log\\left( {\\widehat{x}}_{i} \\middle| x_{i} \\right)\\]\n\n\n  $\\mathcal{L}_{\\cos}$:\na cosine embedding loss which will tend to align the directions of the\nstudent and teacher hidden states vectors.\n\n\nArchitecture\n\nThe student (DistilBERT) has the same general architecture as BERT with\nthese changes:\n\n\n  \n    The token-type embeddings and the pooler are removed.\n  \n  \n    The number of layers is reduced by a factor of 2.\n  \n  \n    Most of the operations used in the Transformer architecture (linear\nlayer and layer normalisation) are highly optimized in modern\nlinear algebra frameworks.\n  \n  \n    Taking advantage of the common dimensionality between teacher and\nstudent networks, they initialized the student from the teacher by\ntaking one layer out of two.\n  \n  \n    Following best practices, they used a very large batches (up to 4K\nexamples per batch) using dynamic masking and without the next\nsentence prediction (NSP) objective.\n  \n\n\nAnd to show how efficient that was, let’s look on a simple\ncomparison between DistilBERT and BERT on GLUE:\n\n\n    \n\n\nAs we can see, DistilBERT retains 97% of BERT’s performance while being\n40% smaller and 60% faster as shown in the following table:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "ELECTRA",
        "url"       : "/language-modeling/ELECTRA",
        "date"      : "23/03/2020",
        "content": "ELECTRA stands for “Efficiently Learning an Encoder that Classifies\nToken Replacements Accurately” which is a discriminator language\nmodel unlike the widely-used generative language models such as\nBERT, GPT, ...etc. ELECTRA was proposed by Stanford University in\ncollaboration with Google Brain in 2020 and published in their paper:\nELECTRA: Pre-training text Encoders.\nThe official code of this paper can be found on Google Research’s official\nGitHub repository:\ngoogle-research/electra.\n\nGenerative language models such as BERT was pre-trained using MLM\nobjective where some tokens of the input sentence get masked, then the\nmodel is trained to reconstruct them back. These generative models\nproduce good results but they require large amounts of data and\ncomputation power to be effective.\n\nThis paper suggests an alternative pre-training task called “Replaced\nToken Detection” (RTD for short) that won’t require that much data or\ncomputation power. According to the following table, the “Replaced Token\nDetection” method outperforms MLM pre-training given the same model size, data,\nand compute budget:\n\n\n    \n\n\nReplaced Token Detection (RTD)\n\nReplaced Token Detection is a pre-training task that was introduced in\nthis paper as a replacement for MLM. Instead of masking, this method\ncorrupts the input by replacing some tokens with plausible alternatives\nsampled from a small generator network. And the discriminator (ELECTRA)\nlearns to distinguish between the original tokens and the synthetically\ngenerated ones as shown below:\n\n\n    \n\n\nAs seen in the above figure, this task trains two neural networks; each\nconsists of a transformer-encoder architecture that maps a sequence of\ninput tokens $X = \\left\\lbrack x_{1},\\ …\\ x_{n} \\right\\rbrack$ into a\nsequence of vectors\n$h\\left( X \\right) = \\left\\lbrack h_{1},\\ …\\ h_{n} \\right\\rbrack$.\n\n\n  Generator $G$:\nGiven an input $X = \\left\\lbrack x_{1},\\ …\\ x_{n} \\right\\rbrack$,\nMLM first selects a random set of positions (integers from 1 to n)\nwith a probability of $15\\%$ to mask out tokens $X^{\\text{masked}}$.\nThen, the generator learns to predict the original tokens of the\nmasked-out ones. For a given position $t$ where\n$x_{t} = \\left\\lbrack \\text{MASK} \\right\\rbrack$, the generator\noutputs a probability for generating a particular token $x_{t}$ with\na softmax layer where $e\\left( x \\right)$ is the embedding of token\n$x$:\n\n\n\\[p_{G}\\left( x_{t} \\middle| X \\right) = \\text{softmax}\\left( {e\\left( x_{t} \\right)}^{T}.h_{G}\\left( X \\right)_{t} \\right)\\]\n\n\n  Discriminator (ELECTRA) $D$:\nFor a given position $t$, the discriminator predicts whether the\ntoken $x_{t}$ is “real” or generated $X^{\\text{corrupt}}$ using a\nsigmoid layer:\n\n\n\\[D\\left( X,t \\right) = \\text{sigmoid}\\left( w^{T}.h_{D}\\left( X \\right)_{t} \\right)\\]\n\nAnd when training the model on the Replaced Token Detection task, the\nmodel tries to minimize the following loss function over a large corpus\n$\\mathcal{X}$ of raw text:\n\n\\[\\mathcal{L} = \\min_{\\theta_{G},\\ \\theta_{D}}\\left( \\sum_{X \\in \\mathcal{X}}^{}{\\mathcal{L}_{\\text{MLM}}\\left( X,\\ \\theta_{G} \\right) + \\lambda\\mathcal{L}_{\\text{Disc}}\\left( X,\\ \\theta_{D} \\right)} \\right)\\]\n\n\\[\\mathcal{L}_{\\text{MLM}}\\left( X,\\ \\theta_{G} \\right) = \\mathbb{E}\\left( \\sum_{i \\in m}^{}{- log\\ p_{G}\\left( x_{i} \\middle| X^{\\text{masked}} \\right)} \\right)\\]\n\n\\[\\mathcal{L}_{\\text{Disc}}\\left( X,\\ \\theta_{D} \\right) = \\mathbb{E}\\left( \\sum_{t = 1}^{n}{- \\mathbb{l}\\left( x_{t}^{\\text{corrupt}} = x_{t} \\right)\\text{.log}\\left( D\\left( X^{\\text{corrupt}},\\ t \\right) \\right) - \\mathbb{l}\\left( x_{t}^{\\text{corrupt}} \\neq x_{t} \\right)\\text{.log}\\left( 1 - D\\left( X^{\\text{corrupt}},\\ t \\right) \\right)} \\right)\\]\n\n\n  Note:\nThis task looks like a GAN (General Adversarial Network) but it is not\nadversarial since the generator doesn’t try to fool the discriminator.\nIts job is to generate replace the masked tokens with the exact tokens\nthat were masked.\n\n\nExperiments\n\nFor most of the experiments, ELECTRA was pre-trained on the same data as\nBERT, which\nconsists of 3.3 Billion tokens from Wikipedia and BooksCorpus. However,\nLarge models were pre-trained on the data used for\nXLNet, which\nextends the BERT dataset to 33B tokens by including data from ClueWeb,\nCommonCrawl, and Gigaword. For fine-tuning on GLUE, a simple linear\nclassifiers is added on top of ELECTRA.\n\nThe following table compares various small models on the GLUE dev set.\nBERT-Small/Base use the same hyper-parameters as ELECTRA-Small/Base. The\ntable shows that ELECTRA performs better than BERT scoring 5 GLUE points\nhigher.\n\n\n    \n\n\n\n  Note:\nIn the original\nBERT paper, there\nwere no BERT~Small~. They created BERT~Small~ using a smaller\nhyper-parameters of BERT~Base~; they reduced the sequence length (from\n512 to 128), with smaller batch size (from 256 to 128), smaller hidden\ndimension size (from 768 to 256), and smaller token embeddings (from 768\nto 128). To provide a fair comparison, they created BERT-Small model\nusing the same hyper-parameters and trained it 1.5M steps, so it uses\nthe same training FLOPs as ELECTRA-Small, which was trained for 1M\nsteps.\n\n\nThe following table compares ELECTRA-Large with BERT~Large~ on the GLUE\ndev set. ELECTRA-Large was trained for longer steps; ELECTRA-400k was\ntrained for 400k steps which is 25% of RoBERTa training time and\nELECTRA-1.74M was trained for 1.74M steps which is similar to RoBERTa\ntraining time. The table shows that ELCETRA-400k performs comparably to\nRoBERTa and XLNet and ELECTRA-1.75M outperforms the other models on\nvarious tasks.\n\n\n    \n\n\nSame applies for SQuAD benchmark, ELECTRA-1.75M scores better than all\nmasked-language modelings given the same compute resources:\n\n\n    \n\n\n\n  Note:\nThe following is the complete list of all hyper-parameters used for\nELECTRA on pre-training (left) and fine-tuning (right):\n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "Longformer: Long Transformer",
        "url"       : "/language-modeling/Longformer",
        "date"      : "10/04/2020",
        "content": "Transformer-based models are unable to process long sequences due to\ntheir self-attention operation, which has a time complexity of\n$O\\left( n^{2} \\right)$ where $n$ is the input length. Longformer stands\nfor “Long Transformer” which is a encoder-side transformer with a novel\nattention mechanism that scales linearly with sequence length making it\neasy to process documents of thousands of tokens or longer. Longformer\nwas proposed by Allen Institute in 2020 and published in their paper:\nLongformer: The Long-Document\nTransformer. The official code\nfor this paper can be found in the official GitHub page of Allen\nInstitute: allenai/longformer.\n\nAttention Patterns\n\nLongformer sparsifies the full-attention mechanism matrix according to\nan “attention pattern” specifying pairs of input locations attending to\none another. According to the paper, there are four different patterns\nconsidered as shown below:\n\n\n  Sliding Window:\nThis is a fixed-window attention surrounding each token. Using\nmultiple stacked layers of such windowed attention results in a\nlarge receptive field. Given a fixed window size $w$, each token\nattends to $\\frac{1}{2}w$ tokens on each side. The computation\ncomplexity of this pattern is $O\\left( n \\times w \\right)$. In a\ntransformer with $l$ layers, the receptive field size at the top\nlayer is $l \\times w$ (assuming $w$ is fixed for all layers).\n\n\n\n    \n\n\n\n  Dilated Sliding Window:\nTo further increase the receptive field without increasing\ncomputation, the sliding window can be “dilated” where the window\nhas gaps of size dilation $d$. Assuming a fixed $d$ and $w$ for all\nlayers, the receptive field is $l \\times d \\times w$, which can\nreach tens of thousands of tokens even for small values of $d$.\n\n\n\n    \n\n\n\n  Global Attention:\nThe windowed and dilated attention attend to subset of the\nsequence. That’s why they decided to add “global attention” on few\npre-selected input tokens. This figure shows an example of a sliding\nwindow attention with global attention at a few tokens at custom\nlocations.\n\n\n\n    \n\n\n\n  Note:\n\n  \n    \n      In multi-headed setup of dialted window attention, using different\n  dilation configurations per head improves performance by allowing\n  some heads without dilation to focus on local context, while others\n  with dilation focus on longer context.\n    \n    \n      Implementing Longformer’s dilated sliding widow attention requires a\n  form of banded matrix multiplication (matrix multiplication where\n  the output is all zero except certain diagonals) that is not\n  directly supported in existing deep learning libraries like\n  PyTorch/Tensorflow. So, they tried to implement it in three\n  different ways:\n\n      \n        \n          Longformer-loop: Naive implementation using for-loops in PyTorch.\n        \n        \n          Longformer-chunks: Chunks $Q$ and $K$ into overlapping blocks of\n  size $w$ and overlap of size $\\frac{1}{2}w$, multiplies the blocks,\n  then mask out the diagonals.\n        \n        \n          Longformer-cuda: is a custom CUDA kernel that they implemented\n  using TVM (Tensor Virtual Machine).\n        \n      \n\n      They compared these different implementations according to time and memory\nand found out that longformer-chunks is the fastest.\n\n      \n    \n\n    \n    \n      They used small window sizes for the lower layers and increase window\n  sizes as they moved to higher layers.\n    \n    \n      Also, they didn’t use dilated sliding windows for lower layers to\n  maximize their capacity to learn and utilize the immediate local\n  context. For the higher layers, they used a small amount of\n  increasing dilation only on $2$ heads. This gives the model the\n  ability to directly attend to distant tokens without sacrificing\n  local context.\n    \n  \n\n\nCharacter-level LM\n\nTraining Longformer is done over 5 phases where they started with a\nshort sequence length of 2,048 and small window size, then the attention\nwindow size and sequence length across multiple is doubled on each\nsubsequent phase while halving the learning rate till they ended with a\nlength of 23,040 on the last phase.\n\nLongformer implementation is based on the\nTransformer-XL\nfound here with the\nmemory mechanism disabled. They used relative position embeddings with\nsinusoidal weights. They used two different model sizes; each with a\ndifferent set of hyper-parameters:\n\n\n    \n\n\nTo compare longformer with previous character-level language modeling,\nthey trained it on text8 &amp;amp; enwik8 benchmark, both contain 100M\ncharacters from Wikipedia split into 90M, 5M, 5M for train, dev, test\nrespectively. Longformer outperforms all other models and achieves\nstate-of-the-art results on both datasets:\n\n\n    \n\n\nPre-training &amp;amp; Fine-tuning\n\nLongformer was pre-trained using masked language modeling (MLM), where\nthe goal is to recover randomly masked tokens in a sequence. Since MLM\npre-training is expensive, they continued pre-training from the\nRoBERTa released\ncheckpoint, and only making the minimal changes necessary to support\nLongformer’s attention mechanism. And since RoBERTa’s input is limited\nto $512$, they decided to copy them till it matches the input to\nlongformer. After that, longformer was pre-trained on the following\ndata:\n\n\n    \n\n\nVery Important Note:\nLongformer’s attention pattern can be plugged into any pre-trained\ntransformer model without the need to change the model architecture.\n\nAfter pre-training, longformer was fine-tuned on on six tasks resulting\nin a model that can process sequences up to 4,096 tokens long (8 times\nlonger than BERT). These six tasks are QA (WikiHop, TriviaQA, HotpotQA),\nCoreference Resolution () and document classification (IMDB,\nHyperpartisan):\n\n\n    \n\n\nLED\n\nLED stands for “Longformer Encoder Decoder” which is a variant of the\nLongformer model that follows an encoder-decoder architecture similar to\nthe original\nTransformer\nmodel; instead of an encoder-only Transformer architecture as the\nLongformer model. LED is intended for long sequence-to-sequence tasks\nsuch as text summarization.\n\nSince pre-training LED is expensive, they initialized LED parameters\nfrom the BART\nfollowing BART’s exact architecture in terms of number of layers and\nhidden sizes. The only difference is that they extend position embedding\nto 16K tokens (BART has only 1K tokens). Also, they initialized the new\nposition embedding matrix by repeatedly copying BART’s 1K position\nembeddings 16 times.\n\nFollowing BART, they released two model sizes, LED-base and LED-large,\nwhich respectively have 6 and 12 layers in both encoder and decoder\nstacks. LED was evaluated on the summarization task using the arXiv\nsummarization dataset which focuses on long document summarization in\nthe scientific domain. The following table shows that LED-large achieves\nstate-of-the-art results, slightly outperforming\nBigBird.\n\n\n\n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "ETC: Extended Transformer Construction",
        "url"       : "/language-modeling/ETC",
        "date"      : "17/04/2020",
        "content": "ETC stands for “Extended Transformer Construction” which is a new\nTransformer architecture for language modeling over long sentences and\nachieves state-of-the-art performance on various long-sentence tasks as\nshown in the following table. ETC was proposed by Google in 2020 and\npublished in this paper: “ETC: Encoding Long and Structured Inputs in\nTransformers”. The official code\nfor this paper can be found on Google Research’s official GitHub\nrepository: research-etc-model\n.\n\n\n    \n\n\nMany variants of the original\nTransformer\nmodel have been proposed for language modeling such as\nBERT,\nRoBERTa,\nALBERT, or even\nT5 limit inputs to\n$n = 512$ tokens due to the $O\\left( n^{2} \\right)$ cost of attention.\nETC scales to longer input sentences up to $n = 8192$ tokens or more.\nETC follows the encoder-side of the original Transformer architecture\nwith three key modifications to tackle long inputs:\n\n\n  \n    Relative Position Encoding\n  \n  \n    Global-local Attention.\n  \n  \n    CPC pre-training task.\n  \n\n\nRelative Position Encoding\n\nInspired by the work of this paper: Self-attention with relative\nposition representations, ETC\nreplaces absolute position encodings with relative position encodings.\nThere will be $2k + 1$ relative positions given a maximum clipping\ndistance $k$, no matter the length of the input sentence. Given the\ninput sequence $x = \\left\\{ x_{1},\\ …\\ x_{n} \\right\\}$, the relative\nposition between two tokens $x_{i}$ and $x_{j}$ will be:\n\n\\[\\left\\{ \\begin{matrix}\nl_{j - i}\\ \\ \\ \\ \\  - k &amp;lt; j - i &amp;lt; k \\\\\nl_{- k}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ j - i \\leq - k \\\\\nl_{k}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ j - i \\geq - k \\\\\n\\end{matrix} \\right.\\]\n\nEach relative position then becomes a learnable vector $a_{l}^{K}$,\nwhich modifies the attention mechanism as we are going to see later.\nNow, we can clearly see that these relative position encodings are\nindependent of input length, so it is easy to adapt a model to greater\ninput lengths.\n\nGlobal-local Attention\n\nGiven the input sequence $x = \\left\\{ x_{1},\\ …\\ x_{n} \\right\\}$, ETC\nmodel will split it into two separate sequences:\n\n\n  The long input $x^{l}$: which contains the input that standard\nTransformer would get.\n\n\n\\[x^{l} = \\left\\{ x_{1}^{l},\\ ...x_{n_{l}}^{l} \\right\\}\\]\n\n\n  The global input $x^{g}$: which is a much smaller number of\nauxiliary tokens $\\left( n_{g} \\ll n_{l} \\right)$:\n\n\n\\[x^{g} = \\left\\{ x_{1}^{g},\\ ...x_{n_{g}}^{g} \\right\\}\\]\n\nThen, attention will be split into four separate pieces:\nglobal-to-global (g2g),\nglobal-to-long (g2l),\nlong-to-global (l2g),\nand long-to-long (l2l).\nAttention in the l2l piece is restricted to a fixed radius as we can see\nin the following figure (part c); because it is the most computationally\nexpensive part of the whole attention mechanism.\n\n\n    \n\n\nAttention in ETC is\n$O\\left( n_{g}\\left( n_{g} + n_{l} \\right) + n_{l}\\left( n_{g} + 2r + 1 \\right) \\right)$.\nSince $n_{l} \\gg n_{g},\\ r$, then the attention mechanism becomes\n$O\\left( n_{g}^{2} + n_{g}n_{l} \\right)$. As we can see, the attention\nmechanism is linear in the size of the long input . The attention\nmechanism is applied via the following steps:\n\n\n  \n    Per-instance, four Boolean attention matrices\n$M^{g2g} \\in \\mathbb{R}^{n_{g} \\times n_{g}}$,\n$M^{g2l} \\in \\mathbb{R}^{n_{g} \\times n_{l}}$,\n$M^{l2g} \\in \\mathbb{R}^{n_{l} \\times n_{g}}$, and\n$M^{l2l} \\in \\mathbb{R}^{n_{l} \\times n_{l}}$are created with zeroes\nfor those pairs of tokens that should not attend to one another and\nones for the rest.\n  \n  \n    Given the global input $x^{g}$ which is a sequence of token\nrepresentations $x_{i}^{g} \\in \\mathbb{R}^{d_{x}}$, a large constant\n$C = 10,000$ (in the paper), $a_{\\text{ij}}^{K}$ learnable vectors\nrepresenting the relative positions, and $W^{Q},\\ W^{K}$ learnable\nweight matrices, the attention embedding between $x_{i}$ and $x_{j}$\nin the global-to-global part can be calculated as:\n  \n\n\n\\[e_{\\text{ij}}^{g2g} = \\frac{x_{i}^{g}W^{Q}\\left( x_{j}^{g}W^{K} + a_{\\text{ij}}^{K} \\right)^{T}}{\\sqrt{d_{z}}} - \\left( 1 - M_{\\text{ij}}^{g2g} \\right)C\\]\n\n\n  A softmax is used to calculate $\\alpha_{\\text{ij}}^{g2g}$:\n\n\n\\[\\alpha_{\\text{ij}}^{g2g} = \\frac{\\exp\\left( e_{\\text{ij}}^{g2g} \\right)}{\\sum_{l = 1}^{n}{\\exp\\left( e_{\\text{il}}^{g2g} \\right)}}\\]\n\n\n  The attention output of the global-to-global part is\n$z^{g} = \\left\\{ z_{1}^{g},\\ …z_{n_{g}}^{g} \\right\\}$ where\n$z_{i}^{g} \\in \\mathbb{R}^{d_{z}}$ which uses $W^{V}$ as a learnable\nweight matrix is calculated as follows:\n\n\n\\[z_{i}^{g} = \\sum_{j = 1}^{n_{g}}{\\alpha_{\\text{ij}}^{g2g}.x_{j}^{g}.W^{V}}\\]\n\n\n  \n    Attention for the other 3 pieces is analogous to this one.\n  \n  \n    At the end, a single softmax is used to jointly calculate\n$\\alpha_{\\text{ij}}^{g2g}$ and $\\alpha_{\\text{ij}}^{g2l}$ while\nanother one for $\\alpha_{\\text{ij}}^{l2g}$ and\n$\\alpha_{\\text{ij}}^{l2l}$. I DON’T KNOW HOW :(\n  \n  \n    Thus, the output of global-local attention is a sequence of length\n$n_{g}$ and one of length $n_{l}$. These sequences go through a\nlayer normalization and feed forward layer in the same way as in the\nstandard transformer.\n  \n\n\nCPC\n\nETC model uses two pre-training tasks. The first one is Masked Language\nModeling (MLM) with whole word masking which means if one word piece\ntoken is masked, then all other tokens of the same word are masked. The\nsecond one is Contrastive Predictive Coding (CPC). The goal of CPC is to\npredict subsequent inputs in latent space, i.e., to predict internal\nhidden representations of blocks of tokens. We adapted this idea in ETC\nby using global input sentence summary tokens.\n\nGiven an input sequence containing $n$ sentences, we mask all the tokens\ncorresponding to a subset of sentences (but leave the sentence summary\ntokens in the global input). Then, the model is trained to minimize the\ndifference between the hidden representation of the global sentence\nsummary tokens for the masked sentences with respect to that of a global\nsummary token that can see the unmasked sentence and nothing else. A\nNoise Contrastive Estimation (NCE) loss is used:\n\nExperiments\n\nETC model was initialized using RoBERTa parameters. This is doable since\nBERT’s attention is a special case of the global-local attention used in\nETC. Same as RoBERTa, they created two basic configurations:\n\n\n\n    \n        \n            \n            Layers\n            Hidden Size\n            Attention Heads\n            # Parameters\n            $$r$$\n            $$k$$\n        \n    \n    \n        Base\n        12\n        768\n        12\n        166 M\n        84\n        12\n    \n    \n        Large\n        24\n        1024\n        16\n        558 M\n        169\n        24\n    \n\n\n\nFor pre-training; they used original BERT datasets, except that\ndocuments with fewer than 7 sentences were filtered out with WordPiece\nvocabulary of 30k uncased wordpieces . ETC-base was pre-trained with the\nsame total number of tokens as the original BERT, while ETC-large was\npre-trained with twice as many. As an optimizer, they used LAMB\noptimizer with learning rate set to $\\sqrt{8} \\times 10^{- 3}$.\n\nThe following are the results obtained by ETC in various benchmarks; the\ndefaulted configuration is marked with “-“:\n\n\n  Natural Questions (NQ):\n\n\n\n    \n\n\n\n  OpenKP:\n\n\n\n    \n\n\n\n  HotpotQA and WikiHop:\n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "Adapter Fusion",
        "url"       : "/language-modeling/adapter_fusion",
        "date"      : "01/05/2020",
        "content": "AdapterFusion is a new variant of the Adapter layers\n where it extends\nthe functionality of adapters to be multi-tasking instead of being per a\nsingle task. AdapterFusion is proposed by researchers in UKP Lab,\nTechnical University of Darmstadt and New York University and\npublished in their paper: AdapterFusion: Non-Destructive Task\nComposition for Transfer Learning\nin May 2020.\n\nAdapterFusion is a novel two stage learning algorithm that shares knowledge\nacross multiple tasks while avoiding catastrophic forgetting. The AdapterFusion\narchitecture, illustrated in the following figure, has two components:\n\n\n  \n    Adapter: trained on a task\n  \n  \n    AdapterFusion layer: combines the representations from several\nadapters in order to improve the performance on the target task.\n  \n\n\n\n    \n\n\nTask Definition\n\nGiven a single-task model that is pre-trained on a task with training\ndata $D_{0}$ and a loss function $L_{0}$, the weights $\\Theta_{0}$ of\nthis model are learned as follows:\n\n\\[\\Theta_{0} = \\underset{\\Theta}{\\arg\\min}{L_{0}\\left( D_{0};\\Theta \\right)}\\]\n\nGiven a multi-task model that is pre-trained on a set of $N$ tasks\nhaving labeled data of varying sizes and different loss functions\n$C = \\left\\{ \\left( D_{1},L_{1} \\right),\\ …\\left( D_{N},L_{N} \\right) \\right\\}$,\nthe aim of the model is to leverage the set of $N$ tasks and learn a\nshared representation $\\Theta_{0 \\rightarrow \\left\\{ 1,\\ …N \\right\\}}$\nthat will enable the model to generalize better on each task; this is\nusually obtained by starting with an initial parameters $\\Theta_{0}$ and\nfine-tune on tasks $\\left\\{ 1,\\ …N \\right\\}$:\n\n\\[\\Theta_{0 \\rightarrow \\left\\{ 1,\\ ...N \\right\\}} = \\underset{\\Theta}{\\arg\\min}\\left( \\sum_{n = 1}^{N}{L_{n}\\left( D_{n};\\Theta_{0} \\right)} \\right)\\]\n\nIn AdapterFusion, the aim is to be able to leverage a set of $N$ tasks\nto improve on a target task $m$ with\n$C_{m} = \\left( D_{m},L_{m} \\right)$ where\n$m \\in \\left\\{ 1,\\ …N \\right\\}$. This is done in two stags:\n\n\n  Knowledge Extraction:\nWe train different adapters for each of the N tasks obtaining:\n\n\n\\[\\left\\{ \\Phi_{1},\\ ...,\\ \\Phi_{N} \\right\\}\\]\n\n\n  Knowledge Composition:\nWe combine the set of $N$ adapters using AdapterFusion Layer while\nfixing both the model parameters $\\Theta$ as well as all adapters\n$\\Phi$ obtaining parameters $\\Psi$ that learn to combine the $N$\ntask adapters to solve the target task\n$C_{m} = \\left( D_{m},L_{m} \\right)$:\n\n\n\\[\\Psi_{m} = \\underset{\\Psi}{\\arg\\min}{L_{m}\\left( D_{m};\\Theta,\\ \\Phi_{1},\\ ...\\Phi_{N},\\ \\Psi \\right)}\\]\n\n\n  Note:\nThe training dataset of the target task $m$ is used twice: once for\ntraining the adapters $\\Phi_{m}$ and again for training Fusion\nparameters $\\Psi_{m}$ which learns to compose the information stored in\nthe $N$ task adapters.\n\n\nAdapterFusion Layer\n\nAs discussed earlier, AdapterFusion learns to compose the $N$ task adapters\n$\\left\\{ \\Phi_{1},\\ …,\\ \\Phi_{N} \\right\\}$ and the shared pre-trained\nmodel $\\Theta$, by introducing a new set of weights $\\Psi$. As\nillustrated in the following figure, they defined the AdapterFusion\nparameters $\\Psi$ to consist of Key, Value and Query matrices at each\nlayer $l$, denoted by $K_{l}$ , $V_{l}$ and $Q_{l}$ respectively.\n\n\n    \n\n\nAt each layer $l$ of the transformer and each time-step $t$, the output\nof the feed-forward sub-layer of layer $l$ is taken as the query vector.\nThe output of each adapter $z_{l,t}$ is used as input to both the value\nand key transformations. Similar to the attention mechanism, we learn a\ncontextual activation of each adapter $n$ using the following formula\nwhere $n \\in \\left\\{ 1,\\ …N \\right\\}$ , $\\bigotimes$ represents dot\nproduct, $\\left\\lbrack .,. \\right\\rbrack$ indicates the concatenation of\nvectors, and $z^{T}$ is the transpose of $z$:\n\n\\[s_{l,t} = \\text{softmax}\\left( h_{l,t}^{T}Q_{l}\\ \\bigotimes\\ z_{l,t,n}^{T}K_{l} \\right)\\]\n\n\\[{z&#39;}_{l,t,n} = z_{l,t,n}^{T}V_{l}\\]\n\n\\[{Z&#39;}_{l,t} = \\left\\lbrack {z&#39;}_{l,t,0},\\ ...{z&#39;}_{l,t,N} \\right\\rbrack\\]\n\n\\[o_{l,t} = s_{l,t}^{T}{Z&#39;}_{l,t}\\]\n"
      },
    
      
      
      {
        "collection": "Language Modeling",
        "title"     : "GPT-3",
        "url"       : "/language-modeling/GPT-3",
        "date"      : "28/05/2020",
        "content": "GPT-3 is an enormous model built on the transformer-decoder architecture\npublished in 2020 by OpenAI in this paper: “Language Models are\nFew-Shot Learners” whose title is\nvery indicative of what the paper wanted to show. The paper didn’t\nprovide any new architecture, they used the same architecture as GPT-2.\nThey just made it way bigger and trained over more data.\n\nThe whole purpose of this paper is to show that GPT-3 can be used with a\nvariety of tasks using either zero-shot, or one-shot or a few-shots\nlearning schemes and even reaching competitiveness with prior\nstate-of-the-art fine-tuned models. Before getting into more details\nabout the model, let’s first discuss what do I mean by these learning\nschemes and how they are different from fine-tuning:\n\n\n  Few-shot (FS):\nIt’s the setting where the model is given K (usually from 10\nto 100) examples of the task at inference time as conditioning,\nbut no weight updates are allowed. As we can see in the following\nfigure, GPT-3 was given three different examples along with\nthe task description:\n\n\n\n    \n\n\n\n  One-shot (1S):\nIt’s the same as few-shot except that only one\ndemonstration is allowed, in addition to the task description. The\nreason to distinguish one-shot from few-shot is that it most\nclosely matches the way in which some tasks are communicated to\nhumans:\n\n\n\n    \n\n\n\n  Zero-shot (0S):\nIt’s the same as one-shot except that no demonstrations are\nallowed, just the task description. This method provides maximum\npotential for robustness but is also the most challenging setting\neven for humans.\n\n\n\n    \n\n\n\n  Fine-Tuning (FT):\nIt has been the most common approach in recent years, and involves\nupdating the weights of a pre-trained model by training on a\nsupervised dataset specific to the desired task. This setting\nlacks from poor generalization out-of-distribution:\n\n\n\n    \n\n\nModel\n\nAs said earlier, they used the same model and architecture as GPT-2. To\nstudy the dependence of performance on model size, they trained 8\ndifferent sizes of model as shown in the following table\n\n\n    \n\n\nWhere:\n\n\n  \n    $n_{\\text{params}}$: is the total number of trainable parameters.\n  \n  \n    $n_{\\text{layers}}$: is the total number of layers.\n  \n  \n    $d_{\\text{model}}$: is the number of units in each bottleneck layer\n(we always have the feed-forward layer four times the size of the\nbottleneck layer,\n$d_{\\text{feedforward}} = 4 \\times d_{\\text{model}}$).\n  \n  \n    $n_{\\text{heads}}$: is the number of attention heads/layers, since\neach layer has just one attention head.\n  \n  \n    $d_{head}$: is the dimension of each attention head.\n  \n\n\nAs you can see, GPT3 is massive as its context-widow\n$n_{\\text{ctx}} = 2048$ tokens wide with about 175 billion learnable\nparameters spread over 96 transformer-decoder layers.\n\n\n    \n\n\nThe data used for this models are according to the following table\n\n\n    \n\n\nAnd the following is a comparison between the training time taken to\ntrain BERT, RoBERTa, T5 and GPT-3. As we can see from the graph, it took\nalmost 5000 days to train GPT-3.\n\n\n    \n\n\nResults\n\nThe following is a comparison among the different learning schemes used\nwith GPT-3 and the state or the art (fine-tuned) model on various tasks:\n\n\n  \n    Language Modeling:\n\n    \n      \n        Dataset: Penn Tree Bank\n      \n      \n        Evaluation Metric: perplexity\n      \n    \n  \n\n\n\n    \n\n\n\n  \n    Long-Range Language Modeling:\n\n    \n      \n        Dataset: LAMBADA\n      \n      \n        Evaluation Metric: perplexity / Accuracy\n      \n    \n  \n\n\n\n    \n\n\n\n  \n    Story Completion:\n\n    \n      \n        Dataset: StoryCloze &amp;amp; HellaSwag\n      \n      \n        Evaluation Metric: Accuracy\n      \n    \n  \n\n\n\n    \n\n\n\n  \n    Question Answering:\n\n    \n      \n        Dataset: NaturalQS, WebQS &amp;amp; TriviaQA\n      \n      \n        Evaluation Metric: Accuracy\n      \n    \n  \n\n\n\n    \n\n\n\n  \n    Machine Translation:\n\n    \n      \n        Dataset: WMT’14 (Fr↔En), WMT’16 (De↔En) &amp;amp; WMT’16 (Ro↔En).\n      \n      \n        Evaluation Metric: BLEU\n      \n    \n  \n\n\n\n    \n\n\n\n  \n    Winograd-Style Tasks: determining to which word a\npronoun refers\n\n    \n      \n        Dataset: Winograd &amp;amp; WinogradXL\n      \n      \n        Evaluation Metric: Accuracy\n      \n    \n  \n\n\n\n    \n\n\n\n  \n    Common Sense Reasoning:\n\n    \n      \n        Dataset: PIQA, ARC, OpenBookQA\n      \n      \n        Evaluation Metric: Accuracy\n      \n    \n  \n\n\n\n    \n\n\n\n  \n    Reading Comprehension:\n\n    \n      \n        Dataset: CoQA, DROP, QuAC, SQuADv2, RACE-h, RACE-m.\n      \n      \n        Evaluation Metric: Accuracy for RACE-h &amp;amp; RACE-m, and F1 for\nthe rest.\n      \n    \n  \n\n\n\n    \n\n"
      },
    
  
  
    
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Attention Mechanism",
        "url"       : "/machine-translation/Attention",
        "date"      : "01/09/2014",
        "content": "A potential issue with the Seq2Seq approach is that a neural network\nneeds to be able to compress all the necessary information of a source\nsentence into a fixed-length vector (context vector). This may make it\ndifficult for the neural network to cope with long sentences, especially\nthose that are longer than the sentences in the training corpus. This\npaper: “On the Properties of Neural Machine Translation:\nEncoder–Decoder Approaches”\nshowed that indeed the performance of a basic encoder–decoder\ndeteriorates rapidly as the length of an input sentence increases.\n\n\n    \n\n\nWhat we can see from the previous graph is that it works quite well for\nshort sentences, so we might achieve a relatively high BLEU score; but\nfor very long sentences, maybe longer than 30 or 40 words, the\nperformance comes down.\n\nIn order to address this issue, this paper “Neural Machine Translation\nby Jointly Learning to Align and\nTranslate” introduced an extension\nto the encoder–decoder model called “Attention mechanism” in 2014. The\nmost important distinguishing feature of this approach from the basic\nencoder–decoder is that it does not attempt to encode a whole input\nsentence into a single fixed-length vector. Instead, it encodes the\ninput sentence into a sequence of vectors and chooses a subset of these\nvectors adaptively while decoding the translation. This frees a neural\ntranslation model from having to squash all the information of a source\nsentence, regardless of its length, into a fixed-length vector. We show\nthis allows a model to cope better with long sentences.\n\nHere, we&#39;ll see the Attention Model which translates maybe a bit more\nlike humans. The way a human translator would translate a sentence is\nnot to first read the whole sentence and then memorize it and then\nregurgitate an English sentence from scratch. Instead, a human\ntranslator reads the first part of the given sentence, maybe generate\npart of the translation. Look at the second part, generate a few more,\nand so on. We kind of work part by part through the sentence, because\nit&#39;s just really difficult to memorize the whole long sentence like\nthat.\n\nIn the following parts, we will talk about the different variants of\nattention mechanisms that have been used in the field so far.\n\nGlobal Soft Attention\n\nAs we said earlier, the attention mechanism was first introduced by\nDzmitry Bahdanau, KyungHyun Cho and Yoshua Bengio in 2014\nand published in this paper: “Neural Machine Translation by Jointly\nLearning to Align and Translate”\nunder the name “Neural Machine Translation By Jointly Learning To Align\nAnd Translate” which later defined as a global, soft attention. Here, we\nare going to explain the attention mechanism in more details as\nexplained in this paper: “Effective Approaches to Attention-based\nNeural Machine Translation” looks\npretty similar to the one mentioned earlier.\n\nTo explain how the attention mechanism works, let’s consider that we\nhave the following:\n\n\n  \n    The source sequence which is of length $n$ given by $x = \\left[ x_{1},…x_{n} \\right]$.\n  \n  \n    The target sequence which is of length $m$ given by $y = \\left[ y_{1},…y_{m} \\right]$.\n  \n  \n    Encoder hidden states $s = \\left[ s_{1},…s_{n} \\right]$.\n  \n  \n    Decoder hidden states $y = \\left[ h_{1},…h_{m} \\right]$.\n  \n\n\nNow, the attention mechanism works as shown in the following figure:\n\n\n    \n\n\nWhose steps goes like the following:\n\n\n  First, we calculate the alignment score by using the  attention score function \ndefined as $\\text{score}$ between the $j^{th}$\nencoder hidden state $s_{j}$ and the $i^{th}$ decoder hidden state $h_{i}$.\n\n\n\\[a_{i,j} = \\text{softmax}\\left( \\text{score}\\left( h_{i},\\ s_{j} \\right) \\right)\\]\n\nIn the next part, we are going to see the different variants of the\n$\\text{score}$ function.\n\n\n  Next, the context vector $c_{i}$ at position $i$ can be calculated as the\nweighted average of previous encoder hidden states and alignment vector $a_{i,j}$.\n\n\n\\[c_{i} = \\sum_{j = 0}^{n}{a_{i,\\ j}\\text{.}s_{j}}\\]\n\n\n  The source side context vector $c_{i}$ and the hidden state $h_{i}$ are\nconcatenated $\\left\\lbrack c_{i}\\ ;\\ h_{i} \\right\\rbrack$ and the non-linear\n$\\tanh$ activation function is applied to give the attention hidden vector \n $\\widetilde{h}_i$  where $W_{c}$ weights are learned in the training process:\n\n\n\\[{\\widetilde{h}}_{i} = \\tanh\\left( W_{c}.\\left\\lbrack c_{i}\\ ;\\ h_{i} \\right\\rbrack \\right)\\]\n\n\n  The attention hidden vector ${\\widetilde{h}}_{i}$ is passed through a\n$\\text{softmax}$ function to generate the probability distribution given by the\nfollowing formula where $W_{s}$ weights are learned in the training process:\n\n\n\\[P\\left( y_{i} \\middle| y_{&amp;lt; i},\\ x \\right) = \\text{softmax}\\left( W_{s}.{\\widetilde{h}}_{i} \\right)\\]\n\nScore Functions\n\nIn this part, we are going to enumerate the different score functions\n$\\text{score}\\left( h_{i},\\ s_{j} \\right)$ used in different papers\nwhich gives different flavors of the attention mechanism where:\n\n\n  \n    $s_{j}$: is the $j^{th}$ hidden state in the encoder architecture.\n  \n  \n    $h_{i}$: is the $i^{th}$ hidden state in the decoder architecture.\n  \n  \n    $n$: is the number of tokens in the encoder.\n  \n\n\n\n\n    \n        \n            Name\n            Function\n            Parameters\n            Reference\n        \n    \n    \n        Concat (additive)\n        $$v_{a}^T.tanh\\left( W_{a}\\left\\lbrack s_{j};h_{i} \\right\\rbrack \\right)$$\n        $$W_{a}$$\n        Luong et al.\n    \n    \n        Linear (additive)\n        $$v_{a}^{T}.\\tanh\\left( W_{a}s_{j} + U_{a}h_{i} \\right)$$\n        $$W_{a},\\ U_{a}$$\n        Bahdanau et al.\n    \n    \n        Bilinear (multiplicative)\n        $$h_{i}^{T}.W_{a}.s_{j}$$\n        $$W_{a}$$\n        Luong et al.\n    \n    \n        Dot (multiplicative)\n        $$h_{i}^{T}.s_{j}$$\n        -\n        Luong et al.\n    \n    \n        Scaled dot (multiplicative)\n        $$\\frac{1}{\\sqrt{n}}\\left( h_{i}^{T}.s_{j} \\right)$$\n        -\n        Vaswani et al.\n    \n    \n        Location-based\n        $$\\text{softmax}\\left( W_{a}.h_{i}^{T} \\right)$$\n        $$W_{a}$$\n        Luong et al.\n    \n\n\n\nNotes:\n\n\n  \n    The multiplicative and additive score functions generally give similar results.\n  \n  \n    The multiplicative score functions are faster in both computation and\nspace-efficiency since it is using efficient matrix multiplication techniques.\n  \n  \n    The additive score function performs much better when the input dimension is large.\n  \n\n\nSoft Vs. Hard Attention\n\nThe only difference between them is that hard attention picks one of the\nencoder hidden states $s = \\left[ s_{1},\\ …\\ s_{n} \\right]$ rather\nthan a weighted average over all the inputs as in the soft attention\ndoes. The hard attention is given by:\n\n\\[c_{i} = \\underset{a_{i,j}}{\\arg\\max}\\left\\{ s_{1},s_{2},...s_{n} \\right\\}\\]\n\nWhile the soft attention is given by:\n\n\\[a_{i,j} = \\text{softmax}\\left( \\text{score}\\left( h_{i},\\ s_{j} \\right) \\right)\\]\n\n\\[c_{i} = \\sum_{j = 0}^{n}{a_{i,\\ j}\\text{.}s_{j}}\\]\n\nGlobal Vs. Local Attention\n\nThe global attention, discussed before, is called “global” because each\ndecoder hidden state takes $h_{i}$ into consideration “all” of the\nencoder hidden states $s = \\left[ s_{1},…s_{n} \\right]$ while\ncomputing the context vector $c_{i}$. This can be both computationally\nexpensive and many times impractical when the source length $n$ is\nlarge.\n\nLuong et al. in their paper: “Effective Approaches to Attention-based\nNeural Machine Translation”\nintroduced the local attention which considers a small window of size\n$D$ of the encoder hidden states\n$s = \\left[ s_{p_{i} - D},…s_{p_{i} + D} \\right]$ when computing the\ncontext vector $c_{i}$ instead of all the hidden states.\n\n\n    \n\n\nWhose steps goes like the following:\n\n\n  First, the model predicts an aligned position $p_{i}$ for\neach target word in the decoder at time $i$ using the following formula:\n\n\n\\[p_{i} = n.\\text{sigmoid}\\left( v_{p}^{T}.\\tanh\\left( W_{p}.h_{i} \\right) \\right)\\]\n\nWhere $W_{p}$ and $v_{p}$ are the model parameters to be learned to\npredict the position and $n$ is the length of the source sequence and.\nAnd $p_{i}$ is a number within $\\lbrack 0,n\\rbrack$.\n\n\n  Next, the alignment score is going to be calculated for each position $s$\nin the predefined window as before. However, to favor alignment points near\n$p_{i}$, we place a Gaussian distribution centered around $p_{i}$ and with\nstandard deviation $\\sigma = \\frac{D}{2}$. Now, our alignment weights are\ndefined as:\n\n\n\\[a_{i,j} = \\text{softmax}\\left( \\text{score}\\left( h_{i},\\ s_{j} \\right) \\right)\\exp\\left( - \\frac{\\left( s - p_{i} \\right)^{2}}{2\\sigma^{2}} \\right)\\]\n\n\n  Then, the context vector $c_{i}$ is then derived as a weighted average over\nthe set of source hidden states within the window\n$\\left\\lbrack p_{i} - D,p_{i} + D \\right\\rbrack$ where $D$ is the window size.\n\n\n\\[c_{i} = \\sum_{j = p_{i} - D}^{p_{i} + D}{a_{i,\\ j}\\text{.}s_{j}}\\]\n\nKey-Value Attention\n\nKey-value attention is first created by Daniluk et\nal. in 2017 which is another\nvariant of attention mechanism which splits the encoder hidden layers\ninto key-value pairs where the keys are used for attention distribution\nand the values for context representation.\n\n\n    \n\n\nNow, if the attention mechanism is self-attention, then we will\ncreate another vector called the “query vector”. And if the attention\nmechanism is not self-attention, then the query vector will be created\nat the decoder network.\n\nThe Key-value attention mechanism can be calculated by following these\nsteps:\n\n\n  Create three vectors from word-embedding vectors. So for each word,\nwe create a Query vector, a Key vector, and a Value\nvector. These vectors are created by multiplying the embedding by\nthree matrices that can be learned.\n\n\n\n    \n\n\nWe can do that using broadcasting which could make things faster like so:\n\n\n    \n\n\n\n  Then, we apply the attention mechanism which can be summarized in\nthis equation:\n\n\n\n    \n\n\nSo, the calculations will look like so:\n\n\n    \n\n\nMulti-head Attention\n\nA multi-headed attention mechanism only means that we need to perform\nthe same attention calculation we outlined above just multiple times\nwith different weight matrices. By doing that, we end up with multiple\ndifferent Z matrices. So, we need to concatenate them together to get\none big matrix as shown below:\n\n\n  concatenate them together to get one big matrix as shown below:\n\n\n\n    \n\n\n\n  Finally, we need to multiply this big matrix with a weight matrix\nthat was trained jointly with the model to get a matrix that\ncaptures information from all attention heads.\n\n\n\n    \n\n\nHierarchical Attention\n\nThis type of architecture was proposed by Yang et al. in his paper:\nHierarchical Attention Networks for Document\nClassification\npublished in 2016. So, it’s an attention mechanism on the sentence\nlevel.\n\n[YOU CAN READ MORE IN THE REFERENCE BOOK STARTING FROM PAGE 418 .]\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Seq2Seq",
        "url"       : "/machine-translation/Seq2Seq",
        "date"      : "10/09/2014",
        "content": "Sequence-to-sequence (seq2seq) models or encoder-decoder architecture,\ncreated by IlyaSutskever and published in their paper: Sequence to\nSequence Learning with Neural Networks\npublished in 2014, have enjoyed great success in a machine translation,\nspeech recognition, and text summarization.\n\nWhen training, Seq2Seq system first reads the source sentence using a\nneural network called “encoder” to build a context Vector (the last\nRNN output in the encoder architecture) which is a sequence of numbers\nthat represents the sentence meaning. Then using another neural network\ncalled “decoder” which takes the translated words along with the context\nvector and tries to train the decoder to predict these words when given\nsimilar context vector.\n\n\n    \n\n\n\n  Note:\nWe should know that the input to both the encoder and the decoder are\nword-embedding and not the word itself. So, when translating from one\nlanguage to another, you need to have two sets of word embedding.\n\n\nWhen testing, we only have the input sentence for the encoder. So, we\nuse the encoder architecture to generate the context vector that will be\nused as the initial state for the decoder architecture to generate the\ntranslated sentence.\n\nAnd as we can see in the following image, the output of each time step\nin the decoder will be fed back to it the decoder to generate more. And\neach word generated has the highest probability among all other words in\nthe vocabulary.\n\n\n    \n\n\n\n  Notes:\n\n  \n    One of the tricks that we usually use in Machine Translation, and it\n  helps to improve the performance, is to reverse the input sentence\n  when training the model. So, if the (English, Foreign) pair is “a b\n  c” → “α β γ”, then we reverse the source sentence and keep the\n  target as it is. So, now the (English, Foreign) pair is “c b a” → “α\n  β γ”. This increases the performance as the distance between the\n  associated words become lower than before and the average distance\n  between corresponding words in the source and target is unchanged.\n  \n\n  \n    \n\n\n  \n    \n      The trend in NMT is not using so much epochs in training. The\n  seq2seq paper mentioned before uses 7.5 epochs for training $12$\n  million sentences containing 348 million English words and 304\n  million French words. And the “Effective Approaches to\n  Attention-based Neural Machine\n  Translation” paper used about\n  12 epochs.\n    \n    \n      There is a trick we can use that doubles the training speed which is\n  when creating mini-batches, we batch the short sentences (&amp;lt; 30\n  words) together and the long sentences together. Padding is also\n  added per batch.\n    \n    \n      Some people would train the encoder and the decoder separately. You\n  can do that with no problem, but it’s preferable to train them\n  altogether as an end-to-end solution.\n    \n    \n      One of the biggest problems we should avoid when creating such a\n  model is to avoid the greedy approach when dealing with the\n  generated sentence. By greedy approach, I mean choosing every word\n  based on current word probability without putting into consideration\n  the following words. That’s why we will stick with another approach\n  which is the “Beam Search”.\n    \n  \n\n\nDifferent Layers\n\nIn this part, we are going to discuss a very special case in a Seq2Seq\narchitecture, but first let’s recap its architecture. Seq2Seq system\nfirst reads the source sentence using a neural network called “encoder”\nto build a context vector(s). These vectors are being used as the\ninitial value for the hidden states in the decoder. The decoder is\nanother neural network that uses the context vector(s) to emit a\ntranslation, word by word.\n\n\n    \n\n\nIn the previous graph, the number of layers in the encoder is the same\nas the decoder. But what happens when the number of layers differ\nbetween them. How we are going to use the context vector(s) as the\ninitial value for the decoder? According to this answer on Quora, there\nare two approaches:\n\n\n  Use a single layer fully connected (FC) network between the encoder\nand decoder. The FC network has (encoder layers) number of input\nneurons and (decoder layers) number of output/hidden layer neurons.\nThis way the sizes of the encoder and decoder would be reconciled\nand you can initialize the decoder hidden states from the output of\nFC network.\n\n\n\n    \n\n\n\n  Use the encoder hidden state output as an extension to the decoder\ninput and initialize the decoder hidden states randomly. This\ntechnique is used in papers like: A Persona-Based Neural\nConversation Model.\n\n\n\n    \n\n\nBeam Search\n\nThe difference between Beam Search and Greedy Algorithm is that the\nfirst chooses a certain number of candidates for each step (based on the\nbeam width) unlike the latter which chooses only\nthe most likely candidate for each step. So, we can consider the greedy\nalgorithm as a beam search algorithm with a beam width of 1. Let’s see how we\ncan do that with our example, “Jane visite l’Afrique en septembre”, step by\nstep. First, we will form our encoder network like so:\n\n\n    \n\n\nUsing a beam width of $3$, we will choose the most likely three words\nfrom our vocabulary based on the context vector (Blue). In other words,\nwe will get the three words that has the highest\n$P\\left( y^{\\left\\langle 1 \\right\\rangle} \\middle| x \\right)$ where $x$\nis the context vector (Blue block). Let’s assume the most likely words\nare “In”, “Jane” and “September”. Now, we will have to create three\ndecoder networks, one for each word like so:\n\n\n    \n\n\nAnd for each choice of these three choices, we will consider what should\nbe the second word. We will also get the three most likely words\naccording to the probability\n$P\\left( y^{\\left\\langle 2 \\right\\rangle} \\middle| x,y^{\\left\\langle 1 \\right\\rangle} \\right)$.\nSo, we will have $3 \\ast 3 = 9$ choices for the second step. Then, we\nwill filter it down to 3 by considering the highest three probabilities\nof these nine choices. And we will keep doing that till the end of the\ngenerated (translated) sentence. If the end of sentence symbols &amp;lt;/s&amp;gt;\nis generated to one of the choices, then we stop generating words in\nthis choice.\n\nSo in the end, the Beam Search Algorithm will produce the sentence with\nthe highest probability of\n$P\\left( y^{\\left\\langle 1 \\right\\rangle}\\ldots y^{\\left\\langle T_{x} \\right\\rangle} \\middle| x \\right)$\nwhich equal to\n$P\\left( y^{\\left\\langle 1 \\right\\rangle} \\middle| x \\right) \\ast P\\left( y^{\\left\\langle 2 \\right\\rangle} \\middle| x,y^{\\left\\langle 1 \\right\\rangle} \\right) \\ast P\\left( y^{\\left\\langle 3 \\right\\rangle} \\middle| x,y^{\\left\\langle 1 \\right\\rangle},y^{\\left\\langle 2 \\right\\rangle} \\right) \\ast \\ldots P\\left( y^{\\left\\langle T_{y} \\right\\rangle} \\middle| x,y^{\\left\\langle 1 \\right\\rangle},\\ldots y^{\\left\\langle T_{y} - 1 \\right\\rangle} \\right)$.\nIn other words, the Beam Search Algorithm tries to maximize:\n\n\\[\\arg\\max_{y}\\prod_{t = 1}^{T_{y}}{P\\left( y^{\\left\\langle T_{y} \\right\\rangle} \\middle| x,\\left\\{ y^{\\left\\langle 1 \\right\\rangle},\\ldots y^{\\left\\langle T_{y} - 1 \\right\\rangle} \\right\\} \\right)}\\]\n\nOne way to optimize the previous formula is to use the logarithmic\nsummation instead of the product. So, by taking logs, we end up with a\nmore numerically stable algorithm that is less prone to numerical\nrounding errors:\n\n\\[\\arg\\max_{y}\\sum_{t = 1}^{T_{y}}{\\log\\left( P\\left( y^{\\left\\langle T_{y} \\right\\rangle} \\middle| x,\\left\\{ y^{\\left\\langle 1 \\right\\rangle},\\ldots y^{\\left\\langle T_{y} - 1 \\right\\rangle} \\right\\} \\right) \\right)}\\]\n\nNow, there&#39;s one other change to this function that makes the machine\ntranslation algorithm work even better. If we have a very long sentence,\nthe probability of that sentence is going to be small, because we’re\nmultiplying as many terms with numbers where all of them are less than 1.\n\nSo, if we multiply all the numbers that are less than 1 together, we\njust tend to end up with a smaller probability. That’s why the Beam\nSearch prefers short sentences. To avoid that, we modify the former\nequation and divide by the length of the sentence like so:\n\n\\[\\frac{1}{T_{y}^{\\alpha}} \\ast \\arg\\max_{y}\\sum_{t = 1}^{T_{y}}{\\log\\left( P\\left( y^{\\left\\langle T_{y} \\right\\rangle} \\middle| x,\\left\\{ y^{\\left\\langle 1 \\right\\rangle},\\ldots y^{\\left\\langle T_{y} - 1 \\right\\rangle} \\right\\} \\right) \\right)}\\]\n\nThe parameter $\\alpha \\in \\lbrack 0,1\\rbrack$ is a hyper-parameter for\nsmoothing the sentence length. So, if $\\alpha$ is equal to $1$, then\nwe’re completely normalizing by length. If $\\alpha$ is equal to $0$,\nthen $T_{y}$ will be $1$ which means that we’re not normalizing at all.\nSo, $\\alpha$ is somewhat in between full normalization and no\nnormalization. It’s another hyper-parameter we have within that can be\ntuned to get better results.\n\nFinally, how do we choose the beam width? The larger the beam width is,\nthe more possibilities we&#39;re considering and the better the sentence we\nwill probably find. But also the the more computationally expensive our\nalgorithm is as we&#39;re also keeping a lot more possibilities around. So,\nhow to choose that perfect value?\n\nIn production systems, it&#39;s not uncommon to see a beam width maybe\naround $10$, and a beam width of $100$ would be considered very large\nfor a production system, depending on the application. But for research\nsystems where people want to squeeze out every last drop of performance\nin order to publish the paper with the best possible result. It&#39;s not\nuncommon to see people use beam widths of $1,000$ or $3,000$.\n\nSo, to be on the safe side, we should try other variety of values of the\nbeam width as we work through our application. So, for many\napplications, we would expect to see a huge gain as you go from a beam\nwidth of $1$, which is very greedy search, to $3$, to maybe $10$. But\nthe gains as you go from $1,000$ to $3,000$ in beam width might not be\nas big.\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Fusion",
        "url"       : "/machine-translation/Fusion",
        "date"      : "11/03/2015",
        "content": "The fusion technique is proposed by the University of Montreal in 2015\nand published in this paper: “On Using Monolingual Corpora in Neural\nMachine Translation”. The idea\nabout fusion is to integrate a language model (LM) trained only on\nmonolingual data (target language) into an NMT system. Since this paper\nwas published in 2015, it uses the encoder-decoder architecture. So, the\nintegrating part will be done on the decoder’s side.\n\nIn the paper, they proposed two method for integrating a language model\ninto an NMT translation system; and they are shallow fusion\nand deep fusion where the language model is a recurrent\nneural network language model (RNN-LM) same as the decoder but trained\nseparately using monolingual data.\n\nShallow Fusion\n\nThe name “Shallow fusion” was introduced in this paper where Koehn tried\nto combine a language model with a statistical machine translation\nsystem. So, at each time step, the translation model proposes a set of\ncandidate words, these candidates are then scored according to the\nweighted sum of the scores given by the translation model and the\nlanguage model according to the following formula:\n\n\\[\\text{log} p\\left( y_{t} = k \\right) = \\log\\ p_{\\text{NMT}}\\left( y_{t} = k \\right) + \\beta\\ \\log\\ p_{\\text{LM}}\\left( y_{t} = k \\right)\\]\n\nWhere:\n\n\n  \n    $y_{t}$ is the suggested output word.\n  \n  \n    $p_{\\text{NMT}}$ is the probability of the neural machine\ntranslation generating word $y_{t} = k$.\n  \n  \n    $p_{\\text{LM}}$ is the probability of the language model generating\nword $y_{t} = k$.\n  \n  \n    $\\beta$ is a hyper-parameter that needs to be tuned to maximize the\ntranslation performance on a development set.\n  \n\n\nDeep Fusion\n\nIn deep fusion, we integrate the RNNLM and the decoder of the NMT by\nconcatenating their hidden states next to each other. Unlike the vanilla\nNMT (without any language model component), the hidden layer of the deep\noutput takes as input the hidden state of the RNNLM in addition to that\nof the NMT, the previous word and the context as shown in the following\nfigure:\n\n\n    \n\n\nThe model is then fine-tuned to use the hidden states from both of these\nmodels when computing the output probability of the next word according\nto the following formula:\n\n\\[p\\left( y_{t} \\middle| y_{&amp;lt; t},\\ x \\right) \\propto \\ \\exp\\left( y_{t}^{T}\\left( W_{o}.f_{o}\\left( s_{t}^{\\text{LM}},\\ s_{t}^{\\text{NMT}},y_{t - 1},\\ c_{t} \\right) + b_{o} \\right) \\right)\\]\n\nWhere:\n\n\n  \n    $x$ is the input and $y_{&amp;lt; t}$ is all the previous output words.\n  \n  \n    $y_{t}$ is the suggested output word at time step $t$. Which means\nthat $y_{t - 1}$ is the output word one step before.\n  \n  \n    $W_{o}$ is the weights of the output layer and $b_{o}$ is the bias.\n  \n  \n    $f_{o}$ is the activation function of the output layer.\n  \n  \n    $s_{t}^{\\text{LM}}$ is the hidden state of the language model.\n  \n  \n    $s_{t}^{\\text{NMT}}$ is the hidden state of the neural machine\ntranslation model.\n  \n  \n    $c_{t}$ is the context vector produced by the encoder.\n  \n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Back-translation",
        "url"       : "/machine-translation/Back-Translation",
        "date"      : "20/11/2015",
        "content": "Back-translation is a semi-supervised mechanism proposed in this paper:\n“Improving Neural Machine Translation Models with Monolingual\nData” by the Rico Sennrich and\nthe University of Edinburgh in 2015 that uses a reverse translation\nsystem make the best use out of target-side monolingual data.\n\nBack-translation operates in a semi-supervised setup where both\nbilingual and monolingual data in the target language are available.\nUsing “English-French” data as a use-case where the source is\nEnglish and the target is French,\nthe back-translation process can be summarized into the following steps:\n\n\n  \n    We have “en-fr” parallel corpus that can be used to train NMT model,\nlet’s call it “Forward NMT en→fr”.\n  \n  \n    We can reverse this data to get “fr-en” parallel corpus which can be\nused to train NMT model, let’s call it “Backward NMT en→fr”.\n  \n  \n    We have a corpus of just “French” data.\n  \n  \n    We will use the “Backward NMT en→fr” to translate this French data\nto English which will get us a “synthetic en-fr parallel\ncorpus”. To be able to get the best synthetic data possible,\nthe paper uses beam search.\n  \n  \n    Then, we are going to mix the original “en-fr” data with the\nsynthetic one to further train the “forward NMT en→fr” model.\n  \n\n\n\n    \n\n\nYou could either leave it at that (hopefully better than just the original\nmodel), or you can extend it to dual learning\n where you have\nto find some monolingual source language data and translate it with the forward\nmodel to further train the backward model (back-translation for the backward\nmodel). With this strengthened backwards model, you can probably generate better\nback-translations for the forward model, and train that one further and so on.\n\n\n  Notes:\n\n  \n    \n      You can iteratively train both models forever, but that only makes sense if\n  you have monolingual source data as well, and it usually stops improving\n  after a two rounds or so.\n    \n    \n      In the paper, they were using the encoder-decoder architecture. However,\n  this technique can be used with any other NMT architecture.\n    \n    \n      As discussed in the paper, back-translation delays overfitting in NMT\n  models especially for small datasets.\n    \n    \n      Usually, back-translation outperforms deep fusion.\n    \n  \n\n\nIn the paper, they tried another method to use monolingual target data hoping\nit will be better than the previous one. They treated the monolingual target\ndata as parallel examples with empty source side. Which means that the decoder\nhas to depend only on the previously generated word when generating the\ntranslation. Also, they froze the layers of the encoder.\n\nThe following are the different results obtained by each method on\nEnglish -&amp;gt; German parallel corpus where:\n\n\n  \n    parallel: means using just the NMT model\n  \n  \n    monolingual: means using the NMT model with\nmonolingual data where the source source sentence is empty.\n  \n  \n    synthetic: means using the NMT model with\nmonolingual data where the source sentence is translated using a backward NMT\nmodel.\n  \n\n\n\n    \n\n\nNoised BT\n\nBack-translation typically uses beam search to generate synthetic source\nsentences. However, beam search can lead to less rich translations since\nit focuses on the head of the model distribution which results in very\nregular synthetic source sentences that do not properly cover the true\ndata distribution. In this paper: “Understanding Back-Translation at\nScale”, published by Facebook AI\nand Google Brain in 2018, they advised that adding noise to the beam\nsearch actually improves the generated synthetic data which improves the\ntranslation model’s performance.\n\nIn particular, we transform source sentences with three types of noise:\n\n\n  \n    Deleting words with probability 0.1.\n  \n  \n    Replacing words by a filler token with probability 0.1.\n  \n  \n    Uniformly swapping words no further than three positions apart.\n  \n\n\nAnd this simple change outperforms all other sampling techniques\nsuch as greedy search, beam search (beam size = 5), top-k sampling\n(k = 10), and randomly sampling. The following table contains the\nBLEU score of the same back-translation NMT model with different\nsynthetic data generation methods:\n\n\n    \n\n\nAnd the following table shows the perplexity of the generated\nsynthetic data. They analyzed the richness of generated\nsynthetic outputs and train a language model on real human text and\nscore synthetic source sentences generated by the different methods\nmentioned above:\n\n\n    \n\n\nThe results show that beam+noise method receives the highest\nperplexity which indicates that beam search outputs are not as rich\nas sampling outputs or beam+noise. This lack of variability probably\nexplains in part why back-translations from pure beam search provide\na weaker training signal than alternatives.\n\nLow vs High Resource\n\nThe experiments so far are based on a setup with a high-resource\nbilingual corpus. In this part, we are going to discuss the effect of\nback-translation on low-resource setup. To simulate such setups, they\ncut off the training data to either 80K sentence-pairs or 640K\nsentence-pairs and then used back-translation and compared this setup to\nthe original setup.\n\nThe following figure shows that the accuracy of the German-English\nback-translation systems steadily increases with more training data: On\nnewstest2012, the BLEU score is $13.5$ for 80K bitext, $24.3$ for 640K\nand $28.3$ BLEU for 5M:\n\n\n    \n\n\nThe figure also shows that sampling is more effective than beam for\nlarger setups (640K and 5.2M bi-texts) while the opposite is true for\nresource poor settings (80K bitext). This is likely because the\nback-translations in the 80K setup are of very poor quality and the\nnoise of sampling and beam+noise is too much for this brittle\nlow-resource setting.\n\nReal Vs Synthetic Data\n\nHow does real human bitext compare to synthetic data in terms of final\nmodel accuracy? To answer this question, they sub-sampled 640k\nsentence-pairs of the bitext for training. And added either one of the\nfollowing three alternatives:\n\n\n  \n    bitext: The remaining of bitext data (real human).\n  \n  \n    BT-bitext: The back-translation of the remaining bitext data\n(synthetic).\n  \n  \n    BT-news: The back-translation the news data (synthetic,\ndifferent domain).\n  \n\n\nThe back-translated data is generated via sampling. This setup\nallows us to compare synthetic data to genuine data since BT-bitext\nand bitext share the same target side. It also allows us to estimate\nthe value of BT data for domain adaptation since the newscrawl\ncorpus (BT-news) is pure news whereas the WMT is a mixture of\neuroparl and commoncrawl.\n\nThe following figure shows the results on both validation sets. Most strikingly,\nBT-news performs almost as well as bitext on newstest2012. This\nshows that synthetic data can be nearly as effective as real human translated\ndata when the domains match:\n\n\n    \n\n\nOn the other hand, when the domain changes, the back-translation performance\ngets hurt. The following figure shows the performance of the previous three\nmodels on a mixture set between WMT training data &amp;amp; news crawl. They named\nthis set “valid-mixed”. The following figure shows the accuracy is not as good\nas before since the domain of the BT data and the test set do not match:\n\n\n    \n\n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "GNMT: Google&#39;s NMT",
        "url"       : "/machine-translation/GNMT",
        "date"      : "26/09/2016",
        "content": "GNMT stands for “Google’s Neural Machine Translation” which is a deep\nmachine translation model proposed in 2016 by Google Research and\npublished in this paper: Google’s Neural Machine Translation System:\nBridging the Gap between Human and Machine\nTranslation. The official code\nfor this paper can be found in the TensorFlow’s official GitHub\nrepository:\nTensorFlow/GNMT.\n\nGNMT is a deep LSTM network with 8 encoder (1 bi-directional layer and 7\nuni-directional layers) and 8 decoder layers (all uni-directional\nlayers) with residual connections throughout the architecture as well as\nattention connections from the decoder network to the encoder. To\nimprove parallelism and therefore decrease training time, the model was\npartitioned on 8 different GPUs; one for each layer.\n\n\n    \n\n\n\n  Note:\nIn case you didn’t pay attention, the first layer of the encoder module\nis bi-directional to allow the best possible coverage of the source\ncontext. The output from the forward path\n${\\overrightarrow{\\mathbb{x}}}_t^{f}$ is concatenated with the output\nfrom the backward path ${\\overleftarrow{\\mathbb{x}}}_t^{b}$ and then\nfed to the next layer of the encoder:\n\n  \n    \n\n\n\nThis model follows the common sequence-to-sequence learning framework.\nIt has three components: an encoder network, a decoder network, and an\nattention network. Next, let’s see how it works. Let\n$\\left( X,\\ Y \\right)$ be a source and target sentence pair where\n$X = \\left\\{ x_{1},\\ …x_{m} \\right\\}$ is a set of $M$ tokens and\n$Y = \\left\\{ y_{1},\\ …y_{n} \\right\\}$ is a set of $N$ tokens. The\nencoder transforms the source sentence $X$ into a list of fixed sized\nvectors:\n\n\\[\\mathbb{x}_{1},\\ ...\\mathbb{x}_{m} = \\text{EncoderRNN}\\left( x_{1},\\ ...x_{m} \\right)\\]\n\nGiven this list of vectors\n$\\left\\{ \\mathbb{x}_1,\\ …\\mathbb{x}_m \\right\\}$, along with the\nstart-of-sentence symbol $y_{0} = \\left\\langle /s \\right\\rangle$,\nthe decoder generates the translation autoregressively (one token at a time),\nuntil the special end-of-sentence symbol $\\left\\langle /s \\right\\rangle$\nis produced:\n\n\\[y_{i} = \\text{DecoderRNN}\\left( \\left\\{ y_{0},\\ ...y_{i - 1} \\right\\},\\left\\{ \\mathbb{x}_{1},\\ ...\\mathbb{x}_{m} \\right\\} \\right)\\]\n\nThe encoder and decoder are connected through an attention module which\nis a feed-forward network that allows the decoder to focus on different\nregions of the source sentence during the course of decoding. Let\n$y_{i - 1}$ be the decoder’s output from the past decoding time step,\nthe attention context $a_{i}$ for the current time step is computed\naccording to the following formulas:\n\n\\[s_{t} = \\text{Attention}\\left( y_{i - 1},\\ \\mathbb{x}_{1},\\ ...\\mathbb{x}_{m} \\right)\\]\n\n\\[p_{t} = \\text{Softmax}\\left( s_{t} \\right) = \\frac{\\exp\\left( s_{t} \\right)}{\\sum_{t = 1}^{M}{\\exp\\left( s_{t} \\right)}}\\]\n\n\\[a_{i} = \\sum_{t = 1}^{M}{p_{t}.}\\mathbb{x}_{t}\\]\n\nDeep stacked networks often give better accuracy over shallower models,\nbut deep networks suffer from gradient vanishing; which means that\ngradients get very small which makes training very slow or impossible at\nsometimes. To overcome this issue, they used residual connections\nbetween layers in the encoder and the decoder.\n\nMore concretely, let $\\text{LSTM}_{i}$ and\n$\\text{LSTM}_{i + 1}$ be the $i^{\\text{th}}$ and\n$\\left( i + 1 \\right)^{\\text{th}}$ LSTM layers in a stack, whose\nparameters are $W^{i}$ and $W^{i + 1}$ respectively.\nAt the $t^{\\text{th}}$ time step, the stacked LSTM residual\nconnection will be:\n\n\\[c_{t}^{i},m_{t}^{i} = \\text{LSTM}_{i}\\left( c_{t - 1}^{i},\\ m_{t - 1}^{i},\\ x_{t}^{i - 1};\\ W^{i} \\right)\\]\n\n\\[x_{t}^{i} = m_{t}^{i} + x_{t}^{i - 1}\\]\n\n\\[c_{t}^{i + 1},m_{t}^{i + 1} = \\text{LSTM}_{i + 1}\\left( c_{t - 1}^{i + 1},\\ m_{t - 1}^{i + 1},\\ x_{t}^{i};\\ W^{i + 1} \\right)\\]\n\nNeural Machine Translation models often operate with fixed word\nvocabularies, but translation is fundamentally an open vocabulary\nproblem and that can cause problems. To fix this issue, they decided to\nuse a subword tokenization model called “Wordpiece Model”.\n\nWordpiece Model\n\nWordpiece Model (WPM) is a word segmentation algorithm created by\nSchuster and Nakajima in their paper: “Japanese, Korean Voice\nSearch”\npublished in 2012 while there were trying to find a good word segmentor\nto solve the Japanese/Korean voice problem at Google. This approach is\ncompletely data-driven and the following is the algorithm:\n\n\n  \n    Initialize the vocabulary to include every character present in the\ntraining data.\n  \n  \n    Build a character language model using the previous sequences.\n  \n  \n    Merge those with the highest likelihood into a new symbol. For\nexample &quot;$\\text{ug}$&quot; would have only been merged if the\nprobability of &quot;$\\text{ug}$&quot; divided by &quot;$u$&quot; and &quot;$g$&quot; is\ngreater than for any other character-pair.\n  \n  \n    Keep merging until certain vocabulary size is met or the likelihood\nfalls below a certain threshold.\n  \n\n\nAt interference, use the built vocabulary to split words into subwords\nthat can be found in the vocabulary. Here is an example of a word\nsequence and the corresponding wordpiece sequence:\n\n\\[Sentence:\\ Jet\\ makers\\ feud\\ over\\ seat\\ width\\ with\\ big\\ orders\\ at\\ stake\\]\n\n\\[wordpieces:\\ \\_ J\\ et\\ \\_ makers\\ \\_ fe\\ ud\\ \\_ over\\ \\_ seat\\ \\_ width\\ \\_ with\\ \\_ big\\ \\_ orders\\ \\_ at\\ \\_ stake\\]\n\nIn the above example, the word “$\\text{Jet}$” wasn’t found in the\nvocabulary, that’w why it was broken into two wordpieces “$\\text{_J}$” and\n“$\\text{et}$” that are in the vocabulary. Same goes for the word\n“$\\text{feud}$” which got broken into two wordpieces “$\\text{_fe}$” and\n“$\\text{ud}$”. The other words remain as single wordpieces since they\nwere already in the vocabulary.\n\n\n  Note:\n“_” is a special character added to mark the beginning of a word.\n\n\nChoosing the vocabulary of the wordpiece model is a hyper-parameter that\nyou can tune based on your experiments. However, a total vocabulary\nbetween 8k and 32k wordpieces usually achieves better results and fast\ndecoding.\n\nBeam Search\n\nWhen decoding, they used beam search algorithm\n($\\text{beam size} = 3.0$) which maximizes the score function\n$s\\left( Y,\\ X \\right) = \\log\\left( P\\left( Y \\middle| X \\right) \\right)$.\nAfter the hypothesis gets the end-of-sentence token, they applied a\nrefined version of the beam search algorithm which is exactly the same\nas the standard beam search with two important refinements:\n\n\n  Coverage Penalty:\nCoverage penalty aims at favoring translations that fully cover the\nsource sentence according to the attention module. The coverage\npenalty is computed by the following formula knowing that\n$\\left| X \\right|$ is the length of the source sentence $X$,\n$p_{i,j}$ is the attention probability of the the $i^{\\text{th}}$\nsource word with the $j^{\\text{th}}$ target word, and\n$\\beta \\in \\left\\lbrack 0,1 \\right\\rbrack$ is a hyper-parameter:\n\n\n\\[cp\\left( X;Y \\right) = \\beta*\\sum_{i = 1}^{\\left| X \\right|}{\\log\\left( \\min\\left( \\sum_{j = 1}^{\\left| Y \\right|}p_{i,j},\\ 1.0 \\right) \\right)}\\]\n\n\n  Length Penalty:\nWith length penalty, they aimed at normalizing the sentence length\nto account for the fact that hypotheses have different length.\nWithout it, beam search will favor shorter results over longer ones.\nThe length penalty term is computed by the following formula knowing\nthat is $\\left| Y \\right|$ the length of $Y$and\n$\\alpha \\in \\left\\lbrack 0,1 \\right\\rbrack$ is a hyper-parameter:\n\n\n\\[lp\\left( Y \\right) = \\left( \\frac{5 + \\left| Y \\right|}{5 + 1} \\right)^{\\alpha}\\]\n\nNow, the beam search score function is:\n\n\\[s\\left( Y,\\ X \\right) = \\frac{\\log\\left( P\\left( Y \\middle| X \\right) \\right)}{cp\\left( X;Y \\right) + lp\\left( Y \\right)}\\]\n\nNote:\n$\\alpha \\in \\left\\lbrack 0.6,0.7 \\right\\rbrack$ was usually found to be\nbest. However, they used $\\alpha = \\beta = 0.2$ in this paper. Also\nnotice that when $\\alpha = 0$ and $\\beta = 0$, the decoder falls back to\npure beam search.\n\nExperiments &amp;amp; Results\n\nAll models’ trainable parameters were uniformly initialized between\n$\\left\\lbrack - 0.04:0.04 \\right\\rbrack$. Gradient clipping was applied\nat $5.0$. As an optimizer, they used a combination of Adam and simple\nSGD learning algorithms. Adam was used for the first $60k$ steps, after\nwhich they switched to simple SGD. For the Adam part, they used a\nlearning rate of $0.0002$, and $0.5$ for the SGD part. They used a\nmini-batch of $128$ examples.\n\n\n  Note:\nThey used a combination of Adam and SGD because Adam (green) accelerated\ntraining at the beginning, but converged to a worse point than the\ncombination (red) as shown below in the following graph:\n\n  \n   \n\n\n\nThey evaluated the model on the WMT En→Fr dataset (36M examples), the\nWMT En→De dataset (5M examples), as well as many Google’s internal\nproduction datasets. For testing, they used newstest2014; and for\nvalidation, they used newstest2012 and newstest2013. To prevent\noverfitting, they applied dropout with a dropout probability of $0.2$\nfor En→Fr and $0.3$ for En→De datasets.\n\nThe following table summarizes the results of running different\nvariations of GNMT on the En→Fr (left) and En→De (right) datasets\nagainst the best Phrase-Based Machine Translation (PBMT) production\nsystem for Google Translate along with the best models proposed by other\npapers (last five models). As can be seen, “WPM-32K” achieves amazing\nresults.\n\n\n    \n\n\nAs seen in the past table, they have tried six\nvariations of the GNMT model:\n\n\n  \n    Word:\nThey selected the most frequent 212K source words as the\nsource vocabulary and the most popular 80k target words as the\ntarget vocabulary. Out-of-Vocabulary (OOV) words were converted into\nspecial &amp;lt;first_char&amp;gt;_UNK_&amp;lt;last_char&amp;gt; symbols. Then, the\nattention mechanism was used to copy a corresponding word from the\nsource to replace these unknown words during decoding.\n  \n  \n    Character:\nThey used characters instead of words as input to the GNMT and\nexpected the decoder to generate characters as the output.\n  \n  \n    WPM:\nAll three models are the same GNMT model with the vocabulary size\nbeing the only difference. WPM-8k, WPM-16k, and WPM-32k refers to\nthe same GNMT model with vocabulary of 8k, 16k, and 32k tokens\nrespectively.\n  \n  \n    Mixed word/character:\nThis is a similar algorithm to the WPM that uses a fixed vocabulary.\nThen, out-of-vocabulary (OOV) words are collapsed into special\nprefixes (&amp;lt;B&amp;gt;,&amp;lt;M&amp;gt;, and &amp;lt;E&amp;gt;) prepended to the words’\ncharacters. These special prefixes mean: beginning, middle, and end\nof the word, respectively. For example, the OOV word “Miki” will be\nconverted to “&amp;lt;B&amp;gt;M &amp;lt;M&amp;gt;i &amp;lt;M&amp;gt;k &amp;lt;E&amp;gt;i”.\n  \n\n\nAnd the following table summarizes the GNMT (WMP-32k) performance on\nGoogle’s internal production datasets in comparison with the best\nPhrase-Based Machine Translation (PBMT) production system for Google\nTranslate and the a human translator:\n\n\n    \n\n\nAs expected, these results show that GNMT system outperforms the PBMT\nsystem in every language pair but not to the extent of surpassing the\nhuman translator.\n\nRL-refined Model\n\nGiven a dataset of parallel text pairs denoted\n$\\mathcal{D} \\equiv \\left\\{ \\left( X^{\\left( i \\right)},\\ Y^{\\left( i \\right)} \\right),\\ …\\left( X^{\\left( \\left| \\mathcal{D} \\right| \\right)},\\ Y^{\\left( \\left| \\mathcal{D} \\right| \\right)} \\right) \\right\\}$\nof size $\\left| \\mathcal{D} \\right|$, a machine translation model aims\nat maximizing the sum of log probabilities of the ground-truth outputs\ngiven the corresponding inputs:\n\n\\[\\mathcal{O}_{\\text{ML}}\\left( \\theta \\right) = \\sum_{i = 1}^{\\left| \\mathcal{D} \\right|}{\\log\\ P_{\\theta}\\left( Y^{＊\\left( i \\right)} \\middle| X^{\\left( i \\right)} \\right)}\\]\n\nThe main problem with this objective is that it does not reflect the\nBLEU score; outputs with higher BLEU scores should obtain higher\nprobabilities. In this paper, they attempted to refine the model using\nreinforcement learning as expressed below:\n\n\\[\\mathcal{O}_{\\text{RL}}\\left( \\theta \\right) = \\sum_{i = 1}^{\\left| \\mathcal{D} \\right|}{\\sum_{Y \\in \\mathcal{Y}}^{}{P_{\\theta}\\left( Y \\middle| X^{\\left( i \\right)} \\right)\\text{.r}\\left( {Y,Y}^{＊\\left( i \\right)} \\right)}}\\]\n\nWhere $r\\left( {Y,Y}^{＊\\left( i \\right)} \\right)$ denotes the\nper-sentence score, and $\\mathcal{Y}$ is a set of all the hypotheses\nsentences. The BLEU score has some undesirable properties when used for\nsingle sentences, as it was designed to be a corpus measure. Therefore,\nthey decided to use GLEU with reinforcement learning instead. For the\nGLEU score, they recorded 1, 2, 3 or 4 (n-grams) tokens in output and\ntarget sequence. Then, computed the recall in the ground truth sequence,\nand the precision in the generated output sequence. Now, GLEU score is\nsimply the minimum of recall and precision:\n\n\\[GLEU = min\\left( \\text{precision}\\left( Y^{＊} \\right),\\ Recall\\left( Y \\right) \\right),\\ \\ \\ GLEU \\in \\left\\lbrack 0,1 \\right\\rbrack\\]\n\nTo further stabilize training, they optimized a linear combination of\n$\\mathcal{O}_{\\text{ML}}$ and $\\mathcal{O}_{\\text{RL}}$\nobjectives using a hyper-parameter $\\alpha$ (typically set to be $0.017$):\n\n\\[\\mathcal{O}_{\\text{Mixed}}\\left( \\theta \\right) = \\alpha*\\mathcal{O}_{\\text{ML}}\\left( \\theta \\right) + \\mathcal{O}_{\\text{RL}}\\left( \\theta \\right)\\]\n\nThe following table presents the average of 8 independent models\nwith/without refinement. The results shows that the refined model\nslightly improves the performance:\n\n\n    \n\n\nTo obtain state-of-the-art results on En→Fr dataset and En→Fr dataset\nwith $41.16$ and $26.30$ respectively, they combined 8 different\nRL-refined models\n\n\n    \n\n\n\n  Note:\nThe refinement model is applied after the ML model converges. Then, the\nrefinement is trained until the BLEU score does not change much on the\ndevelopment set which usually happens after around 400k steps.\n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Dual Learning for Machine Translation",
        "url"       : "/machine-translation/Dual_Learning",
        "date"      : "01/11/2016",
        "content": "Dual learning is a RL mechanism used mainly for machine translation,\nproposed in 2016 by the University of Technology in China in\ncollaboration with Microsoft and published in this paper: “Dual\nLearning for Machine\nTranslation”. One of the\nlimitation of machine translation systems is the limited parallel data\ndue to the the lack of human labeling. The dual-learning mechanism can\nenable an NMT system to automatically learn from monolingual data (in\nboth the source and target languages) through a dual-learning game.\n\nIn the dual-learning mechanism, we use one agent to represent the model\nfor the primal task (translating from English to French for example) and\nthe other agent to represent the model for the dual task (Translating\nfrom French to English), then ask them to teach each other through a\nreinforcement learning process.\n\nSpecifically, the dual-learning mechanism for MT can be described as the\nfollowing two-agent communication game shown in the following graph:\n\n\n    \n\n\nAs you can see from the graph, we have monolingual messages sampled from\ncorpora $D_{A}$ and $D_{B}$ in language A and B, two noisy-channel\ntranslation models $\\theta_{\\text{AB}}$ and $\\theta_{\\text{BA}}$ , and\ntwo language model agents $\\text{LM}{A}$ and $\\text{LM}{B}$:\n\n\n  \n    The first agent (only understands language A), samples a message\n$s_{A}$ from corpora $D_{A}$ and sends it to the second agent\nthrough the noisy channel.\n  \n  \n    The noisy channel translation model $\\theta_{\\text{AB}}$ generates K\ntranslations\n${\\widehat{s}}{1},\\ {\\widehat{s}}{2},\\ …\\ {\\widehat{s}}_{K}$\nusing beam search of K beam size for the received message in\nlanguage B and sends them to the second agent.\n\n\\[P\\left( {\\widehat{s}}_{k} \\middle| s_{A};\\theta_{\\text{AB}} \\right)\\forall k \\in \\lbrack 1,K\\rbrack\\]\n  \n  \n    The second agent (only understands language B) receives the K\ntranslations and compute the reward of each translated sentence\n${\\widehat{s}}{k}$ using the language model $\\text{LM}{B}$ \nand the translation model $\\theta_{\\text{BA}}$:\n\n\\[r_{1,k} = \\text{LM}_{B}\\left( {\\widehat{s}}_{k} \\right)\\forall k \\in \\lbrack 1,K\\rbrack\\]\n\n\\[r_{2,k} = \\log\\left( P\\left( s \\middle| {\\widehat{s}}_{k};\\theta_{\\text{BA}} \\right) \\right)\\forall k \\in \\lbrack 1,K\\rbrack\\]\n  \n  \n    Combine the two rewards using a linear combination where where\n$\\alpha$ is a hyper-parameter:\n\n\\[r_{k} = \\alpha r_{1,k} + \\left( 1 - \\alpha \\right)r_{2,k}\\]\n  \n  \n    Compute the SGD and update the weights of the first noisy-channel\ntranslation model $\\theta_{\\text{AB}}$ using learning rate $\\gamma_{1}$:\n\n\\[\\nabla_{\\theta_{\\text{AB}}}\\widehat{E}\\left\\lbrack r \\right\\rbrack = \\frac{1}{K}\\sum_{k = 1}^{K}\\left( r_{k}.\\nabla_{\\theta_{\\text{AB}}}\\ \\log\\left(  P\\left( {\\widehat{s}}_{k} \\middle| s;\\theta_{\\text{AB}} \\right) \\right) \\right)\\]\n\n\\[\\theta_{\\text{AB}} \\leftarrow \\theta_{\\text{AB}} + \\gamma_{1}\\nabla_{\\theta_{\\text{AB}}}\\widehat{E}\\left\\lbrack r \\right\\rbrack\\]\n  \n  \n    Compute the SGD and update the weights of the second noisy-channel\ntranslation model $\\theta_{\\text{BA}}$ using learning rate $\\gamma_{2}$:\n\n\\[\\nabla_{\\theta_{\\text{BA}}}\\widehat{E}\\left\\lbrack r \\right\\rbrack = \\frac{1}{K}\\sum_{k = 1}^{K}\\left( \\left( 1 - \\alpha \\right)r_{k}.\\nabla_{\\theta_{\\text{BA}}}\\ \\log\\left( P\\left( s \\middle| {\\widehat{s}}_{k};\\theta_{\\text{BA}} \\right) \\right) \\right)\\]\n\n\\[\\theta_{\\text{BA}} \\leftarrow \\theta_{\\text{BA}} + \\gamma_{2}\\nabla_{\\theta_{\\text{BA}}}\\widehat{E}\\left\\lbrack r \\right\\rbrack\\]\n  \n  \n    Assign the translated message ${\\widehat{s}}_{k}$ with the\nhighest probability as message B $s_{B}$ and send it back to\nthe first agent.\n  \n  \n    Do the same but now using message B. And keep doing that till convergence.\n  \n\n\nNotes:\n\n\n  \n    The game can also be started from the second agent with an original\nmessage in language B, and then the two agents will go through a\nsymmetric process and improve the two channels (translation models)\naccording to the feedback.\n  \n  \n    Language models can be trained either using other resources, or just\nusing the monolingual data $D_{A}$ and $D_{B}$.\n  \n\n\nResults\n\nIn the paper, they trained their dual-NMT model using WMT’14 corpus\nwhich contains 12M sentence pairs and they concatenated newstest2012 and\nnewstest2013 and used it as the validation set and used newstest2014 as\nthe testing set. Also, they used “News Crawl: articles from 2012”\nprovided by WMT’14 as monolingual data.\n\nThey constructed the vocabulary with the most common 30K words in the\nparallel corpora, and out-of-vocabulary words were replaced with a\nspecial token &amp;lt;UNK&amp;gt;. For monolingual corpora, they removed the\nsentences containing at least one out-of-vocabulary words.\n\nThey compared their dual-NMT approach with two baselines: the standard\nneural machine translation (NMT for short), and a NMT-model using\nback-translation in two settings:\n\n\n  \n    Small: They used only 10% of the parallel data for warm start.\n  \n  \n    Large: They used all parallel data.\n\n    In the following results, we can see that the Dual-NMT algorithm\noutperforms the baseline algorithms in all the settings despite the\nfact that the two baselines for English→French and French→English\nare trained separately while the dual-NMT conducts joint training.\n  \n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Gated CNN",
        "url"       : "/machine-translation/Gated_CNN",
        "date"      : "23/12/2016",
        "content": "One of the major defects of Seq2Seq models is that it can’t process\nwords in parallel. For a large corpus of text, this increases the time\nspent translating the text. CNNs can help us solve this problem. In this\npaper: “Language Modeling with Gated Convolutional\nNetworks”, proposed by FAIR\n(Facebook AI Research) in 2016.\n\nTO BE CONTINUED!\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "ConvS2S",
        "url"       : "/machine-translation/ConvS2S",
        "date"      : "08/05/2017",
        "content": "One of the major defects of Seq2Seq models is that it can’t process\nwords in parallel. For a large corpus of text, this increases the time\nspent translating the text. CNNs can help us solve this problem. In this\npaper: “Convolutional Sequence to Sequence\nLearning”, proposed by FAIR\n(Facebook AI Research) in 2017. The official repository for this paper\ncan be found on fairseq/convs2s.\n\nIn this paper, we can see that ConvS2S outperformed the Attention model\non both WMT’14 English-German and WMT’14 English-French translation\nusing an entirely-CNN translation model with faster results.\n\n\n    \n\n\nThe following figure shows the whole ConvS2S architecture created by\nFacebook AI Research (FAIR):\n\n\n    \n\n\nAs we can see, this architecture seems a bit complicated and consists of\nmany different components that need clarification. So, let’s divide this\nbig architecture into three main components: Encoder, Decoder and\nAttention.\n\nEncoder\n\nThe encoder component consists of five different steps:\n\n\n  Padding: The recurrent neural nets process text from\nleft-to-right while in CNNs the words that are close together get\nconvoluted together. So, if we have a sentence of $L$ words and the\nrepresentative word vector is $d$ features long, then we can\nrepresent the sentence as a 2D-plane\n$X \\in \\mathbb{R}^{L \\times d}$. And to ensure that the output of\nthe convolution layers matches the input length, we apply padding to\nthe input at each layer based on the following formula given that\n$k$ is the kernel height (the kernel’s width is always the same as\nthe input’s width):\n\n\n\\[p = \\frac{k - 1}{2}\\]\n\nPadding is denoted by the &amp;lt;p&amp;gt; tag which is usually zero as shown in\nthe following image:\n\n\n    \n\n\n\n  Position Embedding: Then, we use word-embedding $w$ (of size 512\nin the paper) combined with the absolute position $p$ of the words\nto obtain word-position knowing that the size of $p$ is the same as\n$w$:\n\n\n\\[e = \\left( e_{1},\\ e_{2},\\ ...e_{L} \\right) = \\left( w_{1} + p_{1},\\ w_{2} + p_{2},\\ ...w_{L} + p_{L} \\right)\\]\n\n\n  Convolution: Now, we have a matrix representing the input\nsentence $X \\in \\mathbb{R}^{L \\times d}$ where $L$ is the length of\nthe sentence. Next, we are going to use convolution of $2d$\ndifferent filters, each convolution kernel is a\n$W \\in \\mathbb{R}^{k \\times d}$ where $k$ is the kernel size.\nPerforming the convolution will result into a matrix of\n$Z \\in \\mathbb{R}^{L \\times 2d}$.\n\n\n\\[Z = W*X + b\\]\n\nIn the following example, we are going to use a $3 \\times d$ filters\n(trigram filters). When we convolve a filter with a sentence, we\nmultiply its values element-wise with the original matrix, then summing\nthem up. We keep doing that till we pass through the whole sentence with\na certain number of filters.\n\n\n    \n    \n\n\n\n  GLU: After applying convolution, we will have an output of\n$Z \\in \\mathbb{R}^{L \\times 2d}$ over which we are going to apply\nGLU (Gated-Linear Unit). We are going to split $Z$ into two matrices\n$A \\in \\mathbb{R}^{L \\times d}$ and $B \\in \\mathbb{R}^{L \\times d}$.\nApplying GLU means applying the following formula:\n\n\n\\[\\text{GLU}\\left( Z \\right) = GLU\\left( \\left\\lbrack \\text{A\\ B} \\right\\rbrack \\right) = A \\otimes \\sigma\\left( B \\right)\\]\n\nWhere $\\otimes$ is the point-wise multiplication and $\\sigma$ is the\nsigmoid function. The term $\\sigma(B)$ controls which inputs $A$ of the\ncurrent context are relevant.\n\n\n  Residual: To enable deep layers, we add residual connections\nfrom the input of each convolution layer to the output from GLU\nstep.\n\n\n\\[Z^{l + 1} = \\text{GLU}\\left( Z^{l} \\right) + Z^{l}\\]\n\nAfter applying the residual connection, we multiply the sum of the input\nand output of a residual block by $\\sqrt{0.5}$ to halve the variance of\nthe sum.\n\nHere is the encoder with input dimensions for reference:\n\n\n    \n\n\nDecoder\n\nThe decoder is the same as the encoder after removing the **residual\npart**:\n\n\n  Padding: Padding in the decoder is a little bit different than\nthe encoding. Here, we pad the input using ($k - 1$) on both sides\ninstead of $\\frac{k - 1}{2}$ where $k$ is the kernel height:\n\n\n\\[p = k - 1\\]\n\n\n  Embedding: The same as the encoder.\n\n\n\\[g = \\left( g_{1},\\ g_{2},\\ ...g_{N} \\right) = \\left( w_{1} + p_{1},\\ w_{2} + p_{2},\\ ...w_{N} + p_{N} \\right)\\]\n\n\n  Convolution: The same as the encoder.\n\n\n\\[H = W*X + b\\]\n\n\n  GLU: The same as the encoder.\n\n\n\\[\\text{GLU}\\left( H \\right) = GLU\\left( \\left\\lbrack \\text{A\\ B} \\right\\rbrack \\right) = A \\otimes \\sigma\\left( B \\right)\\]\n\nHere is the decoder with input dimensions for reference:\n\n\n    \n\n\nNow, we have discussed the encoder and the decoder part of the ConvS2S\narchitecture. Let’s get to the attention mechanism which is used in this\narchitecture.\n\nAttention\n\nIn the following part, we are going to discuss the attention mechanism\nused in this architecture. Before getting into this, let’s recap a few\nterms:\n\n\n  \n    The output matrix of the encoder is $Z \\in \\mathbb{R}^{L \\times d}$\nwhere $L$ is the length of the encoder’s input sequence and $d$ is\nthe embedding size.\n  \n  \n    The output matrix of the decoder is $H \\in \\mathbb{R}^{N \\times d}$\nwhere $N$ is the length of the decoder’s input sequence.\n  \n  \n    $e = \\left( e_{1},\\ …\\ e_{L} \\right)$ is the element embedding for\nthe encoder sequence. While $g = \\left( g_{1},\\ …\\ g_{N} \\right)$\nis the element embedding for the decoder sequence.\n  \n\n\nTo compute the attention, we follow the following steps:\n\n\n  First, we combine the the decoder’s current state $h_{i}^{l}$ with\nthe decoder’s embedding embedding $g_{i}$ to get the decoder state\nsummary $d_{i}^{l}$:\n\n\n\\[d_{i}^{l} = h_{i}^{l} + g_{i}\\]\n\n\n  For decoder layer $l$, the attention $a_{\\text{ij}}^{l}$ of state\n$i$ and source element $j$ is computed as a dot-product between the\ndecoder state summary $d_{i}^{l}$ and each output\n$Z^{u} = \\left( z_{1}^{u},\\ …\\ z_{L}^{u} \\right)$ of the last\nencoder block $u$:\n\n\n\\[a_{i}^{l} = \\text{Softmax}\\left( Z^{u}.d_{i}^{l} \\right)\\]\n\n\\[a_{\\text{ij}}^{l} = \\frac{\\exp\\left( d_{i}^{l}.z_{j}^{u} \\right)}{\\sum_{t = 1}^{L}{\\exp\\left( d_{i}^{l}.z_{t}^{u} \\right)}}\\]\n\n\n  The conditional input $c_{i}^{l}$ to the current decoder layer is a\nweighted sum of the encoder outputs $z_{j}^{u}$ as well as the input\nelement embeddings $e_{j}$. The term $L\\sqrt{\\frac{1}{L}}$ is used\nto scale up the result.\n\n\n\\[c_{i}^{l} = L\\sqrt{\\frac{1}{L}}\\sum_{j = 1}^{L}{a_{\\text{ij}}^{l}\\left( z_{j}^{u} + e_{j} \\right)}\\]\n\nThe attention mechanism can be summarized in the following image:\n\n\n    \n\n\nThe problem is that Convolutional Neural Networks do not necessarily\nhelp with the problem of figuring out the problem of dependencies when\ntranslating sentences. That’s why Transformers were created, they are a\ncombination of both CNNs with attention.\n\nInitialization\n\nThe motivation for their initialization is to maintain the variance of\nactivations throughout the forward and backward passes. The following is\nthe different initialization for different parts of the architecture:\n\n\n  \n    All embeddings are initialized from a normal distribution\n$\\mathcal{N}\\left( 0,\\ 0.1 \\right)$.\n  \n  \n    For layers whose output is not directly fed to a gated linear unit,\nthey initialized the weights from the normal distribution\n$\\mathcal{N}\\left( 0,\\ \\sqrt{\\frac{1}{n_{l}}} \\right)$ where $n_{l}$\nis the number of input connections to each neuron at layer $l$.\n  \n  \n    For layers which are followed by a GLU activation, they initialized\nthe weights from the normal distribution\n$\\mathcal{N}\\left( 0,\\ \\sqrt{\\frac{4}{n_{l}}} \\right)$.\n  \n  \n    They applied dropout to the input of some layers so that inputs are\nretained with a probability of $p$.\n  \n  \n    Biases are uniformly set to zero when the network is constructed.\n  \n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "SMT Vs NMT",
        "url"       : "/machine-translation/SMT_Vs_NMT",
        "date"      : "12/06/2017",
        "content": "Although the NMT had made remarkable achievements on particular\ntranslation experiments, researchers were wondering if the good\nperformance persists on other tasks and can NMT indeed replace SMT.\nAccordingly, Junczys-Dowmunt et\nal. who\nperformed experiments on the “United Nations Parallel Corpus” which\ninvolves 15 language pairs and 30 translation directions, and NMT was\neither on par with or surpassed SMT across all 30 translation directions\nin the experiment measured through BLEU scores which proves how\npromising NMT is.\n\nWhen comparing NMT with SMT, we can see that NMT has some pros like:\n\n\n  \n    NMT does not need prior domain knowledge with translation, which\nenables zero-shot translation.\n  \n  \n    NMT produces more fluent results.\n  \n  \n    NMT produces more phrase-similarities between different languages.\n  \n  \n    NMT is language-agnostic. You can use it basically for any language.\n  \n\n\nOn the other side, there are still problems and challenges of NMT need\nto be tackled:\n\n\n  \n    The training and decoding process is quite slow.\n  \n  \n    The style of translation can be inconsistent for the same word.\n  \n  \n    There exists an “out-of-vocabulary” problem on the translation\nresults.\n  \n  \n    The “black-box” neural network mechanism leads to poor\ninterpretability; thus the parameters for training are mostly picked\nbased on experience.\n  \n  \n    Biases is very clear in NMT:\n  \n\n\n\n    \n\n\nBecause of the characteristics of NMT and its superiority over SMT, NMT\nalso starts to be adopted by the industry recently:\n\n\n  \n    In September 2016, the Google Translate team published a blog\nshowing that they had started using NMT to replace Phrase-Based\nMachine Translation for Chinese-English translations on their\nproduct. The NMT they deployed is named Google Neural Machine\nTranslation (GNMT), and a paper: “Google’s Neural Machine\nTranslation System: Bridging the Gap between Human and Machine\nTranslation” was published at\nthe same time to explain that model in details.\n  \n  \n    In 2017, Facebook AI Research (FAIR) announced their way of\nimplementing NMT with CNN, which can achieve a similar performance\nas the RNN-based NMT while running nine times faster.\n  \n  \n    In June 2017, Google released a solely attention-based NMT model\nwhich used neither CNN nor RNN and purely based on the “attention”\nmechanism under the name “Attention is All you\nneed”.\n  \n  \n    In July 2017, Amazon released their NMT implementation with MXNet.\n  \n  \n    Microsoft talked about their usage of NMT in 2016, although not\nrevealed any further technical details yet.\n  \n\n\nIn June 2017, two researchers from John Hopkins University published a\npaper under the name: Six Challenges for Neural Machine\nTranslation which discusses the\nmost urging problems in Neural Machine Translation systems in comparison\nwith Statistical ones. And the found out that all problems can be\nsummarized in six challenges as mentioned below; knowing that all NMT\nmodels used in this paper were trained using Nematus\ntoolkit and all SMT models were\ntrained using Moses toolkit:\n\nTraining Data Size\n\nA well-known property of SMT systems is that increasing amounts of\ntraining data lead to better results. To measure the effect of the\ntraining data size on NMT systems in the paper, they built\nEnglish-Spanish systems on WMT13\ndata of about 385.7\nmillion English words paired with Spanish. To measure the effect of\ndifferent training dataset size, they split the data in the following\nratios: $\\frac{1}{1024},\\ \\frac{1}{512},\\ …\\frac{1}{2},\\ 1$ and\nmeasured the models performance at each data size as shown in the\nfollowing figure:\n\n\n    \n\n\nAs shown, NMT exhibits a much steeper learning curve starting with very\nbad results and ending with outperforming SMT, and even beating the SMT\nsystem with a big language model. The contrast between the NMT and SMT\nlearning curves is quite striking which shows that NMT systems are\nsuperior when having enough data; while SMT systems are superior with\nsmall amount of training data.\n\nDomain Mismatch\n\nDomain in machine translation is defined by information from a specific\nsource which could be different from other domains in topic,\nstyle, level of formality,\ncomplexity …etc. Same words in different domains have\ndifferent translations and expressed in different styles. To measure how well\nNeural Machine Translation (NMT) and Statistical Machine Translation (SMT) hold\nup with domain mismatch, they trained five different systems using five\ndifferent corpora from different domains as shown below:\n\n\n    \n\n\nThey used German-English pairs with test sets sub-sampled from the data.\nA common byte-pair encoding (BPE) is used for all training runs. The\nfollowing figure shows a comparison between NMT and SMT systems BLEU\nscores when trained on the corpora at the row and tested on the corpora\nat the column:\n\n\n    \n\n\nThe previous figure shows that in-domain NMT and SMT systems are\nsimilar (NMT is better for IT and Subtitles, SMT is better for Law,\nMedical, and Koran), the out-of-domain performance for the NMT systems\nis worse in almost all cases.\n\n\n  Note:\nA currently popular approach for domain adaptation is to train a general\ndomain system, followed by training on in-domain data for a few epochs.\n\n\nRare Words\n\nThe next problem that we will discuss is the effect of rare words on NMT\nand SMT systems. To measure that effect, they used pre-trained NMT and\nSMT systems that have the same case-sensitive BLEU score of 34.5 on the\nWMT 2016 English-German news test set (for the NMT model, this reflects\nthe BLEU score resulting from translation with a beam size of 1).\n\nThen, they followed the approach described in this paper: Interpolated\nbackoff for factored translation models\n for examining the effect\nof source word frequency on translation accuracy. This approach can be\nsummarized in the following steps:\n\n\n  \n    First, they align the source sentence and the machine translation\noutput from each system using fast-align\ntoolkit with “gdfa”\n(grow-diag-final-and).\n  \n  \n    Now, each source word is either unaligned (“dropped”) or aligned.\nFor each target word that was aligned, they checked if that word\nappears in the reference translation or not.\n\n    \n      \n        If the target word appears the same number of times in the  hypothesis\nas in the reference, they awarded the system a score of one.\n      \n      \n        If the target word appears more times in the hypothesis than in  the\nreference, they awarded the system a fractional credit the equals to\n$\\frac{\\text{reference_count}}{\\text{hypothesis_count}}$.\n      \n      \n        If the target word does not appear in the reference, they didn’t  award\nthe system.\n      \n    \n  \n  \n    Then, all scores over the full set of target words is averaged to compute\nthe precision for that source word.\n  \n  \n    Finally, words are binned by frequency and average translation\nprecisions can be computed.\n  \n\n\nThe following figure shows the words-frequency bins on the x-axis\nand the average precision on the y-axis. The values above the\nhorizontal axis represent precisions, while the lower portion\nrepresents what proportion of the words were dropped/deleted.\n\n\n    \n\n\nFrom the previous figure, the first item of note is that the NMT system\nhas an overall higher proportion of deleted words. Of the 64379 words\nexamined, the NMT system deleted 3769 while the SMT system deleted 2274.\nThe next interesting observation is what happens with unknown words\n(left-most side of the figure). The SMT system translates these\ncorrectly 53.2% of the time, while the NMT system translates them\ncorrectly 60.1% of the time.\n\nTo sum up, Surprisingly, NMT systems (at least those using byte-pair\nencoding) is better than SMT systems on translation very infrequent\nwords. However, both NMT and SMT systems do continue to have difficulty\ntranslating some infrequent words, particularly those belonging to\nhighly-inflected categories.\n\nLong Sentences\n\nA well-known flaw of early encoder-decoder NMT models was the inability\nto properly translate long sentences. However, this was fixed by using\nthe attention\nmechanism. To\nmeasure the effect of long sentences on NMT (with attention mechanism)\nand SMT systems, they used the large English-Spanish dataset (from\nbefore) and broke it up into buckets based on sentence length (1-9\nsubword tokens, 10-19 subword tokens, ... etc.) and computed\ncorpus-level BLEU scores for each bucket.\n\nThe following figure shows that NMT is better than SMT in general,\nbut SMT system outperforms NMT on sentences of length 60 and\nhigher:\n\n\n    \n\n\nThe quality for the two systems is relatively close, except for the very\nlong sentences (80 and more tokens). The quality of the NMT system is\ndramatically lower for these since it produces too short translations.\n\nWord Alignments\n\nSome might think that the attention mechanism in NMT systems plays the\nrole of the word alignment between the source and target sentences. To\nexamine this, the researchers compared the soft attention matrix in NMT\nwith word alignments obtained by fast-align\ntoolkit. The following figure shows\ntwo the difference on two sentences (left when translating from\nEnglish→German; right when doing the opposite):\n\n\n    \n\n\nAs seen from the left example, most words match up pretty well and both\nare a bit fuzzy around the words “have-been” / “sind”. In the right\nexample, all the alignment points appear to be off by one position\ndespite the fact that the NMT translation quality was pretty good. Over\nall sentences, they measured how well the attention mechanism of the NMT\nsystem matches the alignments of fast-align using two metrics:\n\n\n  \n    Match score: that checks for each output if the aligned input\nword according to fast-align is indeed the input word that received\nthe highest attention probability.\n  \n  \n    Probability mass score: that sums\nup the probability mass given to each alignment point obtained from\nfast-align.\n  \n\n\nThe following table shows alignment scores for the two systems. The\nresults suggest that there is a big overlap between attention and word\nalignments keeping in mind that German–English was definitely an\noutlier:\n\n\n    \n\n\nThese results shows that attention might look similar to word\nalignments. However, it suggests that it has a broader role. For\ninstance, when translating a verb, attention may also be paid to its\nsubject and object since these may disambiguate it.\n\nBeam Search\n\nIn SMT and NMT, beam search is still the most used technique for\ndecoding. Usually the translation quality in both system increases when\na bigger beam size is chosen. But when comes to NMT, does this pattern\ngo on forever?!\n\nActually, experiments run on the NMT system using 8 language pairs\nsuggest otherwise. As shown below, worse translations are found beyond a\ncertain beam size in almost all cases. The optimal beam size varies from\n4 (Czech–English) to around 30 (English–Romanian).\n\n\n    \n\n\nNormalizing sentence level model scores by length of the output\nalleviates the problem somewhat and also leads to better optimal quality\nin most cases (5 of the 8 language pairs investigated). Based on these\nexperiments, optimal beam sizes are in the range of 30--50 in almost\nall cases, but quality still drops with larger beams. The main\ncause of deteriorating quality are shorter translations under wider\nbeams.\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Transformers",
        "url"       : "/machine-translation/Transformer",
        "date"      : "12/06/2017",
        "content": "Transformer architecture is a novel architecture for encoder-decoder\nparadigm created in an attempt to combine all good things from\nSeq2Seq\narchitecture and\nConvS2S with\nattention mechanisms. Transformer was proposed by a team from Google\nResearch and Google Brain in 2017 and published in a paper under the\nname: “Attention is all you\nneed”. The official code for this\npaper can be found on the Tensor2Tensor official GitHub repository:\ntensor2tensor.\n\nTransformer architecture deals with the input text data in an\nencoder-decoder manner the same as Seq2Seq and tries to parallelize the\ninput data the same as ConvS2S. In this paper, the Transformer\narchitecture consists of six layers of encoder and six layers of decoder\nas shown in the following figure:\n\n\n    \n\n\nArchitecture\n\nMost competitive machine translation models have an encoder-decoder\nstructure where the encoder maps an input sequence of symbol\nrepresentations $X = \\left( x_{1},\\ …x_{n} \\right)$ to a sequence of\ncontinuous representations $Z = \\left( z_{1},\\ …z_{n} \\right)$. Given\n$Z$, the decoder then generates an output sequence\n$Y = \\left( y_{1},\\ …y_{m} \\right)$ of symbols in an autoregressive\nmanner (one token at a time).\n\nThe most critical and influential part of the Transformer is the\nattention mechanism which takes a quadratic time and space over the\ninput sequence which makes training Transformer takes longer time that\nSeq2Seq and ConvS2S models. In this transformer architecture, there are\nthree different attention mechanisms used:\n\n\n  \n    Attention between the input tokens (self-attention).\n  \n  \n    Attention between the output tokens (self-attention).\n  \n  \n    Attention between the input and the output tokens\n  \n\n\n\n  Note:\nThe attention between the input (or output) tokens is called\nself-attention because the attention is between the same parameters.\n\n\nPadding\n\nTo be able to parallelize sentences with different lengths in\ntransformer, we need to define a value that represents the maximum\nlength (MAX_LENGTH) found in our training data. And all sentences whose\nlength is less than MAX_LENGTH should be padded using a PAD vector.\n\nSo, in the following image we have a mini-batch of three sentences where\nthe longest one is seven-tokens long. And the MAX_LENGTH is nine. In\npractice, PAD is the $0^{th}$ index of the embedding matrix which means it\nwill be learnable vector. It’s learnable for convenience not because we\nneed it to be. Also, the PAD vector should be ignored when computing the\nloss.\n\n\n    \n\n\n\n  Note:\nIn fairseq framework, padding is done\nrandomly at either the beginning of the sentence or at the end. Also, the pad\ntoken &amp;lt;p&amp;gt; has an index of 1 while index 0 is reserved for the beginning\nof the sentence token &amp;lt;s&amp;gt;.\n\n\nEncoder\n\n\n    \n\n\nWe are going to focus on the encoder part of the transformer architecture\nwhich consists of different modules:\n\n\n  \n    Embedding: where we map words into vectors representing their\nmeaning such that similar words will have similar vectors.\nThe embedding matrix have a size of $\\mathbb{R}^{n \\times d_m}$ where $n$\nis the input length and $d$ is the embedding dimension.\n  \n  \n    Positional Encoding: Word meaning differs based on its position\nin the sentence. A positional vector is a vector of the same size as\nthe embedding vector that gives context based on word-position in a\nsentence. This can be done by applying following equation:\n  \n\n\n\\[\\text{PE}_{\\left( \\text{pos},\\ 2i \\right)} = \\sin\\left( \\frac{\\text{pos}}{10000^{\\frac{2i}{d_m}}} \\right)\n\\ \\ \\ \\ \\ \n\\text{PE}_{\\left( \\text{pos},\\ 2i + i \\right)} = \\cos\\left( \\frac{\\text{pos}}{10000^{\\frac{2i}{d_m}}} \\right)\\]\n\n    Where $pos$ is the word position/index (starting from\nzero). $i$ is the $i^{th}$ value of the word embedding and $d_m$ is the size of\nthe word embedding. So, if $i$ is even, then we are going to apply the first\nequation; and if $i$ is odd, then we are going to apply the second\nequation. After getting the positional vectors, we add them to the\noriginal embedding vector to get context vector:\n\n\n    \n\n\nI know these functions don’t make sense and the original paper says the\nfollowing:\n\n\n  “We tried to encode position into word embedding using sinusoidal\nfunctions and using learned positional embeddings, and we found that the two\nversions produced nearly identical results.”\n\n\nBut in case you wanted to dig deeper in this part, check this YouTube\nvideo. It’s a good start. Also,\nlook into this article.\n\n\n  Single-Head Self-Attention:\nSelf-attention allows the encoder to associate each input word to\nother words in the input. To achieve self-attention, we feed the\nembedded input $X \\in \\mathbb{R}^{n \\times d_{m}}$ into three\ndifferent linear fully-connected layers\n$W^{Q},W^{K} \\in \\mathbb{R}^{d_{m} \\times d_{k}},\\ W^{V} \\in \\mathbb{R}^{d_{m} \\times d_{v}}$\nproducing three different matrices respectively; which are query\n$Q \\in \\mathbb{R}^{n \\times d_{k}}$, key\n$K \\in \\mathbb{R}^{n \\times d_{k}}$, and value\n$V \\in \\mathbb{R}^{n \\times d_{v}}$.\n\n\n\\[Q = XW^{Q},\\ \\ \\ \\ K = XW^{K},\\ \\ \\ \\ V = XW^{V}\\]\n\n\n    \n\n\n   Now, the attention mechanism will attend the resulting three matrices\nvia the following equation:\n\n\\[\\text{Attention}\\left( Q,\\ K,\\ V \\right) = softmax\\left( \\frac{QK^{T}}{\\sqrt{d_{k}}} \\right)V\\]\n\n   So, we are going to perform a dot product of Q and K to get a score\nmatrix that scores the relation between each word in the input and\nthe other words in the input as well.\n\n\n    \n\n\n   Then, these scores are getting scaled down by dividing over the\nsquare root of the dimension of query and key (which is $d$) to\nallow more stable gradients as the dot product could lead to\nexploding values:\n\n\n    \n\n\n   Then, we are going to perform a Softmax over these down-scaled\nscores to get the probability distribution which is called the\nattention weights:\n\n\n    \n\n\n   Finally, we are going to perform a dot product between the attention\nweights and the values V to get an output vector. The higher the attention\nweight is, the higher it contributes to the output vector.\n\n\n    \n\n\n\n  Note:\nThe name of these three vectors comes from retrieval systems. So,\nwhen you type a query on Google to search for, this query\nwill be mapped to a set of results keys to score each\nresult. And the highest results will be the values you\nwere looking for.\n\n\n\n  Multi-Head Self-Attention:\nA multi-head self-attention is just performing the single-head\nself-attention $h$ times and concatenating the\noutput matrices together before applying a linear layer\n$W^{O} \\in \\mathbb{R}^{h d_v \\times d_m}$ as shown in the following formula:\n\n\n\\[\\text{MultiHead}\\left( Q,\\ K,\\ V \\right) = Concat\\left( \\text{head}_{1},...\\text{head}_{h} \\right)\\ W^{O}\\]\n\n\\[\\text{head}_{i} = \\text{Attention}\\left( Q_i, K_i, V_i \\right)\\]\n\n     In theory, this will make each head learn something\ndifferent about the input. After concatenation, we apply a linear\nfully-connected layer to match dimensions for the residual connection.\nThe following image shows the multi-head attention of just two heads $h=2$:\n\n\n    \n\n\n\n  Residual Connection &amp;amp; Normalization:\nAfter the multi-head self-attention, the positional input embedding\nis added to the output vectors. This is known as a “residual\nconnection” which is mainly used to prevent gradient from vanishing.\nAfter that, a layer normalization is applied:\n\n\n\n    \n    \n\n\n\\[x_{l + 1} = \\text{LayerNorm}\\left( x_{l} + F_{l}(x_{l}) \\right)\\]\n\n   Then, we apply a batch or layer normalization. The difference is\npretty subtle where the batch normalization normalizes over all data in the\nbatch and the layer normalization normalizes over all weights in the layer.\n\n\n  Feed-forward:\nNow, the normalization output gets fed to the feed-forward network\nfor further processing. The feed-forward network is just a couple of\nlinear layers with a $\\text{ReLU}$ activation function in between.\nThe dimension of the feed-forward network is defined by the\n$d_{\\text{ff}}$ parameter where $W_1 \\in \\mathbb{R}^{d_{m} \\times d_{ff}},\nW_2 \\in \\mathbb{R}^{d_{ff} \\times d_{m}}$ are the learnable weights and \n$b_1 \\in \\mathbb{R}^{d_{ff}}, b_2 \\in \\mathbb{R}^{d_{m}}$ are the\nlearnable biases.\n\n\n\\[\\text{FFN}\\left( x \\right) = \\text{ReLU}\\left( xW_{1} + b_{1} \\right)W_{2} + b_{2}\\]\n\n\n    \n\n\nDecoder\n\n\n    \n\n\nIn this part, we are going to focus on the\ndecoder part of the transformer architecture. As we can see, it’s the\nsame components as the encoder except for two things:\n\n\n  \n    Shifted-right inputs:\nInput (sentence in another language) is shifted right by one word\nwhile training because we want to make sure the encoder was able to\nget this word before updating its value.\n  \n  \n    Masked Multi-Head self-Attention:\nThe masked multi-head is a little bit different than the\nmulti-head self-attention one. As the masked one will only be able\nto access the previous words not the following one. In order to\nsolve that, we are going to create look-ahead mask matrix that masks\nany further values with -inf.\n  \n\n\n\n    \n\n\n   We are using -inf as a small numbers that will be zero\nwhen applying the Softmax.\n\n\n  Multi-head Attention #2:\nThe query and key of this block will come from the encoder output\nand the values will be the output of the masked multi-head block.\n\n\nTraining\n\nBefore training, sentences were encoded using byte-pair encoding with a\nshared source-target vocabulary of about $37,000$ tokens for WMT\nEnglish-German and 32,000 for English-French.\n\nFor training, each training batch contained approximately $25,000$\ntokens. They used Adam optimizer with\n$\\beta_{1} = 0.9,\\ \\beta_{2} = 0.98$ and $\\epsilon = 10^{- 9}$. Learning\nrate was varied over the course of training, according to the following\nformula:\n\n\\[lr = d_{\\text{model}}^{- 0.5}.\\min\\left( {step\\_ num}^{- 0.5},\\ step\\_ num*{warmup\\_ steps}^{- 1.5} \\right)\\]\n\nThis corresponds to increasing the learning rate linearly for the first\n$\\text{warmup_steps}$ training steps, and decreasing it thereafter\nproportionally to the inverse square root of the $\\text{step_num}$. They used\n$\\text{warmup_steps} = 4000$.\n\nFor regularization, they used dropout to the output of each sub-layer,\nbefore it is added to the sub-layer input and normalized. In addition,\nthey applied dropout to the sums of the embeddings and the positional\nencodings in both the encoder and decoder stacks. Also, label smoothing\n$\\epsilon_{l_{s}} = 0.1$ was used.\n\nThere were two variants of the Transformer configurations found in the\npaper, Transformer-base and Transformer-big configurations which can be\nsummarized in the following table:\n\n\n\n    \n        \n            \n            $$N$$\n            $$d_{m}$$\n            $$d_{\\text{ff}}$$\n            $$h$$\n            $$d_{k}$$\n            $$d_{v}$$\n            $$P_{\\text{dropout}}$$\n            $$\\epsilon_{l_{s}}$$\n            # parameters\n            train steps\n        \n    \n    \n        Base\n        6\n        512\n        2048\n        8\n        64\n        64\n        0.1\n        0.1\n        65 M\n        100k\n    \n    \n        Large\n        6\n        1024\n        4096\n        16\n        64\n        64\n        0.3\n        0.1\n        213 M\n        300k\n    \n\n\n\nFor decoding, they used beam search with a $\\text{beam size} = 4$ and length\npenalty $\\alpha = 0.6$.\n\nThe following table shows that the big transformer model achieves\nstate-of-the-art performance on the WMT 2014 English-German translation\ntask and English-French translation:\n\n\n    \n\n\nThe base models score was obtained by averaging the last 5 checkpoints,\nwhich were written at 10-minute intervals. For the big models, the last\n20 checkpoints were averaged.\n\n\n  Note:\nFor the English-French dataset, the big transformer used\n$P_{\\text{dropout}} = 0.1$, instead of $P_{\\text{dropout}} = 0.3$.\n\n\nLayer Normalization\n\nNormalization is an important part of the Transformer architecture as it\nimproves the performance and avoids the overfitting. Here, we are going\nto discuss the different layer normalization techniques that can be used\nbased as suggested by this paper: Transformers without Tears: Improving\nthe Normalization of\nSelf-Attention The official code\nfor this paper can be found in this GitHub repository:\ntransformers_without_tears.\n\nThis paper compares between two different orders of layer normalization\nin the Transformer architecture:\n\n\n  Post-Norm:\nPost-normalization is the default type of normalization used in the\nstandard Transformer architecture. It’s called that because it\noccurs after the residual addition:\n\n\n\\[x_{l + 1} = \\text{LayerNorm}\\left( x_{l} + F_{l}(x_{l}) \\right)\\]\n\n\n  Pre-Norm:\nPre-normalization is applied immediately before the sublayer.\nPre-Norm enables warmup-free training providing greater stability\nand doesn’t get affected by the weight initialization unlike\nPost-Norm:\n\n\n\\[x_{l + 1} = x_{l} + F_{l}\\left( \\text{LayerNorm}(x_{l}) \\right)\\]\n\n\n  Very Important Note:\nIn the paper, they found out that post-normalization works best\nwith high-resource languages while pre-normalization works best with\nlow-resource languages.\n\n\nAlso, in the paper, they proposed an alternative to the layer\nnormalization:\n\n\n  Scale-Norm:\nScale-Norm is an alternative for layer normalization. As we can see\nfrom the following equation, Scale-Norm replaced the two learnable\nparameters $\\gamma,\\ \\beta$ in layer normalization with one global\nlearned scalar $g$:\n\n\n\\[\\text{ScaleNorm}\\left( x;g \\right) = g\\frac{x}{\\left\\| x \\right\\|}\\]\n\n\n  Scale-Norm + Fix-Norm:\nFix-Norm is applied to the word embeddings. It looks exactly like\nScale-Norm with only one global learnable scalar g, so we can apply\nboth of them jointly like so:\n\n\n\\[ScaleNorm + FixNorm\\left( x,w;g \\right) = g\\frac{\\text{w.x}}{\\left\\| w \\right\\|.\\left\\| x \\right\\|}\\]\n\nAnd the following are the results published in the paper on various\nmachine translation directions:\n\n\n    \n\n\nLayerDrop\n\nLayerDrop is a novel regularization method for Transformers used to\nprevent them from overfitting. This method was proposed by Facebook AI\nin 2019 and published in this paper: Reducing Transformer Depth On\nDemand With Structured Dropout.\nThe official code for this paper can be found in the official Fairseq\nGitHub repository:\nfairseq/layerdrop.\n\nDeep Networks with Stochastic\nDepth paper has shown that\ndropping layers during training can regularize and reduce the training\ntime of very deep convolutional networks. And this is the core idea of\nLayerDrop where entire layers are randomly dropped at training time\nwhich regularizes very deep Transformers and stabilizes their training,\nleading to better performance.\n\nThe following figure shows a comparison of a 9-layer transformer trained\nwith LayerDrop (right) and 3 transformers of different sizes (left). As\nwe can see, the different pruned version of the transformer on the right\nobtained better results than the same sized transformers trained from\nscratch:\n\n\n    \n\n\nWhich shows that LayerDrop also acts like a distillation technique that\ncan lead to small and efficient transformers of any depth which can be\nextracted automatically at test time from a single large pre-trained\nmodel, without the need for finetuning.\n\nLayerDrop does not explicitly provide a way to select which group of\nlayers will be dropped. So, the publishers considered several different\npruning strategies:\n\n\n  \n    Every Other:\nA straightforward strategy is to simply drop every other layer.\nPruning with a drop rate $p$ means dropping the layers at a depth\n$d$ such that\n$d \\equiv 0\\left( \\text{mod}\\left\\lfloor \\frac{1}{p} \\right\\rfloor \\right)$.\nThis strategy is intuitive and leads to balanced networks.\n  \n  \n    Search on Valid:\nAnother possibility is to compute various combinations of layers to\nform shallower networks using the validation set, then select the\nbest performing for test. This is straightforward but\ncomputationally intensive and can lead to overfitting on validation.\n  \n  \n    Data Driven Pruning:\nAnother approach is to learn the drop rate of each layer. Given a\ntarget drop rate $p$, we learn an individual drop rate $p_{d}$ for\nthe layer at depth $d$ such that the average rate over layers is\nequal to $p$.\n\n    The “Every Other” works the best with the following drop rate\n$p$ where $N$ is the number of layers, $r$ is the target pruned\nsize:\n  \n\n\n\\[p = 1 - \\frac{r}{N}\\]\n\n\n  Note:\nIn the paper, they used a LayerDrop rate of $p = 0.2$ for all their\nexperiments. However, they recommend using $p = 0.5$ to obtain very\nsmall models at inference time.\n\n\nDropHead\n\nDropHead is another novel regularization method for Transformers used to\nprevent overfitting. This method was proposed by Microsoft Research Asia\nin 2020 and published in this paper: Scheduled DropHead: A\nRegularization Method for Transformer\nModels. There is unofficial\nimplementation for this paper, it can be found in this GitHub\nrepository:\ndrophead-pytorch.\n\nIn the core, DropHead drops entire attention heads during training to\nprevent the multi-head attention model from being dominated by a small\nportion of attention heads which can help reduce the risk of overfitting\nand allow the models to better benefit from the multi-head attention.\nThe following figure shows the difference between dropout (left) and\nDropHead (right):\n\n\n    \n\n\nIn the paper, they proposed a specific dropout rate scheduler for the\nDropHead mechanism, which looks like a V-shaped curve (green curve\nbelow): It applies a relatively high dropout rate of $p_{\\text{start}}$\nand linearly decrease it to $0$ during the early stage of training,\nwhich is empirically chosen to be the same training steps for learning\nrate warmup. Afterwards, it linearly increases the dropout rate to\n$p_{\\text{end}}$. To avoid introducing additional hyper-parameters, they\ndecided to set $p_{\\text{start}} = p_{\\text{end}}$.\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "MUSE",
        "url"       : "/machine-translation/MUSE",
        "date"      : "11/10/2017",
        "content": "MUSE or “Multilingual Unsupervised and Supervised Embeddings” is a\nframework created by Facebook AI in 2017 and published in this paper:\nWord Translation Without Parallel\nData. The official implementation\nof the framework can be found in this GitHub repository:\nMUSE.\n\nFrom the name of the paper, we can see that MUSE is for word\ntranslation not machine translation which shows that\nMUSE is more of a look-up table (bilingual dictionary) rather than being\na machine translation model. The crazy part about MUSE is that It builds\nbilingual dictionary between two languages without the use of any\nparallel corpora. And despite that it outperforms supervised\nstate-of-the-art methods on English-Italian where metric is the average\nprecision over top 1, 5, 10 words respectively.\n\n\n    \n\n\nMUSE leverages adversarial training to learn that linear mapping from a\nsource to a target space and it operates in three steps as shown in the\nfollowing figure where there are two sets of embeddings trained\nindependently on monolingual data, English words in red denoted by\n$\\mathbf{X}$ and Italian words in blue denoted by $\\mathbf{Y}$, which we\nwant to align/translate. In this figure, each dot represents a word in\nthat space where the dot size is proportional to the frequency of the\nwords in the training corpus.\n\n\n    \n\n\nMUSE focuses on learning a mapping $\\mathbf{\\text{WX}}$ between these\ntwo sets such that translations are close in the shared space. It does\nthat in three steps:\n\n\n  Adversarial Model:\nUsing adversarial learning, we learn a rotation matrix $\\mathbf{W}$\nwhich roughly aligns the two distributions. The green stars are\nrandomly selected words that are fed to the discriminator to\ndetermine whether the two word embeddings come from the same\ndistribution.\n\n\n\n    \n\n\n\n  Refinement Procedure:\nThe mapping $\\mathbf{W}$ is further refined via Procrustes. This\nmethod uses frequent words aligned by the previous step as anchor\npoints, and minimizes an energy function that corresponds to a\nspring system between anchor points. The refined mapping is then\nused to map all words in the dictionary.\n\n\n\n    \n\n\n\n  CSLS:\nFinally, we translate by using the mapping $\\mathbf{W}$ and a\ndistance metric (CSLS) that expands the space where there is high\ndensity of points (like the area around the word “cat”), so that\n“hubs” (like the word “cat”) become less close to other word vectors\nthan they would otherwise.\n\n\n\n    \n\n\nAdversarial Model\n\nLet $X = \\left\\{ x_{1},\\ …x_{n} \\right\\}$ and\n$Y = \\left\\{ y_{1},\\ …y_{m} \\right\\}$ be two sets of $n$ and $m$ word\nembeddings coming from a source and a target language respectively. An\nadversarial setting formed as a two-player game where the two players\nare:\n\n\n  Discriminator:\nA classification model trained to distinguish between the mapped\nsource embeddings $\\mathbf{W}\\mathbf{X}$ and the target embeddings\n$\\mathbf{Y}$. The discriminator aims at maximizing its ability to\nidentify the origin of an embedding.\n\n\n\\[\\mathcal{L}_{D}\\left( \\theta_{D} \\middle| W \\right) = - \\frac{1}{n}\\sum_{i = 1}^{n}{\\log P_{\\theta_{D}}\\left( \\text{source} = 1 \\middle| Wx_{i} \\right)} - \\frac{1}{m}\\sum_{i = 1}^{m}{\\log P_{\\theta_{D}}\\left( \\text{source} = 0 \\middle| y_{i} \\right)}\\]\n\n\n  W Mapping:\n(Which can be seen as a generator) is model jointly trained to fool\nthe discriminator by making $\\mathbf{\\text{WX}}$ and $\\mathbf{Y}$ as\nsimilar as possible.\n\n\n\\[\\mathcal{L}_{W}\\left( W \\middle| \\theta_{D} \\right) = - \\frac{1}{n}\\sum_{i = 1}^{n}{\\log P_{\\theta_{D}}\\left( \\text{source} = 0 \\middle| Wx_{i} \\right)} - \\frac{1}{m}\\sum_{i = 1}^{m}{\\log P_{\\theta_{D}}\\left( \\text{source} = 1 \\middle| y_{i} \\right)}\\]\n\nWhere:\n\n\n  \n    $\\mathcal{L}_D$ is the discriminator loss while $\\mathcal{L}_W$\nis the mapping loss.\n  \n  \n    $\\theta_{D}$ is the discriminator parameters.\n  \n  \n    $W$ is the learned linear mapping.\n  \n  \n    $P_{\\theta_{D}}\\left( \\text{source} = 1 \\middle| z \\right)$ is the\nprobability that a vector $z$ is the mapping of a source embedding.\nAnd $P_{\\theta_{D}}\\left( \\text{source} = 0 \\middle| z \\right)$ is\nthe probability that a vector $z$ is the mapping of a target\nembedding.\n  \n\n\nRefinement Procedure\n\nThe adversarial approach tries to align all words irrespective of their\nfrequencies. However, rare words are harder to align. Under the\nassumption that the mapping is linear, it is then better to infer the\nmapping using only the most frequent words as anchors. The mapping\n$\\mathbf{W}$ is further refined via Procrustes analysis which\nadvantageously offers a closed form solution obtained from the singular\nvalue decomposition:\n\n\\[W^{*} = \\underset{W \\in O_{d}\\left( \\mathbb{R} \\right)}{\\arg\\min}{\\left\\| WX - Y \\right\\|_{F} = UV^{T},\\ \\ \\ \\ \\ \\ \\text{with }\\text{U}\\Sigma V^{T} = \\text{SVD}\\left( YX^{T} \\right)}\\]\n\nThis method uses frequent words aligned by the previous step as anchor\npoints, and minimizes an energy function that corresponds to a spring\nsystem between anchor points. The refined mapping is then used to map\nall words in the dictionary.\n\nCSLS\n\nCSLS stands for “Cross-Domain Similarity Local Scaling” which is a novel\nmetric proposed by the authors as a comparison metric between two\ndifferent language embeddings. Given a mapped source word embedding\n$Wx_{s}$ and a target embedding $y_{t}$, the CSLS can be formulated as:\n\n\\[\\text{CSLS}\\left( Wx_{s},\\ y_{t} \\right) = 2\\cos\\left( Wx_{s},\\ y_{t} \\right) - r_{T}\\left( Wx_{s} \\right) - r_{S}\\left( y_{t} \\right)\\]\n\n\\[r_{T}\\left( Wx_{s} \\right) = \\frac{1}{K}\\sum_{y_{t} \\in \\mathcal{N}_{T}\\left( Wx_{s} \\right)}^{}{\\cos\\left( Wx_{s},\\ y_{t} \\right)}\\]\n\n\\[r_{S}\\left( y_{t} \\right) = \\frac{1}{K}\\sum_{Wx_{s} \\in \\mathcal{N}_{S}\\left( y_{t} \\right)}^{}{\\cos\\left( Wx_{s},\\ y_{t} \\right)}\\]\n\nNote that all $K$ elements of $\\mathcal{N}_{T}\\left( Wx_{s} \\right)$\nare words from the target language and all $K$ elements of\n$\\mathcal{N}_{S}\\left( y_{t} \\right)$ are mapped words from the\nsource language.\n\nTraining Details\n\n\n  \n    They used unsupervised word vectors that were trained using\nfastText.\n  \n  \n    These correspond to monolingual embeddings of dimension 300 trained\non Wikipedia corpora; therefore, the mapping $\\mathbf{W}$ has size\n300×300.\n  \n  \n    Words are lower-cased, and those that appear less than 5 times are\ndiscarded for training.\n  \n  \n    As a post-processing step, they only considered the first 200k most\nfrequent words.\n  \n  \n    For discriminator, they used a MLP with two hidden layers of size\n2048, and Leaky-ReLU activation functions with dropout of 0.1\ntrained using stochastic gradient descent with a batch size of 32, a\nlearning rate of 0.1 and a decay of 0.95 both for the discriminator\nand W mapping.\n  \n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "UNdreaMT",
        "url"       : "/machine-translation/UNdreaMT",
        "date"      : "30/10/2017",
        "content": "The first model proposed in this area was created under the supervision\nof Cho in 2017 and published in this paper: “Unsupervised Neural\nMachine Translation”. The official code\nof this paper can be found in the following GitHub repository:\nUNdreaMT. The proposed system follows a\nstandard encoder-decoder architecture with an attention mechanism where:\n\n\n  \n    There is one encoder which is a two-layer bidirectional RNN\n  \n  \n    There are two decoders, each is a two-layer RNN.\n  \n  \n    All RNNs use GRU cells with 600 hidden units, and the dimensionality\nof the embeddings is set to 300.\n  \n\n\n\n    \n\n\nThere are, however, three important aspects in which the system\ndiffers from the standard encoder-decoder NMT:\n\n\n  \n    Dual structure:\nWhile NMT systems are typically built for a specific translation\ndirection (A→B). Here, the model exploited the dual nature of\nmachine translation and handle both directions at the same time\n(A→B) and (B→A). Hence the usage of two decoders.\n  \n  \n    Shared encoder:\nThe system makes use of only one encoder that is shared by both\nlanguages involved. This universal encoder is aimed to produce a\nlanguage independent representation of the input text, which each\ndecoder should then transform into its corresponding language.\n  \n  \n    Fixed Cross-lingual embeddings at encoder:\nWhile most NMT systems randomly initialize their embeddings and\nupdate them during training, they used pre-trained cross-lingual\nembeddings in the encoder that are kept fixed during training.\n  \n\n\nThis NMT system is trained in unsupervised using the following two\nstrategies:\n\n\n  \n    Denoising.\n  \n  \n    On-the-fly back-translation\n  \n\n\nDuring training, we alternate between the two languages. Given two\nlanguages L1 and L2, each iteration would perform one mini-batch of\ndenoising for L1, another one for L2, one mini-batch of on-the-fly\nback-translation from L1 to L2, and another one from L2 to L1.\n\nDenoising\n\nThe whole idea of the system is to train a model to reconstruct its own\ninput. More concretely, the system takes an input sentence in a given\nlanguage, encode it using the shared encoder, and reconstruct the\noriginal sentence using the decoder of that language. That’s why we have\ntwo decoders for each language.\n\nBut this training procedure is essentially a trivial copying task. And\nit’s highly probable that our model could just blindly copy elements\nfrom the input sequence without gaining any real knowledge. In order to\navoid that, they used denoising. Denoising is the\nprocess of applying a noise function on the input. The noise function they\nused is “swapping”. For a sequence of N elements, they made N/2 random\nswaps of this kind.\n\nThis way, the system needs to learn about the internal structure of the\nlanguages involved to be able to recover the correct word order. At the\nsame time, by discouraging the system to rely too much on the word order\nof the input sequence.\n\nOn-the-fly Back-translation\n\nSo far, the model only uses monolingual data. In order to train our\nsystem in a true translation setting without violating the constraint of\nusing nothing but monolingual corpora, they adapted the back-translation\napproach proposed to the model.\n\nMore concretely, given an input sentence in one language, the system is\nused in inference mode with greedy decoding to translate the input to\nthe other language. This way, we obtain a pseudo-parallel sentence pair,\nand train the system to predict the original sentence from this\nsynthetic translation.\n\nNote that, contrary to standard back-translation, which uses an\nindependent model to back-translate the entire corpus at one time, the\nmodel takes advantage of the dual structure of the proposed architecture\nto back-translate each mini-batch on-the-fly. Hence, the name\n“on-the-fly back-translation”.\n\nResults\n\nTO BE CONTINUED...\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Unsupervised Machine Translation with Monolingual Data",
        "url"       : "/machine-translation/UMT_with_monolingual_data",
        "date"      : "31/10/2017",
        "content": "The second model proposed in this area was created by Facebook AI in\n2017 and published in this paper: “Unsupervised Machine Translation\nUsing Monolingual Corpora only”.\nThe proposed system follows a standard encoder-decoder architecture with\nstandard attention mechanism assisted by a back-translation procedure\nwhere\n\n\n  \n    There is one encoder which is a three-layer bidirectional RNN\nresponsible for encoding source sentences $\\mathcal{D}_{\\text{src}}$\nto a latent space $Z$.\n  \n  \n    There is one decoder which is a three-layer unidirectional RNN\nresponsible for decoding target sentences $\\mathcal{D}_{\\text{tgt}}$\nto a latent space $Z$.\n  \n  \n    All RNNs use LSTM cells with 300 hidden units, and the\ndimensionality of the embeddings is set to 300 as well.\n  \n\n\nThe key idea is to build a common latent space between two languages\nand to learn to translate by reconstructing in both domains\naccording to two principles:\n\n\n  Denoising: The model has to be able to reconstruct a sentence in\na given language from a noisy version of it, as in standard\ndenoising auto-encoders.\n\n\n\n    \n\n\n\n  Cross-domain Translation: The model also has to be able to\nreconstruct a source sentence given a noisy translation of the same\nsentence in the target language, and vice versa.\n\n\n\n    \n\n\nDenoising\n\nThe whole idea of the system is to train a model to reconstruct its own\ninput. More concretely, the system takes an input sentence in a given\nlanguage, encode it using the shared encoder, and reconstruct the\noriginal sentence using the decoder of that language. That’s why we have\ntwo decoders for each language.\n\nBut this training procedure is essentially a trivial copying task. And\nit’s highly probable that our model could just blindly copy elements\nfrom the input sequence without gaining any real knowledge. In order to\navoid that, they used denoising. Denoising is\nthe process of applying a noise function on the input. They used two\ndifferent noise functions:\n\n\n  \n    Shuffling: They shuffle the input sentence where a word at\nposition $i$ can’t be further than either $i - k$ or $i + k$ where\nthey found out that $k = 3$ is a good value.\n  \n  \n    Dropping: They drop every word in the input sequence by a\nprobability of $p = 0.1$\n  \n\n\nThis way, the system needs to learn about the internal structure of\nthe languages involved to be able to recover the correct word order.\nAt the same time, by discouraging the system to rely too much on the\nword order of the input sequence.\n\nCross-domain Translation\n\nThe second objective is to constrain the model to be able to map an\ninput sentence from a the language $\\ell_{1}$ to $\\ell_{2}$ and vice\nversa. The principle can be summarized in the following steps:\n\n\n  \n    We will use an earlier version of the model, denoted $M$, to\ngenerate a translation $y$ of a given sentence $x$.\n  \n  \n    Then, this translation $y$ will be denoised using our denoising\nfunctions (discussed in the last part) to get a corrupted version of\nthe translation $C\\left( y \\right)$.\n  \n  \n    This corrupted version $C\\left( y \\right)$ will be inserted as input\nto the current version of the model and the model will try to\nreconstruct $x$ from $C\\left( y \\right)$.\n  \n\n\nAdversarial Training\n\nTo boost performance, they created another neural network, which we will\nrefer to as the discriminator, to classify between the encoding of\nsource sentences and the encoding of target sentences.\n\nThe discriminator is a multilayer perceptron with three hidden layers of\nsize 1024, Leaky-ReLU activation functions and an output logistic unit.\nThe discriminator is trained using RMSProp with a learning rate of\n0.0005.\n\nThe discriminator operates on the output of the encoder , which is a\nsequence of latent vectors $Z$ and produces a binary prediction about\nthe language of the encoder input sentence.\n\nFinal Objective Function\n\nThe final objective function at one iteration of our learning algorithm\nis thus:\n\n\\[\\mathcal{L}\\left( \\theta_{\\text{enc}},\\ \\theta_{\\text{dec}},\\ Z \\right) = \\lambda_{\\text{auto}}\\left\\lbrack \\mathcal{L}_{\\text{auto}}\\left( \\theta_{\\text{enc}},\\ \\theta_{\\text{dec}},\\ Z,\\ src \\right) \\\\\n+ \\mathcal{L}_{\\text{auto}}\\left( \\theta_{\\text{enc}},\\ \\theta_{\\text{dec}},\\ Z,\\ tgt \\right) \\right\\rbrack \\\\\n+ \\lambda_{\\text{cd}}\\left\\lbrack \\mathcal{L}_{\\text{cd}}\\left( \\theta_{\\text{enc}},\\ \\theta_{\\text{dec}},\\ Z,\\ src,\\ tgt \\right) \\\\\n+ \\mathcal{L}_{\\text{cd}}\\left( \\theta_{\\text{enc}},\\ \\theta_{\\text{dec}},\\ Z,\\ src,\\ tgt \\right) \\right\\rbrack \\\\\n+ \\lambda_{\\text{adv}}\\left\\lbrack \\mathcal{L}_{\\text{adv}}\\left( \\theta_{\\text{enc}},\\ Z \\middle| \\theta_{D} \\right) \\right\\rbrack\\]\n\nWhere:\n\n\n  $\\mathcal{L}_{\\text{auto}}$ is the auto-encoding loss function;\nwhere\n$\\widehat{x}\\sim d\\left( e\\left( C\\left( x \\right),\\ \\ell \\right),\\ \\ell \\right)$\nmeans that $\\widehat{x}$ is a reconstruction of the corrupted\nversion of $x$. And $\\mathrm{\\Delta}\\left( \\widehat{x},\\ x \\right)$\nis the sum of token-level cross-entropy losses:\n\n\n\\[\\mathcal{L}_{\\text{auto}}\\left( \\theta_{\\text{enc}},\\ \\theta_{\\text{dec}},\\ Z,\\ \\ell \\right) = \\mathbb{E}_{x\\sim D_{\\ell},\\ \\widehat{x}\\sim d\\left( e\\left( C\\left( x \\right),\\ \\ell \\right),\\ \\ell \\right)}\\left\\lbrack \\mathrm{\\Delta}\\left( \\widehat{x},\\ x \\right) \\right\\rbrack\\]\n\n\n  $\\mathcal{L}_{\\text{cd}}$ is the cross-domain translation loss\nfunction:\n\n\n\\[\\mathcal{L}_{\\text{cd}}\\left( \\theta_{\\text{enc}},\\ \\theta_{\\text{dec}},\\ Z,\\ \\ell_{1},\\ \\ell_{2} \\right) = \\mathbb{E}_{x\\sim D_{\\ell_{1}},\\ \\widehat{x}\\sim d\\left( e\\left( C\\left( M\\left( x \\right) \\right),\\ \\ell_{2} \\right),\\ \\ell_{1} \\right)}\\left\\lbrack \\mathrm{\\Delta}\\left( \\widehat{x},\\ x \\right) \\right\\rbrack\\]\n\n\n  $\\mathcal{L}{\\text{adv}}$ is the adversarial loss function where\n$e\\left( x{i},\\ell_{i} \\right)$ is the output of the encoder and\n$p_{D}$ is the discriminator output which is either $0$ or $1$:\n\n\n\\[\\mathcal{L}_{\\text{adv}}\\left( \\theta_{\\text{enc}},\\ \\text{Z\\ } \\middle| \\ \\theta_{D} \\right) = - \\mathbb{E}_{\\left( x_{i},\\ \\ell_{i} \\right)}\\left\\lbrack \\log\\left( p_{D}\\left( \\ell_{i} \\middle| e\\left( x_{i},\\ell_{i} \\right) \\right) \\right) \\right\\rbrack\\]\n\n\n  $\\lambda_{\\text{audo}},\\ \\lambda_{\\text{cd}},\\ \\lambda_{\\text{adv}}$\nare hyper-parameters weighting the importance of the auto-encoding,\ncross-domain translation and adversarial loss respectively.\n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "NAT: Non-Autoregressive Transformer",
        "url"       : "/machine-translation/NAT",
        "date"      : "07/11/2017",
        "content": "NAT, stands for “Non-Autoregressive Translation”, is an NMT model that\navoids the autoregressive property of the decoding and produces its\noutputs in parallel. NAT was created by Salesforce in 2017 and published\nin their paper: “Non-Autoregressive Neural Machine\nTranslation”. The official code\nfor this paper can be found on the official Salesforce GitHub\nrepository: nonauto-nmt.\n\nBefore getting into Non-Autoregressive model, let’s first recap what is\nan Autoregressive model. Given a source sentence\n$X = \\left( x_{1},\\ …x_{m} \\right)$, an Autoregressive model factors the\ndistribution over possible output sentences\n$Y = \\left( y_{1},\\ …y_{n} \\right)$ into a chain of conditional\nprobabilities with a an auto-regressive (left-to-right) manner as shown below:\n\n\\[p\\left( Y \\middle| X;\\theta \\right) = \\prod_{t = 1}^{n}{p\\left( y_{t} \\middle| y_{0:t - 1},\\ x_{1:m};\\ \\theta \\right)}\\]\n\nWhere the special tokens $y_{0} = \\left\\langle \\text{bos} \\right\\rangle$\nand $y_{n + 1} = \\left\\langle \\text{eos} \\right\\rangle$ are used to\nrepresent the beginning and end of all target sentences. These\nconditional probabilities are parameterized using a neural network\n$\\theta$.\n\nNAT Architecture\n\nThe autoregressive model corresponds to the word-by-word nature of human\nlanguage production and effectively captures the distribution of real\ntranslations. That’s why they achieve state-of-the-art performance on\nlarge-scale corpora. But there are also drawbacks such as individual steps of\nthe decoder must be run sequentially rather than in parallel which takes longer.\nThe NAT model proposed in this paper tries to solve that problem:\n\n\n    \n\n\nThe NAT model, as shown in the following figure, is composed of the following\nfour modules: Encoder stack, Decoder stack\n, a newly added fertility predictor added\nat the end of the encoder stack, and a translation predictor\n added at the end of the decoder stack.\n\n\n    \n\n\nEncoder Stack\n\nThe encoder stays unchanged from the original Transformer network ☺.\n\nDecoder Stack\n\nTo be able to parallelize the decoding process, the NAT needs to know\nhow long the target sentence will be. And since the target sentence\nlength is different but not that far from the source sentence, they\ninitialized the decoding process using copied source inputs from the\nencoder side based on the word’s “fertility”.\n\nSince we are processing the output as a whole, we no longer need the\nmasking before the self-attention. Instead, they only masked out each\nquery position only from attending to itself, which they found to\nimprove decoder performance relative to unmasked/standard  self-attention.\n\nAlso, they added a new block called “Multi-head Positional\nAttention” in each decoder layer, which is a multi-head attention\nmodule with the same general attention mechanism used in other parts of the\nTransformer network:\n\n\\[\\text{Attention}\\left( Q,K,\\ V \\right) = \\text{softmax}\\left( \\frac{QK^{T}}{\\sqrt{d_{\\text{model}}}} \\right)\\text{.V}\\]\n\nThis incorporates positional information directly into the attention\nprocess and provides a stronger positional signal than the embedding\nlayer alone.\n\nFertility Predictor\n\nWords’ fertility are integers for each word in the source sentence that \ncorrespond to the number of words in the target sentence that can be aligned to\nthat source word if a hard alignment algorithm was used (like\nIBM Models for example).\n\nUsing the fertility predictor and given a source sentence\n$X = \\left( x_{1},\\ …x_{m} \\right)$, the conditional probability\nover the possible output sentence $Y = \\left( y_{1},\\ …y_{n} \\right)$ is:\n\n\\[p\\left( Y \\middle| X;\\theta \\right) = \\sum_{f_{1},\\ ...f_{m} \\in \\mathcal{F}}^{}\\left( \\prod_{t = 1}^{m}{p_{F}\\left( f_{t} \\middle| x_{1:m};\\ \\theta \\right)}.\\prod_{t = 1}^{n}{p\\left( y_{t} \\middle| x_{1}\\left\\{ f_{1} \\right\\},\\ ...x_{m}\\left\\{ f_{m} \\right\\};\\ \\theta \\right)} \\right)\\]\n\nWhere:\n\n\n  $\\mathcal{F} \\in \\left( f_{1},\\ …f_{m} \\right)$ is the set of\nall fertility sequences, one fertility value per source token.\nKnowing that the sum of all fertility values is the length of the\ntarget sentence:\n\n\n\\[\\sum_{t = 1}^{m}f_{t} = n\\]\n\n\n  \n    $p_{F}\\left( f_{t} \\middle| x_{1:m};\\ \\theta \\right)$ is the\nfertility prediction model which is a one-layer neural network with\na softmax classifier ($L = 50$  in these experiments) on top of the\noutput of the last encoder layer. This models the way that fertility\nvalues are a property of each input word but depend on information\nand context from the entire sentence.\n  \n  \n    $x\\left\\{ f \\right\\}$ denotes the source token $x$ repeated\n$f$ times.\n  \n\n\nTranslation Predictor\n\nAt inference time, the model can identify the translation with the highest\nconditional probability by marginalizing over all possible latent fertility\nsequences. In other words, the optimal output translation sequence given a\nsource sentence $x$ and an optimal sequence of fertility values $\\widehat{f}$\nis:\n\n\\[\\widehat{Y} = G\\left( x_{1:m},\\ {\\widehat{f}}_{1:m};\\theta \\right)\\]\n\nAs seen from the previous equation, finding an optimal output translation\nsequence heavily depends on the fertility values. To search over the whole\nfertility space is a big task. So, they proposed three heuristic decoding\nalgorithms to reduce the search space of the NAT model given the fertility\ndistribution $p_{F}$:\n\n\n  Argmax Decoding:\nThe optimal sequence of fertility value is the highest-probability\nfertility for each input word:\n\n\n\\[{\\widehat{f}}_{t} = \\underset{f}{\\arg\\max}{p_{F}\\left( f_{t} \\middle| x_{1:m};\\theta \\right)}\\]\n\n\n  Average Decoding:\nThe optimal sequence of fertility value is the the expectation of\nits corresponding softmax distribution ($L = 50$  in these experiments):\n\n\n\\[{\\widehat{f}}_{t} = \\text{Round}\\left( \\sum_{f_{t}=1}^{L}{p_{F}\\left( f_{t} \\middle| x_{1:m};\\theta \\right).f_{t}} \\right)\\]\n\n\n  Noisy parallel decoding (NPD):\nA more accurate approximation of the true optimum of the target\ndistribution is to draw samples from the fertility space and compute\nthe best translation for each fertility sequence. We can then use\nthe autoregressive teacher to identify the best overall translation:\n\n\n\\[{\\widehat{f}}_{t} = \\underset{f_{t}\\sim p_{F}}{\\arg\\max}{p_{AR}\\left( G\\left( x_{1:m},\\ f_{1:m};\\theta \\right) \\middle| X;\\theta \\right)}\\]\n\nLoss Function\n\nGiven a source sentence $X = \\left{ x_{1},\\ …x_{m} \\right}$, a\ntarget sequence $Y = \\left{ y_{1},\\ …y_{n} \\right}$ and a fertility\nvalues $f_{1:m}$, the loss function of the NAT model $\\mathcal{L}$ can\nbe described below:\n\n\\[\\mathcal{L} = log\\sum_{f_{1:m} \\in \\mathcal{F}}^{}{p_{F}\\left( f_{1:m} \\middle| x_{1:m};\\ \\theta \\right)\\text{.p}\\left( y_{1:n} \\middle| x_{1:m};\\theta \\right)}\\]\n\nThe resulting loss function allows us to train the entire model in a\nsupervised fashion, using the inferred fertilities to simultaneously\ntrain the translation model $p$ and supervise the fertility neural\nnetwork model $p_{F}$.\n\n\n  Note:\nThere are two possible options that can be used as labels for the the\nfertility network. The first is an external aligner, which produces a\ndeterministic integer fertilities for each (source, target) pair in a\ntraining corpus. The second option is the attention weights used in an\nautoregressive teacher model.\n\n\nAfter training the NAT model to convergence, they proposed a fine-tuning\nstep where they freeze a trained autoregressive teacher model while\ntrain the NAT model on its soft labels. This introduced an additional\nloss term consisting of the reverse K-L divergence with the teacher\noutput distribution:\n\n\\[\\mathcal{L}_{\\text{RKL}}\\left( f_{1:m};\\ \\theta \\right) = \\sum_{t = 1}^{n}{\\sum_{y_{t}}^{}\\left\\lbrack \\text{log\\ }p_{\\text{AR}}\\left( y_{t} \\middle| {\\widehat{y}}_{1:t},\\ x_{1:m} \\right).p_{\\text{NA}}\\left( y_{t} \\middle| x_{1:m},\\ f_{1:m};\\theta \\right) \\right\\rbrack}\\]\n\nWhere $p_{\\text{AR}}$ is the probability distribution of the\nautoregressive teacher model, while $p_{\\text{NA}}$ is the probability\ndistribution of the NAT model. ${\\widehat{y}}{1:t}$ is the optimal\noutput sequence which equals to\n$G\\left( x{1:m},\\ f_{1:m};\\theta \\right)$. Such a loss is more\nfavorable towards highly peaked student output distributions than a\nstandard cross-entropy error would be.\n\nThen they trained the whole model jointly with a weighted sum\n($\\lambda = 0.25$) of the original distillation loss and two such terms,\none an expectation over the predicted fertility distribution, normalized\nwith a baseline, and the other based on the external fertility inference\nmodel:\n\n\n    \n\n\nWhere ${\\overline{f}}{1:m}$ is the average fertility computed by the\naverage decoding discussed before. The gradient with respect to the\nnon-differentiable $\\mathcal{L}{\\text{RL}}$ term can be estimated with\nREINFORCE, while term $\\mathcal{L}_{\\text{BP}}$ can be trained using\nordinary back-propagation\n\nAs seen from the below figure, using fine-tuning with a teacher model\nreally helps improving the NAT model’s performance:\n\n\n    \n\n\nResults\n\nExperiments were run on three widely used public machine translation\ncorpora: IWSLT16 En-De, WMT14\nEn-De, and WMT16\nEn-Ro. All the data are\ntokenized using byte-pair encoding (BPE). For WMT datasets, BPE\nvocabulary were shared; for IWSLT, they used separate vocabulary.\nAdditionally, they shared encoder and decoder word embeddings only in\nWMT experiments.\n\nNAT model was initialized using the teacher model. The teacher model is a\nstandard Transformer\narchitecture with base configurations on the WMT dataset and smaller set\nof hyper-parameters when trained on IWSLT dataset since it’s smaller\nthan WMT. The two sets of configuration can be seen below:\n\n\n\n    \n        \n            \n            $$N$$\n            $$d_{m}$$\n            $$d_{\\text{ff}}$$\n            $$h$$\n            $$\\epsilon_{l_{s}}$$\n            Warmup steps\n        \n    \n    \n        WMT\n        6\n        512\n        2048\n        8\n        0\n        10k\n    \n    \n        IWSLT\n        5\n        287\n        507\n        8\n        0\n        746\n    \n\n\n\nThe following table shows the BLEU scores on official test sets\n(newstest2014 for WMT En-De and newstest2016 for WMT En-Ro) and the\ndevelopment set for IWSLT. The NAT models without NPD use argmax\ndecoding and latency is computed as the time to decode a single sentence\nwithout mini-batching, averaged over the whole test set:\n\n\n    \n\n\nAs seen from the previous table, the NAT performs between 2-5 BLEU\npoints worse than its autoregressive teacher across all three datasets.\n\nThe translation latency, computed as the time to decode a single\nsentence without mini-batching, for each sentence in the IWSLT\ndevelopment set as a function of its length. The autoregressive model\nhas latency linear in the decoding length, while the latency of the NAT\nis nearly constant for typical lengths:\n\n\n    \n\n\nThey also conducted an extensive ablation study with the proposed NAT on\nthe IWSLT dataset. First, they noted that the model fails to train when\nprovided with only positional embeddings as input to the decoder.\nSecond, they found out using teacher model really helps. Third, the\nfertility-based copying improves performance by four BLEU points when\nusing ground-truth training or two when using distillation.\n\n\n    \n\n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "RNMT+",
        "url"       : "/machine-translation/RNMT_plus",
        "date"      : "26/04/2018",
        "content": "RNMT stands for “RNN N-based Neural Machine Translation models” which\nare the models that used recurrent networks in the architecture. RNMT+\nis an enhanced version of RNMT models proposed by Google AI in 2018 and\npublished in their paper: The Best of Both Worlds: Combining Recent\nAdvances in Neural Machine\nTranslation. In this paper, the\nresearchers took a step back and looked at techniques and methods that\ncontributed significantly to the success of recent models and tried to\napply them to the RNMT model resulting in RNMT+.\n\nArchitecture\n\nRNMT+ is a new architecture -as shown below- that was built using\ndifferent components borrowed from different previous architectures like\nRNN,\nConvS2S, and\nTransformers.\nThese modifications made the model significantly outperform all\nindividual architectures.\n\nRNMT+ consists of 6 bidirectional LSTM layers in the encoder. For each\nbidirectional layer, the outputs of the forward layer and the backward\nlayer are concatenated before being fed into the next layer. The decoder\nnetwork consists of 8 unidirectional LSTM layers. Residual connections\nare added to the third layer and above for both the encoder and decoder.\nRNMT+ has $1024$ LSTM nodes in all encoder and decoder layers. The input\nembedding dimension is $1024\\ $as well\n\n\n    \n\n\nInspired by the Transformer model, per-gate layer normalization is\napplied within each LSTM cell which greatly stabilizes training. No\nnon-linearity is applied to the LSTM output. A projection layer is added\nto the encoder final output. Multi-head additive attention is used\ninstead of the single-head attention other RNMT models. In addition to\nfeeding the attention context to all decoder LSTM layers, it is also fed\nto the softmax.\n\nRegarding regularization, they used the following techniques:\n\n\n  \n    Dropout: They applied dropout to both embedding layers and each\nLSTM layer output before it is added to the next layer’s input.\nAttention dropout is also applied.\n  \n  \n    Label Smoothing: They used uniform label smoothing with an\n$uncertainty = 0.1$.\n  \n  \n    L2 Regularization: L2 regularization (weight Decay) was applied\nonly for WMT’14 En→De task as the corpus is smaller and thus more\nregularization is required.\n  \n\n\nExperiments\n\nAll experiments of this paper compared RNMT+ with ConvS2S and\nTransformers. They were done on the standard WMT’14 En→Fr and En→De\ndatasets. Each sentence was tokenized using Moses tokenizer and then\nencoded using Wordpiece Model. A shared vocabulary of 32K sub-word units\nfor each source-target language pair was used. They used newstest 2014\nfor testing and a combination of newstest 2012 and newstest 2013 for\nvalidation.\n\nFor training, they used We use Adam optimizer with\n$\\beta_{1} = 0.9,\\ \\beta_{2} = 0.999,\\ \\epsilon = 10^{- 6}$ and varied\nthe learning rate according to the following schedule:\n\n\\[lr = 10^{- 4}*min\\left( 1 + \\frac{t*\\left( n - 1 \\right)}{n*p},\\ n,\\ n*\\left( 2n \\right)^{\\frac{s - n*t}{e - s}} \\right)\\]\n\nWhere $t$ is the current step, $n$ is the number of concurrent model\nreplicas used in training, $p = 500$ is the number of warm-up steps,\n$s = 600,000$ is the start step of the exponential decay, and\n$e = 1,200,000$ is the end step of the decay. Specifically, they first\nincreased the learning rate linearly during the number of warm-up steps,\nkept it a constant until the decay’s start step $s$, then exponentially\ndecayed it until the decay end step $e$, and kept it at $5*10^{- 5}$\nafter the decay ends.\n\n\n  Note:\nIn order to ensure a fair setting for comparison, they implemented all\narchitectures using the same framework, the same pre-processed data and\nevaluation methodology for all our experiments. The only thing changes\nwas the batch size: RNMT+ used $4096$ sentence pairs while ConvS2S and\nTransformer used $65536$.\n\n\nThe following table shows that RNMT+ slightly performs better than the\nTransformer-Big model on both WMT14 En→Fr (left table) and En→De (right\ntable) tasks. Note that the numbers before and after ‘$\\pm$’ are the\nmean and standard deviation of test BLEU score over an evaluation\nwindow.\n\n\n    \n\n\nThe following table shows RNMT+ performance after removing each one of\nthe most important techniques used in the model independently. The\nresults shown below were obtained by averaging BLEU scores on WMT’14 En\n→ Fr test set. An asterisk ‘*’ indicates an unstable training. From the\ntable, we can see that layer normalization is most critical\ncomponent to stabilize the training process.\n\n\n    \n\n\nHybrid Models\n\nIn an encoder-decoder architecture, a natural assumption is that the\nrole of an encoder is to build feature representations that can best\nencode the meaning of the source sequence, while a decoder should be\nable to process and interpret the representations from the encoder and,\nat the same time, track the current target history.\n\nSo, they tried to find out which model out of the two models\n(Transformer-Big &amp;amp; RNMT+) has the best encoder and the best decoder.\nThey did that by combining the encoder from a certain model and the\ndecoder from the other model. The following table shows that the\nTransformer-big architecture has the best encoder while RNMT+ has the\nbest decoder.\n\n\n    \n\n\n\n  Note:\nThey tried to add ConvS2S to the past experiment, but it took a lot of\ntime to train it and the results weren’t promising. So, they skipped it.\n\n\nBetter Encoder\n\nLastly, they tried to mix the RNMT+ encoder with the Transformer encoder\nto produce a new architecture for the encoder that combines the best\nthings from the two models that could result in richer feature\nrepresentations. They tried two different architectures (as shown\nbelow):\n\n\n    \n\n\n\n  \n    Cascaded Encoder:\nThe cascaded encoder aims at combining the representational\npower of RNNs and self-attention. The idea is to enrich a set of\nstateful representations by cascading a feature extractor with a\nfocus on vertical mapping. The transformer encoder is fine-tuned\nwhile the RNMT+ encoder is frozen.\n  \n  \n    Multi-Column Encoder:\nA multi-column encoder merges the outputs of several independent\nencoders into a single combined representation. They found out the\nbest way to merge these outputs into a single representation is by a\nsimple concatenation of individual column outputs.\n  \n\n\nAccording to the following table, the cascaded encoder\nimproves over the Transformer encoder by more than 0.5 BLEU points on the WMT’14\nEn→Fr task which suggests that the Transformer encoder is able to extract richer\nrepresentations if the input is augmented with sequential context. On the other\nhand, the multi-column encoder followed by an RNMT+ decoder achieves better\nresults on WMT’14 En→De task.\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "UT: Universal Transformer",
        "url"       : "/machine-translation/Universal_Transformer",
        "date"      : "10/07/2018",
        "content": "In “Universal Transformers”, the researchers from Google extended the\nstandard Transformer architecture to be computationally universal\n(Turing complete) using a novel, efficient flavor of parallel-in-time\nrecurrence which yields stronger results across a wider range of tasks.\nThis model was proposed by Google AI in 2018 and published in their\npaper: Universal Transformers.\nThe official code of this paper can be found on the Tensor2Tensor\nofficial GitHub repository:\ntensor2tensor/universal_transformer.py.\n\nThe Universal Transformer (UT) combines the parallelizability of the\nTransformer and the recurrent inductive bias of RNNs, which seems to\nbe better suited to a range of natural language understanding problems.\nAs shown in the following figure, the Universal Transformer is based on\nthe popular encoder-decoder architecture commonly used in most neural\nsequence-to-sequence models:\n\n\n    \n\n\nUnlike the standard transformer, Both the encoder and decoder of the\nUniversal Transformer operate by applying a recurrent neural network to\nthe representations of each of the positions of the input and output\nsequence, respectively. Now, let’s describe the encoder and decoder in\nmore detail.\n\nEncoder\n\n\n    \n\nGiven an input sequence of length $m$ and\nd-dimensional embeddings, we will have a matrix\n$H^{0} \\in \\mathbb{R}^{m \\times d}$. The UT then iteratively computes\nrepresentations $H^{t}$ at step $t$ for all $m$ positions in parallel by\napplying:\n\n\n  The first step in the encoder is to apply the position/time\nembedding. For the positions $1 \\leq i \\leq m$ and the time-step\n$1 \\leq t \\leq T$ separately for each vector-dimension\n$1 \\leq j \\leq d$, the position/time embedding\n$P^{t} \\in \\mathbb{R}^{m \\times d}$ is applied according to the\nfollowing formula:\n\n\n\\[P_{i,2j}^{t} = \\sin\\left( \\frac{i}{10000^{\\frac{2j}{d}}} \\right) + \\sin\\left( \\frac{t}{10000^{\\frac{2j}{d}}} \\right)\\]\n\n\\[P_{i,2j + 1}^{t} = \\cos\\left( \\frac{i}{10000^{\\frac{2j}{d}}} \\right) + \\cos\\left( \\frac{t}{10000^{\\frac{2j}{d}}} \\right)\\]\n\n\n  The multi-headed self-attention mechanism defined before in the\nstandard Transformer paper: Attention is all you\nneed:\n\n\n\\[\\text{MultiHead}\\left( H^{t} \\right) = Concat\\left( \\text{head}_{1},...\\text{head}_{k} \\right)\\ W^{O}\\]\n\n\\[\\text{head}_{i} = \\text{Attention}\\left( H^{t}W_{i}^{Q},H^{t}W_{i}^{K},H^{t}W_{i}^{V} \\right) = \\text{softmax}\\left\\lbrack \\frac{H^{t}W_{i}^{Q}\\left( H^{t}W_{i}^{K} \\right)^{T}}{\\sqrt{d}} \\right\\rbrack H^{t}W_{i}^{V}\\]\n\n   They map the state $H^{t}$ to queries, keys and values with affine\nprojections using learned parameter matrices\n$W_{i}^{Q},W_{i}^{K},W_{i}^{V} \\in \\mathbb{R}^{d \\times \\frac{d}{k}},W_{i}^{O} \\in \\mathbb{R}^{d \\times d}$\nwhere $d$ is the embedding size and $k$ is the number of heads.\n\n\n  The multi-headed self-attention is followed by a dropout and a\nresidual connection:\n\n\n\\[\\text{Dropout}\\left( \\text{MultiHead}\\left( H^{t - 1} + P^{t} \\right) \\right)\\]\n\n\n  Then, the dropped-out multi-headed self-attention block is followed\nby a layer normalization accompanied by a transition function as\nshown below:\n\n\n\\[H^{t} = \\text{LayerNorm}\\left( A^{t} + \\text{Dropout}\\left( \\text{Transition}\\left( A^{t} \\right) \\right) \\right)\\]\n\n\\[A^{t} = \\text{LayerNorm}\\left( \\left( H^{t - 1} + P^{t} \\right) + \\text{Dropout}\\left( \\text{MultiHead}\\left( H^{t - 1} + P^{t} \\right) \\right) \\right)\\]\n\n\n  \n    In the paper, they used one of two different $Transition()$\nfunctions:\n\n    \n      \n        Either a separable convolution as described in the Xception\npaper.\n      \n      \n        Or a fully-connected neural network that consists of a single\nReLU activation function between two affine transformations,\napplied to each row of $A^{t}$.\n      \n    \n  \n\n\nAfter $T$ steps (each updating all positions of the input sequence in parallel),\nthe final output of the Universal Transformer encoder is a matrix\n$H^{t} \\in \\mathbb{R}^{m \\times d}$ for the $m$ symbols of the input sequence.\n\nDecoder\n\n\n    \n\n\nThe decoder shares the same basic structure of the encoder. However,\nafter the self-attention function, the decoder additionally also attends\nto the final encoder representation $H^{t}$. The encoder-decoder\nMulti-head attention uses the same multihead self-attention function but\nwith queries Q obtained from projecting the decoder representations, and\nkeys and values (K and V ) obtained from projecting the encoder\nrepresentations.\n\nLike the Transformer model, the UT is autoregressive; meaning it\nproduces its output one symbol at a time. Which means that the decoder\nself-attention distributions are masked so that the model can only\nattend to positions to the left of any predicted symbol.\n\nTo generate the output, the per-symbol target distributions are obtained\nby applying an affine transformation $O \\in \\mathbb{R}^{d \\times V}$\nfrom the final decoder state to the output vocabulary size $V$ followed\nby a softmax which yields an ($m \\times V$)-dimensional output matrix\nnormalized over its rows:\n\n\\[p\\left( y_{\\text{pos}} \\middle| y_{\\left\\lbrack 1:pos - 1 \\right\\rbrack},\\ H^{T} \\right) = \\text{softmax}\\left( OH^{T} \\right)\\]\n\nNote that $H^{T}$ is the encoder’s final output, not the transpose of\n$H$.\n\nDynamic Halting\n\nIn sequence processing systems, certain symbols (e.g. some words or\nphonemes) are usually more ambiguous than others. It is therefore\nreasonable to allocate more processing resources to these more ambiguous\nsymbol. Adaptive Computation Time\n(ACT) is mechanism implemented to\ndo exactly that in standard RNNs.\n\nInspired by it, the researchers of this paper added a dynamic ACT\nhalting mechanism to each symbol of the encoder. Once the per-symbol\nrecurrent block halts, its state is simply copied to the next step until\nall blocks halt, or we reach a maximum number of steps.\n\nThis dynamic halting was implemented in TensorFlow as follows. In each\nstep of the UT with dynamic halting, we are given the halting\nprobabilities, remainders, number of updates up to that point, and the\nprevious state (all initialized as zeros), as well as a scalar threshold\nbetween 0 and 1 (a hyper-parameter).\n\ndef should_continue(u0, u1, halting_probability, u2, n_updates, u3):\n    return tf.reduce_any(\n        tf.logical_and(\n            tf.less(halting_probability, threshold),\n            tf.less(n_updates, max_steps)))\n\n# Do while loop until above is false\n(_, _, _, remainder, n_updates, new_state) = tf.while_loop(\n    should_continue,\n    ut_with_dynamic_halting,\n    (state, step, halting_probability, remainders, n_updates, previous_state))\n\nThen, they compute the new state for each position and calculate the new\nper-position halting probabilities based on the state for each position.\nThe UT then decides to halt for some positions that crossed the\nthreshold, and updates the state of other positions until the model\nhalts for all positions or reaches a predefined maximum number of steps:\n\ndef ut_with_dynamic_halting(state, step, halting_probability, remainders, n_updates, previous_state):\n    # calculate the probabilities based on the state\n    p = common_layers.dense(state, 1, activation=tf.nn.sigmoid, use_bias=True)\n\n    # mask of inputs which have not halted at this step\n    still_running = tf.cast(tf.less(halting_probability, 1.0) tf.float32)\n\n    # mask of inputs which halted at this step\n    new_halted = tf.cast(tf.greater(halting_probability + p * still_runing, threshold) tf.float32) * still_running\n\n    # mask of inputs which have not halted and didn\\&#39;t halt at this step\n    still_running = tf.cast(tf.less_equal(halting_probability + p * still_runing, threshold) tf.float32) * still_running\n\n    # Add the halting probability for this step to the halting\n    # probabilities for those inputs which haven\\&#39;t halted yet\n    halting_probability += p * still_running\n\n    # compute remainders for the inputs which halted at this step\n    remainders += new_halted * (1 - halting_probability)\n\n    # compute remainders for the inputs which halted at this step\n    halting_probability += new_halted * remainders\n\n    # increment n_updates for all inputs which are still running\n    n_updates += still_running + new_halted\n\n    # compute the weight to be applied to the new state and output\n    # 0 when the input has already halted,\n    # p when the input hasn\\&#39;t halted yet,\n    # the remainders when it halted this step.\n    update_weights = tf.expand_dims(p * still_running + new_halted * remainders, -1)\n\n    # apply transformation to the state\n    transformed_state = transition(self_attention(state))\n\n    # Interpolate transformed and previous states for non-halted inputs\n    new_state = (transformed_state * updated_weights) + ( previous_state * (1 - update_weights))\n\n    # increment steps\n    steps += 1\n\n    return (transformed_state, step, halting_probability, remainders, n_updates, new_state)\n\n\nMachine Translation\n\nIn the paper, they trained a Universal Transform on the WMT 2014\nEnglish-German translation task using the same setup as in the standard\ntransformer in order to evaluate its performance on a large-scale\nsequence-to-sequence task. The universal transformer improves by 0.9\nBLEU over the standard Transformer and 0.5 BLEU over a Weighted\nTransformer with approximately the same number of parameters:\n\n\n    \n\n\n\n  Note:\nThe $Transition()$ function used here is a fully-connected recurrent\ntransition function. Also, the dynamic ACT halting wasn’t used.\n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Context-Aware Transformer",
        "url"       : "/machine-translation/Context-Aware_Transformer",
        "date"      : "15/07/2018",
        "content": "Context-Aware Transformer is a modified version of the\nTransformer\nmodel which is designed to allow the flow of information from the\ncontext to the decoder to provide better and more coherent results.\nContext-Aware Transformer was proposed by Yandex, University of\nEdinburgh, and University of Amsterdam in 2018 and published in their\npaper: Context-Aware Neural Machine Translation Learns Anaphora\nResolution.\n\n\n  Note:\nThe paper’s name holds the term “anaphora” which is defined as the\n“repetition of a word or phrase at the beginning of successive clauses”.\nWe can see Anaphora clear in the words of Langston\nHughes in his poem:\nLet America be America\nagain\nwhere he said: “I am the farmer, bondsman to the soil. I\nam the worker sold to the machine. I am the Negro,\nservant to you all…”.\n\n\nStandard Transformer\nencoders process sentences in isolation and ignore context information\nthat could prevent mistakes and improve translation coherence. On the\nother hand, Context-Aware Transformer encoders process sentences along\nwith their contexts independently, and then a single attention layer, in\na combination with a gating function, is used to produce a context-aware\nrepresentation of the source sentence.\n\nThe following figure shows a simple comparison between standard\nTransformer model and context-aware Transformer. As you can see from the\nfigure, context-aware encoders take two inputs: the sentence and its\ncontext while the decoder remains unchanged.\n\n\n    \n\n\nContext-Aware Encoder\n\nAs shown in the following figure, the\ncontext-aware encoder consists of three different blocks:\n\n\n    \n\n\n\n  \n    Source Encoder:\nThe source encoder is composed of a stack of $N$ layers. The first\n$N - 1$ layers are identical and represent the original layers of\nthe standard Transformer’s encoder while the last layer incorporates\ncontextual information.\n  \n  \n    Context Encoder:\nThe context encoder is composed of a stack of $N$ layers. The first\n$N - 1$ layers are identical and represent the original layers of\nthe standard Transformer’s encoder while the last layer incorporates\ncontextual information.\n  \n  \n    Gated Contextual Attention:\nThe output of the source encoder’s attention\n$c_{i}^{\\left( s_ attn \\right)}$ and the output of the context\nencoder’s attention $c_{i}^{\\left( c_ attn \\right)}$ are combined\nvia a gated sum with a sigmoid function $\\sigma$ for non-linearity:\n  \n\n\n\\[g_{i} = \\sigma\\left( W_{g}\\left\\lbrack c_{i}^{\\left( \\text{s\\_attn} \\right)};\\ c_{i}^{\\left( \\text{c\\_attn} \\right)} \\right\\rbrack + b_{g} \\right)\\]\n\n\\[c_{i} = g_{i} \\odot c_{i}^{\\left( \\text{s\\_attn} \\right)} + \\left( 1 - g_{i} \\right) \\odot c_{i}^{\\left( \\text{c\\_attn} \\right)}\\]\n\n\n  Note:\nThe parameters of the first $N - 1$ layers are shared between both\nencoders.\n\n\nExperiments\n\nIn the paper, they used\nOpenSubtitles2018 corpus for\nEnglish and Russian. The data after cleaning was around 2 million\ninstances for training; and for validation and testing, they randomly\nselect two subsets of 10000 instances from movies not encountered in\ntraining. The whole data used in this paper is available in the\nfollowing link.\n\nSentences were encoded using byte-pair encoding with source and target\nvocabularies of about 32k tokens. They used the same parameters and\noptimizer as in the original Transformer paper.\n\nThe following table shows the BLEU score of multiple Transformer models;\nthe first two are standard Transformer models while the other three are\ncontext-aware. From the table, we can see that the best model is the one\nusing a context encoder for the previous sentence.\n\n\n    \n\n\nIn order to verify these results, they evaluated the context encoder\n(previous sentence) on the same test set with shuffled context\nsentences. And they found out that its performance dropped\nsignificantly. This confirms that the model does rely on context\ninformation to achieve the improvement in translation.\n\nTo check the top words depending on the\ncontext, they analyzed the distribution of attention to context for\nindividual source words in the test set to see for which words the model\ndepends most on contextual history. Then, they averaged the attention\nscore and the word position. Then, sorted them according to the\nattention score. The following table contains the top-10 words:\n\n\n    \n\n\nAn interesting finding is that contextual attention is high for the\ntranslation of “it”, “yours”, “ones”, “you” and “I”, which are indeed\nvery ambiguous out-of-context when translating into Russian. Also, the\n“yes”, “yeah”, and “well” are ambiguous since they are used in\nsentence-initial position.\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Translationese Effect",
        "url"       : "/machine-translation/translationese_effect",
        "date"      : "21/08/2018",
        "content": "Translationese is a common term that refers to to translated texts. The\nfundamental law of translation states that “phenomena pertaining to the\nmake-up of the source text tend to be transferred to the target text”\nwhich means that translated texts tend to be simpler and retain some\ncharacteristics from the source language.\n\nA 2009-published paper: Automatic detection of translated text and its\nimpact on machine translation has\nfound out that an MT system performs better when trained on parallel\ndata whose source side is original and whose target side is\ntranslationese than if it was trained on the opposite. In other words,\nif there is a dataset published that contains English→French\ntranslations, it will be better if we use this dataset that direction\nand not the opposite (French→English).\n\nAnd this 2018-published paper: The Effect of Translationese in Machine\nTranslation Test Sets found out\nthat the effect of translationese tends to be high with low-resource\nlanguage which could inflate the expectations in terms of translation\nquality for these languages. You can even reproduce their results by\ntaking a look at their official GitHub repository:\ntranslationese.\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "The Evolved Transformer",
        "url"       : "/machine-translation/Evolved_Transformer",
        "date"      : "30/01/2019",
        "content": "The Evolved Transformer (ET) ia an evolved version of the Transformer\narchitecture created by applying Neural Architecture Search (NAS)\nalgorithm over the standard architecture.The Evolved Transformer was\nproposed by Google Brain in 2019 and published in their paper with the\nsame name: The Evolved Transformer.\nThe official implementation of the Evolved Transformer can be found on the\nTensor2Tensor official GitHub repository:\ntensor2tensor/universal_transformer.py.\n\nWe first construct a large search space inspired by the recent advances\nin feed-forward sequence models and then run evolutionary architecture\nsearch with warm starting by seeding our initial population with the\nTransformer. To directly search on the computationally expensive WMT\n2014 EnglishGerman translation task, we develop the Progressive Dynamic\nHurdles method, which allows us to dynamically allocate more resources\nto more promising candidate models.\n\nThe goal of this work is to examine the use of neural architecture\nsearch methods to design better feed-forward architectures for seq2seq\ntasks. Specifically, we apply tournament selection architecture search\nand warm start it with the Transformer, considered to be the\nstate-of-art and widely-used, to evolve a better and more efficient\narchitecture. To achieve this, we construct a search space that reflects\nthe recent advances in feed-forward seq2seq models and develop a method\ncalled Progressive Dynamic Hurdles (PDH) that allows us to perform our\nsearch directly on the computationally demanding WMT 2014 EnglishGerman\n(En-De) translation task.\n\nET out-performed the human-designed transformer on four well-established\nlanguage tasks: WMT 2014 English-German, WMT 2014 English-French, WMT\n2014 English-Czech and LM1B:\n\n\n    \n\n\nNAS\n\nNAS stands for “Network Architecture Search” which is an evolution-based\narchitecture search\n\nTO BE CONTINUED!\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "SB-NMT: Synchronous Bidirectional NMT",
        "url"       : "/machine-translation/SB-NMT",
        "date"      : "13/05/2019",
        "content": "SB NMT stands for “Synchronous Bi-directional Neural Machine\nTranslation” which is a model proposed by the the University of Chinese\nAcademy of Sciences in 2019 and published in their paper under the same\nname: Synchronous Bidirectional Neural Machine\nTranslation. The official code\nfor this paper can be found on the following GitHub repository:\nsb-nmt.\n\nSB-NMT architecture is the same as the standard Transformer with the\nexception that the decoder has a Synchronous Bi-directional\nAttention sub-layer instead of the Multi-head Self-attention one\nwhich enable the decoder to predict its outputs using left-to-right and\nright-to-left decoding simultaneously and interactively, in order to\nleverage both of the history and future information at the same time.\n\n\n    \n\n\nTO BE CONTINUED…\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Robust NMT",
        "url"       : "/machine-translation/Robust_NMT",
        "date"      : "06/06/2019",
        "content": "Neural machine translation (NMT) often suffers from the vulnerability to\nnoisy perturbation in the input. Google AI has proposed an approach to\nimproving the robustness of NMT models called AdvGen published in 2019\nin their paper: Robust Neural Machine Translation with Doubly\nAdversarial Inputs. AdvGen\nconsists of two parts:\n\n\n  \n    Attack the translation model with adversarial source examples.\n  \n  \n    Defend the translation model with adversarial target inputs to\nimprove its robustness against the adversarial source inputs.\n\n    This approach achieves significant improvements over the previous\nstate-of-the-art Transformer model on two common translation\nbenchmarks: Chinese-English and English-German which substantiates\nthat the proposed model improves the generalization performance over\nthe clean benchmark datasets.\n  \n\n\nAdvGen\n\nAdvGen is a gradient-based approach to construct adversarial examples\nand use these examples to both attack as well as defend the NMT model.\nThe intuition behind it is that an ideal model would generate similar\ntranslation results for similar input sentences despite any small\ndifference caused by perturbations.\n\nAttack Source Input\n\nGiven a parallel sentence pair $(x,\\ y)$, For the original input $x$, we\ninduce a possible adversarial word ${x’}{i}$ for the token $x{i}$ in\nx:\n\n\\[{x&#39;}_{i} = \\underset{x \\in \\mathcal{V}_{x_{i}}}{\\arg\\max}\\left( \\text{sim}\\left( e\\left( x \\right) - e\\left( x_{i} \\right) \\right),\\ g_{x_{i}} \\right)\\]\n\nWhere:\n\n\n  \n    $e\\left( x_{i} \\right)$ is the embedding of token $x_{i}$.\n  \n  \n    $g_{x_{i}}$ is a gradient vector with respect to\n$e\\left( x_{i} \\right)$:\n  \n\n\n\\[g_{x_{i}} = \\frac{\\partial}{\\partial e\\left( x_{i} \\right)} - logP\\left( y \\middle| x;\\ \\theta \\right)\\]\n\n\n  \n    $sim( \\cdot ,\\  \\cdot )$ denotes the similarity function by\ncalculating the cosine distance between two vectors.\n  \n  \n    $\\mathcal{V}_{x_{i}}$ is a subset of the vocabulary for\nthe source language that is specific for each token $x_{i}$\nis defined as the n most probable words among the top n scores calculated\nby $a$ is a bidirectional language model for the source language\n$P_{\\text{lm}}$:\n  \n\n\n\\[\\mathcal{V}_{x_{i}} = top\\_ n\\left( Q\\left( x_{i},\\ x \\right) \\right);\\ \\ \\ \\ Q\\left( x_{i},\\ x \\right) = P_{\\text{lm}}\\left( x \\middle| x_{&amp;lt; i},\\ x_{&amp;gt; i};\\ \\theta_{\\text{lm}}^{x} \\right)\\]\n\nMore formally, the AdvGen function can be written as an algorithm like so:\n\n\n    \n\n\nDefense Target Input\n\nAfter generating an adversarial example $x’$, we treat $(x’,\\ y)$ as a\nnew training data point to improve the model’s robustness. These\nadversarial examples in the source tend to introduce errors which may\naccumulate and cause drastic changes to the decoder prediction. To\ndefend the model from errors in the decoder predictions, AdvGen\ngenerates an adversarial target input.\n\nFormally, let $z$ be the decoder input for the sentence pair $(x,\\ y)$.\nUsing the same AdvGen function, we will generate an adversarial target\ninput $z’$ from $z$ by:\n\n\\[z&#39; = \\text{AdvGen}\\left( z,\\ Q_{\\text{trg}},\\ D_{\\text{trg}},\\  - \\log P\\left( y \\middle| x&#39; \\right) \\right)\\]\n\nNote that:\n\n\n  \n    For the target, the translation loss in the attack part\n$- logP\\left( y \\middle| x;\\ \\theta \\right)$ is replaced by\n$- logP\\left( y \\middle| x’ \\right)$.\n  \n  \n    $Q_{\\text{trg}}$ is the likelihood for selecting the target\nword candidate set $\\mathcal{V}_{z}$. To compute it, we combine\nthe NMT model prediction $P_{\\text{nmt}}$ with a bidirectional\nlanguage model $P_{\\text{lm}}$ for the target language with a\ntunable hyper-parameter $\\lambda$ that balances the importance\nbetween two models as follow:\n  \n\n\n\\[Q_{\\text{trg}}\\left( z_{i},\\ z \\right) = \\lambda P_{\\text{lm}}\\left( z \\middle| z_{&amp;lt; i},\\ z_{&amp;gt; i};\\ \\theta_{\\text{lm}}^{y} \\right) + \\left( 1 - \\lambda \\right)P_{\\text{nmt}}\\left( z \\middle| z_{&amp;lt; i},\\ x&#39;;\\ \\theta_{\\text{nmt}} \\right)\\]\n\n\n  $D_{\\text{trg}}$ is a distribution for sampling positions for the\ntarget input. Different from the uniform distribution used in the\nsource, in the target sentence we want to change those relevant\nwords influenced by the perturbed words in the source input. To do\nso, we use the attention matrix $M$ learned in the NMT model,\nobtained at the current mini-batch, to compute the distribution over\n$(x,\\ y,\\ x’)$ by:\n\n\n\\[P\\left( j \\right) = \\frac{\\sum_{i}^{}M_{\\text{ij}}\\ \\delta\\left( x_{i},\\ {x&#39;}_{i} \\right)}{\\sum_{k}^{}{\\sum_{i}^{}M_{\\text{ik}}\\ \\delta\\left( x_{i},\\ {x&#39;}_{i} \\right)}},\\ \\ \\ j \\in \\left\\lbrack 1,\\ ...,\\ \\left| y \\right| \\right\\rbrack\\]\n\nWhere $M_{\\text{ij}}$ is the attention score between\n$x_{i}$ and $y_{j}$; and\n$\\delta\\left( x_{i},\\ {x’}_{i} \\right)$ is an indicator function\nthat yields 1 if $x_{i} \\neq {x’}_{i}$ and 0 otherwise.\n\nRobustness Loss\n\n\n    \n\n\nThis algorithm details the entire procedure to\ncalculate the robustness loss for a parallel sentence pair $(x,\\ y)$.\nThis algorithm takes at most a 20% time overhead compared to the\nstandard Transformer model.\n\nAccordingly, we compute the robustness loss on $S$ as:\n\n\\[\\mathcal{L}_{\\text{robust}}\\left( \\theta_{\\text{nmt}} \\right) = \\frac{1}{\\left| S \\right|}\\sum_{\\left( x,y \\right) \\in S}^{}{- \\log P\\left( y \\middle| x&#39;,\\ z&#39;;\\ \\theta_{\\text{nmt}} \\right)}\\]\n\nAnd the final loss will be a combination of four loss functions:\n\n\\[\\mathcal{L}\\left( \\theta_{\\text{nmt}},\\ \\theta_{\\text{lm}}^{x},\\ \\theta_{\\text{lm}}^{y} \\right) = \\mathcal{L}_{\\text{clean}}\\left( \\theta_{\\text{nmt}} \\right) + \\mathcal{L}_{\\text{lm}}\\left( \\theta_{\\text{lm}}^{x} \\right) + \\mathcal{L}_{\\text{robust}}\\left( \\theta_{\\text{nmt}} \\right) + \\mathcal{L}_{\\text{lm}}\\left( \\theta_{\\text{lm}}^{y} \\right)\\]\n\nWhere:\n\n\n  \n    $\\mathcal{L}_{\\text{lm}}\\left( \\theta_{\\text{lm}}^{x} \\right)$\nand $\\mathcal{L}_{\\text{lm}}\\left( \\theta_{\\text{lm}}^{y} \\right)$\nare loss functions for source and target bidirectional language models,\nrespectively.\n  \n  \n    $\\mathcal{L}_{\\text{clean}}\\left( \\theta_{\\text{nmt}} \\right)$\nis the loss function for the NMT model.\n  \n  \n    $\\mathcal{L}_{\\text{robust}}\\left( \\theta_{\\text{nmt}} \\right)$\nis the robustness loss.\n  \n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Tagged Back-Translation",
        "url"       : "/machine-translation/Tagged_BT",
        "date"      : "15/06/2019",
        "content": "Tagged BT stands for “Tagged Back-Translation” which is a simpler\nalternative Noised\nBT,\nconsisting of tagging back-translated source sentences with an extra token.\nTagged BT was proposed in 2019 by Google and published in this paper: Tagged\nBack-translation.\nTagged BT results on WMT outperform noised BT in English-Romanian and match\nperformance on English-German:\n\n\n    \n\n\nThis work hypothesizes that Noised BT outperformed Back-Translation\nbecause Noised BT was signaling the model that the source side is\nback-translated, allowing it to treat the synthetic parallel data\ndifferently than the natural bitext. To support this hypothesis, they\nprepended each sentence of the synthetic data (which comes from\nBack-translation) with a reserved token\n$\\left\\langle \\text{BT} \\right\\rangle$ as shown below:\n\n\n    \n\n\nIterative Tagged BT\n\nAn Iteration-1 Tagged BT model is a Tagged BT model where the\nback-translations were generated by a model trained only on bitext. An\niteration-2 Tagged BT model is a Tagged BT model where the\nback-translations were generated by an Iteration-1 Tagged BT model. And\nso on. In a more general form, an Iteration-k BT model is trained on BT\ndata generated by an Iteration-(k-1) BT model, for $k &amp;gt; 1$.\n\nExperiments\n\nFor bitext data, experiments were performed on WMT18 En-De, WMT16 En-Ro,\nand WMT15 En-Fr bitext. They filtered out empty sentences and sentences\nlonger than 250 subwords. They removed pairs whose whitespace-tokenized\nlength ratio is greater than 2.\n\nFor monolingual data, WMT Newscrawl data (2007-2017 for De, 2016 for Ro,\n2007-2013 for En, and 2007-2014 for Fr) were used. They filtered\nsentences with more than 70 tokens or 500 characters. Furthermore, after\nback-translation, They removed any sentence pairs where the\nback-translated source is longer than 75 tokens or 550 characters.\n\nThe data stats used in the experiments can be found in the following\ntable;\n\n\n\n    \n        \n            \n            En-De\n            En-Ro\n            Ro-En\n            En-Fr\n        \n    \n    \n        Bitext Size\n        5 M\n        0.6 M\n        0.6 M\n        41 M\n    \n    \n        Monolingual Size\n        216.5 M\n        2.2 M\n        149.9 M\n        39 M\n    \n\n\n\nThey used the standard\nTransformer\nimplemented in the\nlingvo\nframework; they used the transformer-base setup for En-Ro data and the\ntransformer-big setup for both En-De and En-Fr. Both use a vocabulary of\n32k subword units.\n\nIn this paper, they compared Tagged BT model with standard\nBack-Translation, Noised BT model, 3-tokens constrained permutation BT\n(P3BT), and Noised Tagged BT model:\n\n\n    \n\n\nThe following table shows the BLEU score on Newstest En-De dataset with\nback-translated data either sampled down to 24M or using the full set of\n216M sentence pairs. As shown, Tagged BT is on par with or slightly\nsuperior to their Noised BT, out-performing it on four test sets and\nunder-performing it on three:\n\n\n    \n\n\nThe following table shows the BLEU score of En-Fr test sets which is a\nmuch higher-resource language pair than En-De. Same as before, Tagged BT\nout-performs all other methods, beating Noised BT by an average of +0.3\nBLEU over all test sets.\n\n\n    \n\n\nSame applies for En-Ro (left table) and Ro-En (right table):\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Transformer + Noisy Channel",
        "url"       : "/machine-translation/Transformer_+_NoisyChannel",
        "date"      : "15/08/2019",
        "content": "Transformer model directly estimates the posterior probability of a\ntarget sequence $y$ given a source sequence $x$. The Noisy Channel model\noperates in the reverse direction. It estimates the likelihood\nprobability $p\\left( x \\middle| y \\right)$ with the help of a language\nmodel probability $p\\left( y \\right)$. To do so, the Noisy channel model\napplies the Naive Bayes Rule:\n\n\\[p\\left( y \\middle| x \\right) = \\frac{p\\left( x \\middle| y \\right)\\text{.p}\\left( y \\right)}{p\\left( x \\right)}\\]\n\nThe model responsible for calculating $p\\left( y \\middle| x \\right)$ is\ncalled “Direct Model”, the model responsible for calculating\n$p\\left( x \\middle| y \\right)$ is called “Channel Model”. Modeling\n$p(x)$ is ignored since it is constant for all $y$.\n\nThe noisy channel approach was widely used in statistical machine\ntranslation. However, in this paper: Simple and Effective Noisy Channel\nModeling for Neural Machine\nTranslation published by FAIR in\n2019 they decided to use the standard Transformer architecture as a\nchannel model. The official code for this paper can be found in the\nfairseq official GitHub repository:\nfairseq/noisychannel.\n\nIn this paper, a standard Transformer model is used as the channel\nmodel, they basically trained the model to translate the target sentence\nto the source sentence. And they trained another Transformer model as\nthe direct model. And they also trained a transformer language model.\n\nExperiments\n\nUsing the English-German WMT’17 dataset for training, news2016 for\nvalidation and news2017 for testing, they used:\n\n\n  \n    Language Model:They trained two big Transformer language models with\nadaptive input\nrepresentations\nwith 12 blocks. one on the German newscrawl data distributed by\nWMT’18 comprising 260M sentences and another one on the English\nnewscrawl data comprising 193M sentences. Both use a BPE vocabulary\nof 32K tokens.\n  \n  \n    Direct Model:\nThey use big Transformers where the encoder and decoder embeddings\nare not shared between them since the source and target vocabularies\nwere learned separately.\n  \n  \n    Channel Model:\nThey trained Transformer models to translate from the target to the\nsource (En-De).\n  \n\n\nFor comparison, they tried the following configurations:\n\n\n  \n    DIR: just the direct model.\n  \n  \n    DIR ENS: an ensemble of two direct models.\n  \n  \n    DIR+LM: a direct model + a language Model.\n  \n  \n    DIR+RL: a direct model + a right-to-left seq2seq model.\n  \n  \n    DIR+RL+LM: a direct model + a right-to-left seq2seq model + a\nlanguage model.\n  \n  \n    CH+DIR: a channel model + a direct model.\n  \n  \n    CH+DIR+LM: a channel model + a direct model + a language model.\n  \n\n\nOnline Decoding\n\nTo perform decoding in the noisy channel model, we will need to perform\na two-step beam search. For beam size $k_{1}$ in the channel model, we\nwill collect $k_{2}$ possible next words extensions from the direct\nmodel for each beam. The, we will score the resulting\n$k_{1} \\times k_{2}$ according to the following equation:\n\n\\[\\frac{1}{t}\\log p\\left( y \\middle| x \\right) + \\frac{\\lambda}{s}\\left( \\log p\\left( x \\middle| y \\right) + \\log p\\left( y \\right) \\right)\\]\n\nWhere $t$ is the length of the target prefix $y$, $s$ is the source\nsentence length and $\\lambda$ is a tunable weight.\n\nIn online decoding, you don’t have the whole target sentence. In this\ncase, you can’t use the right-to-left seq2seq model since it doesn’t\nknow how the target sentence will be like. The following table\nsummarizes the BLEU score over news2016 and news2017 en-de datasets:\n\n\n    \n\n\nThe previous results were produces using $k_{1} = 5,\\ k_{2} = 10$.\n\nRe-ranking\n\nIn re-ranking, you have access to the full target sentence. The purpose\nis just re-rank them. The following table shows the re-ranking BLEU with\ndifferent n-best list sizes on news2016 of WMT De-En. As we can see, the\nnoisy channel model configuration obtained the highest scores:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Align &amp; Translate with Transformers",
        "url"       : "/machine-translation/Align_and_Translate",
        "date"      : "04/09/2019",
        "content": "In this part, we are going to take a deep look into this paper: Jointly\nLearning to Align and Translate with Transformer\nModels which was published by\nApple Inc. in 2019. The official code for this paper can be found in the\nofficial Fairseq GitHub repository:\nfairseq/joint_alignment_translation.\n\nSince translation and alignment tasks are very closely related, the\npublishers of this paper presented an approach to train a Transformer\nmodel to produce both accurate translations and alignments. The\nalignments are extracted from the attention probabilities learned during\ntraining. This approach produced competitive results compared to other\nstatistical alignment tools such as GIZA++ without sacrificing\ntranslation accuracy.\n\nAttention Matrix\n\n\n    \n\nIn this part, we are going to recap how the encoder-decoder multi-head\nattention sub-layer works and how the attention matrix is represented.\nLet $d_{\\text{emb}},\\ d_{k},\\ d_{v},\\ H$ denote the embedding dimension,\ndimensions of the key and value projections and number of heads,\nrespectively.\n\nThe multi-head attention between a source sentence of $M$ tokens and a\ntarget sentence of $N$ tokens is calculated via the following equation:\n\n\\[\\text{MultiHead}\\left( Q,\\ K,\\ V \\right) = Concat\\left( A_{1},\\ ...\\ A_{H} \\right)W^{O}\\]\n\n\\[A_{i} = Attention\\left( QW_{i}^{Q},\\ KW_{i}^{K},\\ VW_{i}^{V} \\right) = \\text{softmax}\\left( \\frac{QW_{i}^{Q}\\left( KW_{i}^{K} \\right)^{T}}{\\sqrt{d_{k}}} \\right)VW_{i}^{V}\\]\n\nWhere:\n\n\n  \n    $A_{i} \\in \\mathbb{R}^{M \\times N}$ is the Attention matrix of the\n$i^{th}$ head out of $H$ heads.\n  \n  \n    $W_{i}^{Q} \\in \\mathbb{R}^{d_{\\text{emb}} \\times d_{k}},\\ W_{i}^{K} \\in \\mathbb{R}^{d_{\\text{emb}} \\times d_{k}},\\ W_{i}^{V} \\in \\mathbb{R}^{d_{\\text{emb}} \\times d_{v}},\\ W^{O} \\in \\mathbb{R}^{Nd_{v} \\times d_{\\text{emb}}}$\nare projections matrices of the $i^{th}$ head. All of them are learned\nparameters.\n\n    Each alignment matrix $A_{i}$ gives a probability distribution over\nsource/target pair. This distribution is then converted to a\ndiscrete alignment by aligning each target word to the corresponding\nsource word with the highest attention probability.\n  \n\n\nLayer Average Baseline\n\nAs we can see from the previous sections, there are multiple heads and\nmultiple layers in the transformer architecture. Which alignment should\nbe considered the correct alignment of the input pair?\n\nIn the paper, they found out that the best results come from\naveraging all attention heads of the second-to-last layer out\nof the six decoder layers they used. They refer to this as “the layer\naverage baseline”.\n\nMulti-task Learning\n\nMulti-task learning is a subfield of machine learning in which multiple\nlearning tasks are solved at the same time, while exploiting\ncommonalities and differences across tasks. In this paper, the\npublishers exploited the correlation between Translation and Alignment\ntasks to create a multi-objective loss function $\\mathcal{L}$:\n\n\\[\\mathcal{L} = \\mathcal{L}_{t} + \\lambda\\mathcal{L}_{a}\\left( A \\right)\\]\n\nWhere:\n\n\n  \n    $\\lambda$ is a hyper-parameters to weigh the importance of these two\nloss functions.\n  \n  \n    $\\mathcal{L}_{t}$ is the translation loss function given a source sentence $s = \\left[ s_{1}\\text{…}s_{M} \\right]$ and a target sentence $t = \\left[ t_{1}\\text{…}t_{N} \\right]$:\n  \n\n\n\\[\\mathcal{L}_{t} = - \\frac{1}{N}\\sum_{i = 1}^{N}{\\log\\left( p\\left( t_{i} \\middle| s,\\ t_{&amp;lt; i} \\right) \\right)}\\]\n\n\n  $\\mathcal{L}_{a}$ is the alignment loss:\n\n\n\\[\\mathcal{L}_{a}\\left( A \\right) = - \\frac{1}{N}\\sum_{i = 1}^{N}{\\sum_{j = 1}^{M}G_{i,j}^{p}\\text{.}\\log\\left( A_{i,j} \\right)}\\]\n\nThe alignment loss is calculated in a supervised manner using the\nlayer-average baseline as the true alignment. First, they converted the\ntrue alignments into a probability distribution. Let $G_{N \\times M}$\ndenote a 0-1 matrix such that $G_{i,j} = 1$ if the $j^{th}$ source token is\naligned to the $i^{th}$ target token. Then, they normalized the rows of $G$\nto get matrix $G^{p}$.\n\nCalculating the translation loss $\\mathcal{L}_{t}$ needs a masked target\nwhile calculating the alignment loss $\\mathcal{L}_{a}$ needs the target\nwithout masking. That’s why the loss function is implemented by\nexecuting two forward passes of the decoder model: One with the masking\nof the future target tokens for computing the translation loss\n$\\mathcal{L}_{t}$ and the other one with no masking for computing the\nalignment loss $\\mathcal{L}_{a}$.\n\nResults\n\nTo achieve state-of-the-art results, they change to the big transformer\nconfiguration with an embedding size of 1024 and 16 attention heads, 6\nlayers in the encoder and decoder. The total number of parameters is\n213M. They trained the transformer with a batch size of 7168 tokens on\n64 Volta GPUs for 30k updates and apply a learning rate of $1e^{-3}$,\n$\\beta_{1} = 0.9$, $\\beta_{2} = 0.98$. The dropout probability is set to\n$0.3$.\n\nAs we can see from this table, the multi-task learning with full context\n(knowing of all target words) and with alignment generated from GIZA++\ntoolkit achieved the least AER while maintaining the translation BLEU\nscore on two different language-pairs.\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "CMLM Transformer",
        "url"       : "/machine-translation/CMLM_Transformer",
        "date"      : "04/09/2019",
        "content": "CMLM stands for “Conditional Masked Language Modeling” Transformer which\nis an encoder-decoder Transformer\narchitecture trained with a masked\nlanguage modeling (MLM) training objective and uses “masked-predict”\nalgorithm for decoding. This model was proposed by FAIR in 2019 and\npublished in their paper: Mask-Predict: Parallel Decoding of\nConditional Masked Language\nModels. The official code for\nthis paper can be found on Facebook Research’s official GitHub\nrepository:\nfacebookresearch/Mask-Predict.\n\nUnlike most machine translation systems which generate translations\nautoregressively from left to right, the CMLM Transformer uses a\nnon-autoregressive decoding algorithm called “masked-predict” that\niteratively decodes in linear time. CMLM Transformer is the same as the\nstandard Transformer\nwith one change in the decoder; the masking of the self-attention in the\ndecoder was removed to make the decoder attend to both left and right\ncontexts when generating the output translation.\n\n\n  Note:\nThe difference between MLM and CMLM is that MLM predicts masked tokens given\nthe remaining sentence while CMLM predicts masked tokens given the source\nsentence + the remaining of target sentence.\n\n\nCMLM Training\n\nA conditional masked language modeling (CMLM) predicts a set of target\ntokens $Y_{\\text{mask}}$ given a source text $X$ and part of the target\ntext $Y_{\\text{obs}}$. It makes the strong assumption that the tokens\n$Y_{\\text{mask}}$ are conditionally independent of each other (given $X$\nand $Y_{\\text{obs}}$). During training, CMLM is done as follows:\n\n\n  \n    A number of tokens that should be masked is chosen randomly from a\nuniform distribution between one and the sequence’s length.\n  \n  \n    Then, that number of tokens get masked $Y_{\\text{mask}}$. Following\nBERT, masking\nis done by replacing the token with a special\n$\\left\\lbrack \\text{MASK} \\right\\rbrack$ token.\n  \n  \n    CMLM is optimized using cross-entropy loss over every token in\n$Y_{\\text{mask}}$. This can be done in parallel, since the model\nassumes that the tokens in $Y_{\\text{mask}}$ are conditionally\nindependent of each other.\n  \n\n\nIn traditional left-to-right machine translation, where the target\nlength is determined by the generation of the special end of sentence\ntoken $\\left\\lbrack \\text{EOS} \\right\\rbrack$. However, for CMLMs to\npredict the entire sequence in parallel, they must know its length in\nadvance. Here, they added a special\n$\\left\\lbrack \\text{LENGTH} \\right\\rbrack$ token to the encoder and then\ntrain the model to predict the length of the target sequence. Its loss\nis added to the cross-entropy loss.\n\nMask-Predict Decoding\n\nMask-predict is a new highly-parallel decoding algorithm that predicts\nany subset of the target words conditioned on the input text $X$ and a\npartially masked target translation $Y_{\\text{mask}}$. Decoding starts\nwith a completely masked target text, then the model predicts all of the\nwords in parallel, then repeatedly masks out and regenerates the subset\nof words that the model is least confident about depending on the other\nhigh-confidence predictions. After a few cycles, the model starts to\nproduce high-quality translations.\n\nMore formally; given the target sequence’s length N, we define two\nvariables: the target sequence $\\left( y_{1},\\ …y_{N} \\right)$ and the\nprobability of each token $\\left( p_{1},\\ …p_{N} \\right)$. The\nalgorithm runs for a predetermined number of iterations $T$, which is\neither a constant or a simple function of $N$. At each iteration, we\nperform a mask operation, followed by predict.\n\n\n  Mask:\nFor the first iteration ($t = 0$), we mask all the tokens. For later\niterations, we mask the $n$ tokens with the lowest probability\nscores. The number of masked tokens $n$ is a function of the\niteration $t$; usually a linear decay $n = N.\\frac{T - t}{T}$. For\nexample, if $T = 10$, $90\\%$ of the tokens will be masked at\n$t = 1$. $80\\%$ at $t = 2$, and so forth.\n\n\n\\[Y_{\\text{mask}}^{\\left( t \\right)} = \\underset{i}{\\text{arg }\\min}\\left( p_{i},\\ n \\right)\\]\n\n\\[Y_{\\text{obs}}^{\\left( t \\right)} = Y\\backslash Y_{\\text{mask}}^{\\left( t \\right)}\\]\n\n\n  Predict:\nAfter masking, the CMLM predicts the masked tokens\n$Y_{\\text{mask}}^{\\left( t \\right)}$ mask, conditioned on the source\ntext $X$ and the unmasked target tokens\n$Y_{\\text{obs}}^{\\left( t \\right)}$. We select the prediction with\nthe highest probability for each masked token $y_i ∈ Y(t)$ mask and\nunmask them by update its probability score:\n\n\n\\[y_{i}^{\\left( t \\right)} = \\underset{w}{\\text{arg }\\text{max}}{P\\left( y_{i} = w \\middle| X,\\ Y_{\\text{obs}}^{\\left( t \\right)} \\right)}\\]\n\n\\[p_{i}^{\\left( t \\right)} = \\underset{w}{\\text{max}}{P\\left( y_{i} = w \\middle| X,\\ Y_{\\text{obs}}^{\\left( t \\right)} \\right)}\\]\n\nThe following example is from the WMT’14 German→English validation set\nthat illustrates how mask-predict generates text. At each iteration, the\nhighlighted tokens are masked and re-predicted, conditioned on the other\ntokens in the sequence.\n\n\n    \n\n\nIn the first iteration ($t = 0$), the entire target sequence is masked\n($Y_{\\text{mask}}^{\\left( 0 \\right)} = Y,\\ Y_{\\text{obs}}^{\\left( 0 \\right)} = \\varnothing$),\nand is thus generated by the CMLM in a purely non-autoregressive\nprocess. This produces an ungrammatical translation with repetitions\n(“completed completed”).\n\nIn the second iteration ($t = 1$), 8 of the 12 tokens generated in the\nprevious step were predicted with the lowest probabilities. That’s why\nthey got masked with the $\\left\\lbrack \\text{MASK} \\right\\rbrack$ token\nand re-predicted while conditioning on the input sequence $X$ and the\nfour unmasked tokens\n$Y_{\\text{obs}}^{\\left( 0 \\right)} = {The&quot;,\\20”,\\ November&quot;,\\.”}$\nwhich results in a more grammatical and accurate translation.\n\nIn the third iteration ($t = 2$), 4 of the 8 tokens generated in the\nprevious step were predicted with the lowest probabilities. Now that the\nmodel is conditioning on 8 tokens, it is able to produce an more fluent\ntranslation; “withdrawal” is a better fit for describing troop movement,\nand “November 20th” is a more common date format in English\n\n\n  Notes:\n\n  \n    \n      In the third iteration ($t = 2$), two of the four masked tokens were\n  predicted at the first step ($t = 0$), and not re-predicted at the\n  second step ($t = 1$). This is quite common for earlier predictions\n  to be masked at later iterations because they were predicted with\n  less information and thus tend to have lower probabilities.\n    \n    \n      When decoding, they unmask the highest $l$ tokens. This is a\n  hyper-parameter that can be seen as the beam size for beam search of\n  non-autoregressive decoders. The following table shows an experiment\n  of base CMLM with $T = 10$. Surprisingly, more $l$ tokens can\n  degrade performance.\n\n      \n\n\n    \n    \n      The following figure shows the decoding speed of CLML Transformer,\n  compared to the standard base transformer on the WMT’14 EN-DE test\n  set, with beam sizes $b = 1$ (orange triangle) and $b = 5$ (red\n  triangle). Each blue circle represents a mask-predict decoding run\n  with a different number of iterations $T = \\left\\{ 4,\\ …,\\ 10 \\right\\}$\n  and length candidates $l = \\left\\{ 1,\\ 2,\\ 3 \\right\\}$:\n\n      \n\n\n    \n  \n\n\nExperiments\n\nIn this paper, they followed the standard hyper-parameters for\ntransformers in both small and base configuration:\n\n\n\n    \n        \n            \n            Layers\n            Attention Heads\n            Model Dimension\n            Hidden Dimension\n        \n    \n    \n        Small\n        6\n        8\n        512\n        512\n    \n    \n        Base\n        6\n        8\n        512\n        2048\n    \n\n\n\nThey followed the weight initialization scheme from\nBERT, which samples\nweights from $\\mathcal{N}\\left( 0,\\ 0.02 \\right)$, initializes biases to\nzero, and sets layer normalization parameters to\n$\\beta = 0,\\ \\gamma = 1$. For regularization, they used $0.3$ dropout,\n$0.01$ L2 weight decay, and smoothed cross validation loss with\n$\\varepsilon = 0.1$.\n\nThey used batches of $128k$ tokens using Adam\noptimizer with $\\beta = (0.9,\\ 0.999)$ and $\\varepsilon = 10^{- 6}$. The\nlearning rate warms up to a peak of $5*10^{- 4}$ within $10,000$ steps,\nand then decays with the inverse square-root schedule. All models were\ntrained for 300k steps, measured the validation loss at the end of each\nepoch, and averaged the 5 best checkpoints. During decoding, they used a\nbeam size of $beam\\ size = 5$ for autoregressive decoding, and similarly\nuse $l = 5$ length candidates for mask-predict decoding.\n\nThe following table shows that among the parallel decoding methods, CMLM\nTransformer yields the state-of-the-art BLEU scores on WMT’14 EN-DE, in\nboth directions. Another striking result is that a CMLM with only 4\nmask-predict iterations yields higher scores than 10 iterations of the\niterative refinement model:\n\n\n    \n\n\nThe translations produced by CMLM Transformer also score competitively\nwhen compared to standard transformers. In all 4 benchmarks, CMLM-base\nreaches within 0.5-1.2 BLEU points from a well-tuned base transformer, a\nrelative decrease of less than 4% in translation quality. In many\nscenarios, this is an acceptable price to pay for a significant speedup\nfrom parallel decoding.\n\nThe following table shows that these trends also hold for\nEnglish-Chinese translation, in both directions, despite major\nlinguistic differences between the two languages:\n\n\n    \n\n\nBased on experiments run on EN-DE and EN-RO datasets, they found out\nthat multiple iterations $T$ is a must to solve the “multi-modality\nproblem” which means that the model often predict the same word $w$ with\nhigh confidence, but at different positions. As we can see from the\nfollowing table, the proportion of repetitive tokens drops drastically\nduring the first 2-3 iterations:\n\n\n    \n\n\nDuring experiments they noticed that longer sequences need more\niterations; the following table shows that increasing the number\nof decoding iterations ($T$) appears to mainly improve the performance\non longer sequences:\n\n\n    \n\n\nHaving said that, the performance differences across length buckets are\nnot very large, and it seems that even 4 mask-predict iterations are\nenough to produce decent translations for long sequences ($40 \\leq N$).\n\nModel Distillation\n\nFollowing previous convention on non-autoregressive machine translation\nmodels, they trained CMLM Transformer on translations produced by a\nstandard transformer model (large for EN-DE and EN-ZH, base for EN-RO).\nFor a fair comparison, they also trained standard left-to-right base\ntransformers on translations produced by large transformer models for\nEN-DE and EN-ZH, in addition to the standard baselines.\n\nTo determine CMLM’s dependence on knowledge distillation, they train\nmodels on both raw and distilled data, and compared their performance.\nThe following table shows that in every case, training with model\ndistillation substantially outperforms training on raw data:\n\n\n    \n\n\nIt appears as though CMLMs are heavily dependent on model\ndistillation.\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Evaluation Metrics",
        "url"       : "/machine-translation/Evaluation",
        "date"      : "01/01/2020",
        "content": "Machine Translation models translate a given sentence by searching for\nthe sentence that best suits a given criterion. However, all approaches\nhave to be evaluated to quantify the quality and accuracy of the\nproduced translations. Naturally, the best method would be to have human\nexperts rate each produced translation (Candidate) in order to evaluate\nthe whole MT system based on the reference translations.\n\nThis is a quite costly process and is not viable for development of MT systems.\nFor this reason a number of metrics exist that automate the process and use\ndifferent scoring methods to automatically evaluate the produced translation\nbased on a reference sentence. The evaluation metrics objective is to be as\nclose as possible to the human translation and currently the most common\ntechniques for calculating the correlation between human and automatic\nevaluations are the Spearman’s rank correlation coefficient and the\nPearson product-moment correlation coefficient. These evaluation metrics\ncan be divided into three categories:\n\n\n  \n    Count-based Metrics.\n  \n  \n    Edit-distance-based Metrics.\n  \n  \n    Pre-trained Metrics.\n  \n\n\nCount-based Metrics\n\nCount-based metrics compute the n-grams of both reference and candidate\nand then compare them with each other using a scoring function.\n\nBLEU\n\nBLEU, stands for “Bilingual Evaluation Understudy”, is an\nevaluation metric for machine translation proposed by Aachen University\nin 2002 and published in this paper: “Discriminative Training and\nMaximum Entropy Models for Statistical Machine\nTranslation”. BELU is considered\nthe most commonly used evaluation metric for machine translation so far.\nIn the following part, we will get a sense of how it works:\n\n\n    \n\n\nTaking the former example where the French sentence has been translated\nby two different linguists. Let’s say that our Machine Translation Model\nhas produced a bad translation for the French sentence; which is:\n\n\n    \n\n\nLet’s see how we can calculate the BLEU score for this translation.\nFirst, we will split the sentence into words and see if each word\nappears in the provided references like so. But, BLEU score doesn’t care\nabout only the words. It’s cares about word-pairs as well. So, let’s see\nthe bigram word-pairs of the previous translation too:\n\n\n    \n\n\nSo, the unigram score will be\n$\\frac{2 + 1 + 1 + 1}{3 + 2 + 1 + 1} = \\frac{5}{7}$ and the bigram score\nwill be\n$\\frac{1 + 0 + 1 + 1 + 1}{2 + 1 + 1 + 1 + 1} = \\frac{4}{6} = \\frac{2}{3}$.\nIn other words, the BLEU score of a n-gram model will be:\n\n\\[P_{n} = \\left( \\sum_{ngram \\in \\hat{y}}^{}{\\text{Coun}t_{\\text{maxref}}\\left( \\text{ngram} \\right)} \\right) \\ast \\left( \\sum_{ngram \\in \\hat{y}}^{}{\\text{Coun}t_{\\text{output}}\\left( \\text{ngram} \\right)} \\right)^{- 1}\\]\n\nIt’s common to get the values of $P_{1}$ till $P_{4}$ then combine these\nscores in the following formula:\n\n\\[\\text{BLEU} = \\text{BP} \\ast \\exp\\left( \\frac{1}{4} \\ast \\sum_{n = 1}^{4}P_{n} \\right)\\]\n\n$\\text{BP}$ factor stands for “Brevity Penalty”. It turns out that if we\noutput very short translations, it&#39;s easier to get high BLEU score\nbecause most of the words will appear in the references. But we don&#39;t\nwant translations that are very short. So, the $\\text{BP}$, or the\nbrevity penalty, is an adjustment factor that penalizes translation\nsystems that output translations that are too short. So, the formula for\nthe brevity penalty is the following. It&#39;s equal to $1$ if our machine\ntranslation system actually outputs things that are longer than the\nhuman generated reference outputs. And otherwise is some formula like\nthat that overall penalizes shorter translations:\n\n\\[\\text{BP} = \\left\\{ \\begin{matrix}\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ 1\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ L_{\\text{out}} &amp;gt; L_{\\text{ref}} \\\\\n\\exp\\left( 1 - \\frac{L_{\\text{ou}t}}{L_{\\text{ref}}} \\right)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{otherwise} \\\\\n\\end{matrix} \\right.\\]\n\nSuch that $L_{\\text{out}}$ is the length of the generated sentence by\nour Machine Translation Model, $L_{\\text{ref}}$ is the length of the\nreference sentence.\n\nMETEOR\n\nMeteor, stands for “Metric for Evaluation of Translation\nwith Explicit Ordering”, is an evaluation metric for machine\ntranslation proposed by Carnegie Mellon University in 2005 and published\nin this paper: “METEOR: An Automatic Metric for MT Evaluation with\nImproved Correlation with Human\nJudgments”.\n\nThis metric was designed to fix some of the problems found in the BLEU\nmetric, and also produce good correlation with human judgement.\nAccording to the paper, METEOR shows correlation of up to 0.964 with\nhuman judgement at the corpus level, compared to BLEU&#39;s achievement of\n0.817 on the same data set.\n\nTo see how this metric works, let’s take an example and evaluate the\nfollowing hypothesis translation using the reference one:\n\n\n    \n\n\nWe can calculate the METEOR metric using the following steps:\n\n\n  The first step is to create an alignment between two sentences\n(hypothesis, reference) resulting something like this:\n\n\n\n    \n\n\n\n  Then, calculate the precision $P$ and the recall $R$:\n\n\n\\[P = \\frac{\\#\\ unigrams\\ in\\ hypothesis\\ found\\ in\\ reference}{\\#\\ unigrams\\ in\\ hypothesis} = \\frac{6}{7}\\]\n\n\\[R = \\frac{\\#\\ unigrams\\ in\\ hypothesis\\ found\\ in\\ reference}{\\#\\ unigrams\\ in\\ reference} = \\frac{6}{6} = 1\\]\n\n\n  Then, Precision and recall are combined using the harmonic mean with\nrecall weighted 9 times more than precision:\n\n\n\\[F_{\\text{mean}} = \\frac{10P.R}{R + 9P} = \\frac{10.\\frac{6}{7}.1}{1 + 9.\\frac{6}{7}} = 0.9836\\]\n\n\n  Calculate the hypothesis chunks; a chunk is defined as a set of\nunigrams that are adjacent in the hypothesis and in the reference.\nSo, in this example, we have 2 chunks that consists of 6 unigrams:\n\n\n\\[c = 2,\\ \\ u_{m} = 6\\]\n\n\n    \n\n\n\n  Now, we can calculate the penalty; The penalty has the effect of\nreducing the F~mean~ by up to 50% if there are no bigram or longer matches:\n\n\n\\[penalty = 0.5\\left( \\frac{c}{u_{m}} \\right)^{3} = 0.5\\left( \\frac{2}{6} \\right)^{3} = 0.0185\\]\n\n\n  Finally, the METEOR value is:\n\n\n\\[M = F_{\\text{mean}}\\left( 1 - penalty \\right) = 0.9836\\left( 1 - 0.0185 \\right) = 0.9654\\]\n\n\n  Notes:\n\n  \n    If there are two alignments with the same number of mappings, the\n  alignment is chosen with the fewest crosses/intersections. From\n  the two alignments shown below, the alignment on the left would be\n  selected at this point:\n  \n\n  \n    \n\n\n  \n    \n      The highest value for METEOR metric is 1 unlike BLEU.\n    \n    \n      To calculate a score over a whole corpus, the aggregate values for\n  P, R and penalty are taken and then combined using the\n  same formula.\n    \n    \n      METEOR also works for comparing a candidate translation against more\n  than one reference translations. In this case the algorithm\n  compares the candidate against each of the references and selects\n  the highest score.\n    \n  \n\n\nBEER\n\nBEER, stands for “BEtter Evaluation as Ranking”, is an\nevaluation metric for machine translation proposed by the university of\nAmesterday in 2014 and published in this paper: “BEER: BEtter\nEvaluation as Ranking” and the\nofficial GitHub repository can be found here: \nbeer.\n\nTO BE CONTINUED…\n\nChrF\n\nChrF, stands for “character F-score” is another count-based metric for\nevaluating machine translation models. ChrF was proposed by Maja\nPopovic´at the Humboldt University of Berlin in 2015 and published in\nthis paper: “CHRF: character n-gram F-score for automatic MT\nevaluation”.\n\n\n  Note:\nYou can use nltk.chrf_score() function for this metric.\n\n\nThis metric depends on the character-level n-gram since it correlates very well\nwith human judgments. The general formula for the CHRF score is:\n\n\\[\\text{chr}F_{\\beta} = \\left( 1 + \\beta^{2} \\right).\\frac{chrP + chrR}{\\beta^{2}.chrP + chrR}\\]\n\nWhere $\\text{chrP}$ and $\\text{chrR}$ are the n-gram precision and\nrecall respectively averaged over all character n-grams. And and $\\beta$\nis a parameter which assigns $\\beta$ times more importance to recall\nthan to precision. If $\\beta = 1$, they have the same importance.\n\nTo understand this metric better, let’s take an example and evaluate the\nfollowing hypothesis translation using the reference one:\n\n\n    \n\n\nWe can calculate the chrF metric using the following steps:\n\n\n  Calculate the unigram character for both reference and hypothesis:\n\n\n\n\n    \n        \n            \n            a\n            c\n            e\n            h\n            i\n            k\n            l\n            o\n            s\n            t\n            v\n            y\n        \n    \n    \n        Reference\n        1\n        1\n        2\n        1\n        1\n        0\n        1\n        1\n        0\n        2\n        1\n        0\n    \n    \n    Hypothesis\n        0\n        0\n        2\n        1\n        3\n        2\n        1\n        0\n        0\n        3\n        0\n        1\n    \n\n\n\n\n  \n    Calculate the following metrics:\n\n    \n      $tp$ (True Positive): count of characters found in both\nhypothesis and reference. So, in this example, we have (e,2),\n(h,1), (i,1), (l,1), (t,2):\n    \n  \n\n\n\\[tp = 7\\]\n\n\n  $\\text{tpfp}$ (True Positive + False Positive): count of characters\nfound in hypothesis.\n\n\n\\[tpfp = 11\\]\n\n\n  $\\text{tpfn}$ (True Positive + False Negative): count of characters\nfound in reference.\n\n\n\\[tpfn = 13\\]\n\n\n  Then, we can calculate the precision $\\text{chr}P$ and the recall\n$\\text{chrR}$:\n\n\n\\[\\text{chr}P = \\frac{\\text{tp}}{\\text{tpfp}} = \\frac{7}{11},\\ \\ \\ \\ \\ chrR = \\frac{\\text{tp}}{\\text{tpfn}} = \\frac{7}{13}\\]\n\n\n  Now, we can calculate the character f-score:\n\n\n\\[\\text{chr}F_{\\beta} = \\left( 1 + \\beta^{2} \\right).\\frac{chrP + chrR}{\\beta^{2}.chrP + chrR}\\]\n\n\n  All of that for unigram character model. In the paper, they did that\nwhen n=1 till n=6. So, we will do the same when n=2,3,4,5,6 and\nthen average the f-score.\n\n\nEdit-Distance-Based Metrics\n\nEdit distance based metrics utilize the edit distance to express the\ndifference between the candidate and the reference. Edit distance is a\nway to quantify how two words are far apart. More formally, the minimum\nedit distance between two strings is defined as the minimum number of\nediting operations (operations like insertion, deletion, substitution,\nand shifts of adjacent letters) needed to transform one string into\nanother. The most common edit distance is the Levenshtein distance.\n\nMMS\n\nMMS, stands for “Maximum Matching String”, is an evaluation\nmetric for machine translation proposed by New York University in 2003\nand published in this paper: “Evaluation of Machine Translation and its\nEvaluation”.\n\nTO BE CONTINUED…\n\nTER\n\nTER, stands for “Translation Edit Rate”, is an evaluation\nmetric for machine translation proposed by the university of Maryland in\n2006 and published in this paper: “A Study of Translation Edit Rate\nwith Targeted Human\nAnnotation” and\nan unofficial repository implementing TER can be found in the\nsacreBLEU python package.\n\nTER measures the amount of editing that a human would have to perform to\nchange a candidate translation so it exactly matches a reference\ntranslation normalized by the average length of the references.\n\n\\[TER = \\frac{\\#\\ of\\ edits}{average\\ \\#\\ of\\ reference\\ words}\\]\n\nLet’s take an example to see how TER works; consider the following\ntranslation:\n\n\n    \n\n\nHere, the hypothesis (HYP) is fluent and means the same thing (except\nfor missing “American”) as the reference (REF). However, TER does not\nconsider this an exact match as:\n\n\n  \n    The phrase “this week” is “shifted” (this counts as one\n shift). Shifts have a cost of one no matter how far this\nphrase moves.\n  \n  \n    The phrase “Saudi Arabia” in the reference appears as “the Saudis”\nin the hypothesis (this counts as two separate\nsubstitutions).\n  \n  \n    The word “American” appears only in the reference (this counts as\none insertion).\n\n    Then, the TER score is:\n  \n\n\n\\[TER = \\frac{4}{13} = 30.77\\%\\]\n\n\n  Important Notes:\n\n  \n    \n      Since we are concerned with the minimum number of edits needed to\n  modify the hypothesis, we only measure the number of edits to the\n  closest reference.\n    \n    \n      TER assumes that all edits (insertion, deletion, ...etc.) have the\n  same cost.\n    \n    \n      Punctuations are treated as normal words.\n    \n    \n      Mis-capitalization is counted as an edit.\n    \n    \n      In the paper, T(1) denoted that TER was used on just one reference\n  sentence while T(4) was used on four different reference sentences.\n    \n  \n\n\nCharacTER\n\nCharacTER is a Translation Edit Rate (TER) on Character evaluation\nmetric for machine translation proposed by Aachen University in 2016 and\npublished in this paper: “CharacTER: Translation Edit Rate on Character\nLevel” and the official code for\nthis metric can be found on this GitHub repository:\nCharacTER.\n\nCharacTer is defined as the minimum number of character edits required\nto adjust a hypothesis until it completely matches the reference,\nnormalized by the length of the hypothesis sentence:\n\n\\[CharacTER = \\frac{shift\\ cost\\  + \\ edit\\ distance}{\\#\\ characters\\ in\\ the\\ hypothesis\\ sentence}\\]\n\nCharacTer calculates shift edit on word level; two words are considered\nto be matched if they are exactly the same, or if the edit\ndistance between them is below a threshold value.\n\nEED\n\nEED, stands for “Extended Edit Distance”, is an evaluation\nmetric for machine translation proposed by Aachen University in 2019 and\npublished in this paper: “EED: Extended Edit Distance Measure for\nMachine Translation”. The\nofficial code for this metric can be found in the following GitHub\nrepository: ExtendedEditDistance.\n\nThis paper proposes an extension of the Levenshtein edit distance, which\nachieves better human correlation whilst remaining fast, flexible and\neasy to understand. This extension is can be described as a “jump”, a\njumps provides the opportunity to continue the edit distance computation\nfrom a different point. In the following figure, a jump is represented\nas a dashed line:\n\n\n    \n\n\nEED utilizes the idea of jumps as an extension of the edit distance. EED\noperates at character level and is defined as follows:\n\n\\[EED = min\\left( \\frac{\\left( e + \\alpha \\text{.j} \\right) + \\rho \\text{.v}}{\\left| r \\right| + \\rho \\text{.v}},\\ 1 \\right),\\ \\ \\ \\text{EED} \\in \\left\\lbrack 0,\\ 1 \\right\\rbrack\\]\n\nWhere:\n\n\n  \n    $e$ is the sum of the edit operation with uniform cost of 1 for\ninsertions and substitutions and 0.2 for deletions.\n  \n  \n    $j$ denotes the number of jumps performed with the corresponding\ncontrol parameter $\\alpha = 2.0$.\n  \n  \n    $v$ defines the number of characters that have been visited multiple\ntimes or not at all and scales over $\\rho = 0.3$.\n  \n  \n    \n      \n        \n          $\\left\n          r \\right\n          $ is the length of the reference sentence.\n        \n      \n    \n  \n\n\nPre-trained Metrics\n\nThese kind of metrics use pre-trained neural models to evaluate the\nquality of MT output texts given the source sentence, the human\nreference, or both. One thing good about these metrics is that they are\nnot strictly dependent on the human translation, so they can better\nevaluate synonyms or paraphrases.\n\nOn the other hand, their performance is influenced by the data on which\nthey have been trained. Also, the pre-trained models introduce a\nblack-box problem where it is difficult to diagnose potential unexpected\nbehavior of the metric, such as various biases learned from training\ndata.\n\nYiSi\n\nCheck the paper: YiSi - A unified semantic MT quality evaluation and\nestimation metric for languages with different levels of available\nresources. The official code for\nthis paper can be found on this GitHub repository:\nyisi.\n\nTO BE CONTINUED…\n\nBERTScore\n\nBERTScore is an automatic evaluation metric for text generation proposed\nby Cornell University in 2020 and published in their paper: Bertscore:\nEvaluating Text Generation With\nBert. The official repository for\nthis paper can be found here\nbert_score.\n\nBERTScore computes a similarity score for each token in the candidate\nsentence with each token in the reference sentence. However, instead of\nexact matches, we compute token similarity using contextual embeddings.\nWhich makes it the perfect candidate for evaluating machine translation\nmodels. The scoring algorithm is relatively straightforward as shown in\nthe following figure:\n\n\n    \n\n\nGiven a reference sentence and a candidate sentence $x$ and a candidate\nsentence $\\widehat{x}$, the scoring algorithm goes like so:\n\n\n  \n    Tokenize the sentences using the tokenizer provided by each model;\nso the reference sentence becomes\n$x = \\langle x_{1},\\ …x_{k} \\rangle$ and the candidate sentence becomes\n$\\widehat{x} = \\langle \\widehat{x}_1,\\text{ …}\\widehat{x}_l \\rangle$\n  \n  \n    Given these tokenized sentences, BERT generates representation the\nsame size as the tokenized sentences; the translation hypothesis\nembedding\n$\\mathbf{x}=\\langle \\mathbf{x}_{\\mathbf{1}}\\mathbf{,\\ …}\\mathbf{x}_{\\mathbf{k}} \\rangle$\nand the reference sentence embedding\n$\\widehat{\\mathbf{x}}=\\langle \\widehat{\\mathbf{x}}_{\\mathbf{1}},\\text{ …}\\widehat{\\mathbf{x}}_{\\mathbf{l}} \\rangle$.\n  \n  \n    Compute a matrix of pair-wise cosine similarities of all words from\nthe hypothesis and from the reference sentence. The cosine\nsimilarity of a reference token $x_{i}$ and a candidate token\n${\\widehat{x}}_{j}$ is:\n  \n\n\n\\[cosine = \\frac{\\mathbf{x}_{\\mathbf{i}}^{\\mathbf{T}}.{\\widehat{\\mathbf{x}}}_{\\mathbf{j}}}{\\left\\| \\mathbf{x}_{\\mathbf{i}} \\right\\|.\\left\\| {\\widehat{\\mathbf{x}}}_{\\mathbf{j}} \\right\\|}\\]\n\n\n  Use greedy approach, we get the maximum similarity where each token\nin the reference sentence is matched to the most similar token in\nthe candidate sentence to use them to compute the precision &amp;amp;\nrecall.\n\n\n\\[R_{\\text{BERT}} = \\frac{1}{\\left| x \\right|}\\sum_{x_{i} \\in x}^{}{\\max_{\\widehat{x}_j \\in \\widehat{x}}{\\mathbf{x}_{\\mathbf{i}}^{\\mathbf{T}}.{\\widehat{\\mathbf{x}}}_{\\mathbf{j}}}}\\]\n\n\\[P_{\\text{BERT}} = \\frac{1}{\\left| \\widehat{x} \\right|}\\sum_{\\widehat{x}_j \\in \\widehat{x}}^{}{\\max_{x_{i} \\in x}{\\mathbf{x}_{\\mathbf{i}}^{\\mathbf{T}}.{\\widehat{\\mathbf{x}}}_{\\mathbf{j}}}}\\]\n\n\n  We combine precision and recall to compute the F1 measure: the\nharmonic average of precision and recall.\n\n\n\\[F_{\\text{BERT}} = \\frac{2P_{\\text{BERT}}.R_{\\text{BERT}}}{P_{\\text{BERT}} + R_{\\text{BERT}}}\\]\n\n\n  Since previous work demonstrated that rare words can be more\nindicative for sentence similarity than common words, we incorporate\ninverse document frequency (idf) weighting. Given $M$ reference\nsentences $\\left[ x^{\\left( i \\right)} \\right]_{i = 1}^{M}$, the\nidf score of a word-piece token $w$ is:\n\n\n\\[\\text{idf}\\left( w \\right) = - \\log\\left( \\frac{1}{M}\\sum_{i = 1}^{M}{\\mathbb{I}\\left\\lbrack w \\in x^{\\left( i \\right)} \\right\\rbrack} \\right)\\]\n\n   Where $\\mathbb{I}\\left\\lbrack . \\right\\rbrack$ is an indicator function.\n\n\n  Now, the precision &amp;amp; recall becomes:\n\n\n\\[R_{\\text{BERT}} = \\frac{1}{\\sum_{x_{i} \\in x}^{}{\\text{idf}\\left( x_{i} \\right)}}\\sum_{x_{i} \\in x}^{}{\\text{idf}\\left( x_{i} \\right)\\max_{\\widehat{x}_j \\in \\widehat{x}}{\\mathbf{x}_{\\mathbf{i}}^{\\mathbf{T}}.{\\widehat{\\mathbf{x}}}_{\\mathbf{j}}}}\\]\n\n\\[P_{\\text{BERT}} = \\frac{1}{\\sum_{x_{i} \\in x}^{}{\\text{idf}\\left( x_{i} \\right)}}\\sum_{\\widehat{x}_j \\in \\widehat{x}}^{}{\\max_{x_{i} \\in x}{\\text{idf}\\left( x_{i} \\right).\\mathbf{x}_{\\mathbf{i}}^{\\mathbf{T}}.{\\widehat{\\mathbf{x}}}_{\\mathbf{j}}}}\\]\n\n\n  Finally, BERTScore have the same numerical range of cosine\nsimilarity $\\left\\lbrack - 1,1 \\right\\rbrack$. However, in practice\nscores are in a more limited range $\\left\\lbrack 0,1 \\right\\rbrack$.\nSo, we rescale BERTScore with respect to its empirical lower bound b\nas a baseline like so:\n\n\n\\[{\\widehat{R}}_{\\text{BERT}} = \\frac{R_{\\text{BERT}} - b}{1 - b}\\]\n\n\\[{\\widehat{P}}_{\\text{BERT}} = \\frac{P_{\\text{BERT}} - b}{1 - b}\\]\n\n\\[{\\widehat{F}}_{\\text{BERT}} = \\frac{F_{\\text{BERT}} - b}{1 - b}\\]\n\n\n  We compute b using Common Crawl monolingual datasets. For each\nlanguage and contextual embedding model, we create 1M\ncandidate-reference pairs by grouping two random sentences. Because\nof the random pairing and the corpus diversity, each pair has very\nlow lexical and semantic overlapping. We compute $b$ by averaging\nBERTScore computed on these sentence pairs.\n\n\nBLEURT\n\nBLEURT is an automatic evaluation metric for text generation proposed by\nGoogle Research in 2020 and published in their paper: BLEURT: Learning\nRobust Metrics for Text\nGeneration. The official\ncode for this paper can be found on Google Research’s official GitHub\nrepository: bleur.\n\nTO BE CONTINUED…\n\nCOMET\n\nBLEURT is an automatic evaluation metric for machine translation\nproposed by Unbabel AI in 2020 and published in their paper: COMET: A\nNeural Framework for MT\nEvaluation. The official\nrepository for this paper can be found on this GitHub repository: \nCOMET.\n\nTO BE CONTINUED…\n\nPrism\n\nPrism is an automatic evaluation metric for machine translation proposed\nby John Hopkins Unversity in 2020 and published in their paper:\nAutomatic Machine Translation Evaluation in Many Languages via\nZero-Shot Paraphrasing. The\nofficial code for this paper can be found on this GitHub repository:\nprism.\n\nTO BE CONTINUED…\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Reformer: Efficient Transformer",
        "url"       : "/machine-translation/Reformer",
        "date"      : "13/01/2020",
        "content": "Reformer is an efficient version of the transformers proposed by Google\nResearch in 2020 and published in this paper: “Reformer: The efficient\nTransformer”. The official code\nfor this paper can be found in this GitHub repository:\nreformer-pytorch. In\nthis paper, the authors introduced two techniques to improve the memory\nefficiency of Transformers while keeping the same great performance.\n\n\n  \n    They replaced the dot-product attention by one that uses\nlocality-sensitive hashing, changing its complexity from\n$O\\left( N^{2} \\right)$ to\n$O\\left( N \\text{log}\\left( N \\right) \\right)$, where $N$ is the\nlength of the sequence.\n  \n  \n    They used reversible residual layers instead of the standard\nresiduals, which allows storing activations only once in the\ntraining process instead of $L$ times, where $L$ is the number of\nlayers.\n  \n  \n    They split activations inside feed-forward layers and processing\nthem in chunks which removes the effect of the depth and saves\nmemory inside feed-forward layers.\n\n    Before getting into more details, let’s first see how a transformer\nmodel consumes memory assuming that the vocabulary is $V$, the word\nembedding size is $d$, the input sequence length is $L$, the depth\nof the feed-forward network is $d_{\\text{ff}}$ where the $i^{th}$ layer\nhas $N_{i}$ neurons, and the number of batches is $B$:\n  \n  \n    Word embedding matrix is $V*d$.\n  \n  \n    Positional Embedding is $d*L$.\n  \n  \n    Feed-forward weights $\\prod_{i = 1}^{d_{\\text{ff}}}N_{i}$.\n  \n  \n    The original paper uses 6 heads of attention, each attention head\nhas:\n\n    \n      \n        $3*L*d$ parameters for V, Q and K.\n      \n      \n        $L*L$ attention weights.\n\n        This is just for one layer, the memory with N layers is N-times\nlarger than that due to the fact that activations need to be\nstored for back-propagation. And all of that was just for the\nencoder, the decoder takes even more memory. Also, this was done\nusing one batch, using the batch size will roughly multiply this\nnumber by a factor of B. And the standard way of saving\nparameters is float-32 bit. So, all this needs to be multiplied\nby 4 to get the number of bytes needed for saving this model.\nWhich is a lot!\n      \n    \n  \n\n\nNowadays, researchers tend to use large-scale long-sequence transformers\nsince they yield great results. But this strains resources to the point\nthat this trend is breaking NLP research as these large Transformer\nmodels can only realistically be trained in large industrial research\nlaboratories. And that’s hurting the field since only the big\ncorporations are the ones who can afford such a thing. So, there was a\nneed for a more efficient Transformer.\n\nLSH Attention\n\nLSH Attention stands for “Locality-Sensitive Hashing Attention” which is\na memory and computationally efficient way to perform the attention\nmechanism. A hashing scheme that assigns each vector $x$ to a hash\n$h(x)$ is called “locality-sensitive” if nearby vectors get the same\nhash with high probability and distant ones do not.\n\nDot-product Attention\n\nBefore getting into the First, let’s recap the standard attention used\nin the original Transformer paper which was the scaled dot-product\nattention:\n\n\\[\\text{Attention}\\left( Q,\\ K,\\ V \\right) = softmax\\left( \\frac{QK^{T}}{\\sqrt{d_{k}}} \\right)V\\]\n\nAs we can see from the previous formula, the input consists of queries\n$Q$, keys $K$ and values $V$. These matrices are obtained by projecting\nthe contextualized word embedding (Word embedding + position embedding)\ninto $Q$, $K$, and $V$ (each of $B*L*d$ dimension) using three different\nfeed-forward networks.\n\nModifications\n\nLSH attention can be seen as a more optimized form of the scaled-dot\nproduct attention which was obtains using the following modifications:\n\n\n  \n    It uses the same feed-forward network to obtain both $Q$ and $K$ as\nit turns out that this does not affect the performance of the\nTransformer model.\n  \n  \n    It doesn’t calculate the whole $QK^{T}$ term. It only computes the\nattention for each query $q_{i}$ separately and then re-computes it\non the backward pass when needed for gradients.\n  \n  \n    According to the equation, the model actually is interested in the\nmost similar q and k represented in the\n$\\text{softmax}\\left( QK^{T} \\right)$ term. That’s why LSH attention\nonly considers a subset of $K$ that are close to $q_{i}$ rather than\nthe whole $K$. The problem of finding nearest neighbors quickly in\nhigh-dimensional spaces can be solved by Local-sensitivity\nhashing scheme, hence the name of\nthe mechanism.\n  \n\n\nSo, now let’s rewrite the same scaled-dot product attention formula\nafter putting in mind the mentioned modifications:\n\n\\[o_{i} = \\sum_{j \\in \\mathcal{P}_{i}}^{}{\\exp\\left( q_{i}.k_{j} - z\\left( i,\\ \\mathcal{P}_{i} \\right) \\right)v_{j}}\\]\n\n\\[\\mathcal{P}_{i} = \\left\\{ j:\\ i \\geq j \\right\\}\\]\n\nAs you can see, now we are considering just a single query position i at\na time (denoted $q_{i}$). We introduced the notion\n$\\mathcal{P}_{i}$ to represent the set that the query at position\ni attends to. And we replaced the normalizing term in the softmax\n$\\sum_{i \\in L}^{}{\\exp\\left( q_{i}.k_{i} \\right)}$ with a\nfunction $z$.\n\nFor batching purposes, we typically perform attention over a larger set\n${\\widetilde{\\mathcal{P}}}_{i} = \\left[ 0,\\ 1,\\ …,\\ l \\right]$\nwhile masking out elements not in $\\mathcal{P}_{i}$ using a\nmasking function $m()$. So, the formula becomes:\n\n\\[o_{i} = \\sum_{j \\in {\\widetilde{\\mathcal{P}}}_{i}}^{}{\\exp\\left( q_{i}.k_{j} - m\\left( j,\\ \\mathcal{P}_{i} \\right) - z\\left( i,\\ \\mathcal{P}_{i} \\right) \\right)v_{j}}\\]\n\n\\[\\text{m}\\left( j,\\ \\mathcal{P}_{i} \\right) = \\left\\{ \\begin{matrix}\n\\infty\\ \\ \\ \\ \\ \\ \\text{if}\\text{j} \\notin \\mathcal{P}_{i} \\\\\n0\\ \\ \\ \\ \\ \\ \\text{otherwise} \\\\\n\\end{matrix} \\right.\\]\n\nLSH Attention Mechanism\n\nNow, let’s turn to LSH attention, which we can think of in terms of\nrestricting the set $\\mathcal{P}_{i}$ by only allowing attention\nwithin a single hash bucket\n$\\mathcal{P}_{i} = \\left( j:h\\left( q_{i} \\right) = h\\left( k_{i} \\right) \\right)$.\nAnd this can be done by the following steps:\n\n\n    \n\n\n\n  \n    First, apply the LSH scheme over the given queries Q and keys K by\nemploying random projections. To get $b$ hashes, we first fix a\nrandom matrix $R$ of size\n$\\left\\lbrack b_{k},\\ \\frac{b}{2} \\right\\rbrack$. And then define a\nhashing function $h(x) = \\text{argmax}(\\lbrack xR; - xR\\rbrack)$\nwhere $\\lbrack u;\\ v\\rbrack$ denotes the concatenation of two\nvectors knowing that they ensured that\n$h\\left( k_{j} \\right) = h\\left( q_{j} \\right)$ by setting\n$\\frac{q_{j}}{\\left| q_{j} \\right|}$.\n  \n  \n    Then, sort the queries and keys according to their hash bucket and\nthen allow attention within each bucket (right graph) rather than\nthe full matrix (left graph).\n  \n\n\n\n    \n\n\n\n  Next, they sorted the queries by bucket number and, within each\nbucket, by sequence position; this defines a permutation where\n$i \\rightarrow s_{i}$ after sorting. In the sorted attention matrix,\npairs from the same bucket will cluster near the diagonal.\n\n\n\n    \n\n\n\n  Hash buckets in this formulation tend to have different number of\nqueries and keys. In fact, it is possible for a bucket to contain\nmany queries but no keys. To alleviate these issues, they process\n$m$ consecutive queries where they attend to each other, and one\nchunk back. In the following graph $m = 2$.\n\n\n\n    \n\n\n\n  Following our earlier notation, this corresponds to setting where\n$m = \\frac{2*l}{n_{\\text{buckets}}}$ where $l$ is the sequence\nlength and the average bucket size is\n$\\frac{l}{n_{\\text{buckets}}}$:\n\n\n\\[{\\widetilde{\\mathcal{P}}}_{i} = \\left\\{ j:\\ \\left\\lfloor \\frac{s_{i}}{m} \\right\\rfloor - 1 \\leq \\left\\lfloor \\frac{s_{j}}{m} \\right\\rfloor \\leq \\left\\lfloor \\frac{s_{i}}{m} \\right\\rfloor \\right\\}\\]\n\nMulti-round LSH attention\n\nWith hashing, there is always a small probability that similar items\nnevertheless fall in different buckets. This probability can be reduced\nby doing multiple rounds of hashing with $n_{\\text{rounds}}$ distinct\nhash functions\n$\\left( h^{\\left( 1 \\right)},\\ h^{\\left( 2 \\right)},\\ …\\  \\right)$\nsuch that:\n\n\\[\\mathcal{P}_{i} = \\bigcup_{r = 1}^{n_{\\text{rounds}}}\\mathcal{P}_{i}^{\\left( r \\right)}\\ \\ \\ \\ \\ \\ \\ \\ \\mathcal{P}_{i}^{\\left( r \\right)} = \\left\\{ j:\\ h^{\\left( r \\right)}\\left( q_{i} \\right) = h^{\\left( r \\right)}\\left( q_{j} \\right) \\right\\}\\]\n\nThe multi-round case essentially involves performing LSH attention\n$n_{\\text{rounds}}$ times in parallel. So, starting with the original\nformula:\n\n\\[o_{i} = \\sum_{j \\in {\\widetilde{\\mathcal{P}}}_{i}}^{}{\\exp\\left( q_{i}.k_{j} - m\\left( j,\\ \\mathcal{P}_{i} \\right) - z\\left( i,\\ \\mathcal{P}_{i} \\right) \\right)v_{j}}\\]\n\nWe can combine it with the multiple rounds hashing like so:\n\n\\[o_{i} = \\sum_{r = 1}^{n_{\\text{rounds}}}{\\exp\\left( z\\left( i,\\ \\mathcal{P}_{i}^{\\left( r \\right)} \\right) - z\\left( i,\\ \\mathcal{P}_{i} \\right) \\right)}\\sum_{j \\in {\\widetilde{\\mathcal{P}}}_{i}}^{}{\\frac{1}{N_{i,j}}\\exp\\left( q_{i}.k_{j} - m\\left( j,\\ \\mathcal{P}_{i}^{\\left( r \\right)} \\right) - z\\left( i,\\ \\mathcal{P}_{i}^{\\left( r \\right)} \\right) \\right)v_{j}} = \\sum_{r = 1}^{n_{\\text{rounds}}}{\\exp\\left( z\\left( i,\\ \\mathcal{P}_{i}^{\\left( r \\right)} \\right) - z\\left( i,\\ \\mathcal{P}_{i} \\right) \\right)}o_{i}^{\\left( r \\right)}\\]\n\nEach round of LSH attention produces a vector $o_{i}^{\\left( r \\right)}$\nthat can be computed independently from other rounds, except for the\ninclusion of a term $N_{i,j}$ to avoid that they folded the $N_{i,j}$\nfactor into the masking term . So, the becomes:\n\n\\[o_{i}^{\\left( r \\right)} = \\sum_{j \\in {\\widetilde{\\mathcal{P}}}_{i}}^{}{\\exp\\left( q_{i}.k_{j} - m_{i,j}^{\\left( r \\right)} - z\\left( i,\\ \\mathcal{P}_{i}^{\\left( r \\right)} \\right) \\right)v_{j}}\\]\n\nWhere;\n\n\\[m_{i,j}^{\\left( r \\right)} = \\left\\{ \\begin{matrix}\n\\ \\ \\ \\infty\\ \\ \\ \\ \\ \\ \\text{if}\\ \\ \\text{j} \\notin \\mathcal{P}_{i} \\\\\n10^{5}\\ \\ \\ \\ \\text{if}\\ \\ i = j \\\\\n\\log N_{i,j}\\ \\ \\ \\text{otherwise} \\\\\n\\end{matrix} \\right.\\]\n\n\\[N_{i,j} = \\left| \\left\\{ r&#39;:\\ j \\in \\mathcal{P}_{i}^{\\left( r&#39; \\right)} \\right\\} \\right|\\]\n\nThey introduced a special case for $m_{i,j}^{\\left( r \\right)}$ for when\n$i = j$. This case is added because causal masking in a standard\nTransformer allows position $i$ to attend to itself, which is not\ndesirable in a shared-QK formulation. They set the mask to a large but\nfinite value to disallow attention-in-place, except in the situation\nwhere a token has no other valid attention targets.\n\nReversible Transformer\n\nTO BE CONTINUED\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Linformer: Linear Transformer",
        "url"       : "/machine-translation/Linformer",
        "date"      : "08/06/2020",
        "content": "Linformer is an efficient version of the transformers proposed by\nFacebook AI in 2020 and published in this paper: “Linformer:\nSelf-Attention with Linear\nComplexity”. The official code\nfor this paper can be found in the FairSeq official GitHub repository:\nlinformer.\nLinformer can perform the self-attention mechanism in the transformer in\nlinear time $O\\left( n \\right)$ instead of a quadratic time\n$O\\left( n^{2} \\right)$ in both time and space. In this paper, the\npublishers demonstrate that the self-attention mechanism can be\napproximated by a low-rank matrix.\n\nRecap\n\nThe Transformer is built upon the idea of Multi-Head Self-Attention\nwhich allows the model to jointly attend to information at different\npositions from different representation sub-spaces. The multi-head\nself-attention is defined as:\n\n\\[\\text{MultiHead}\\left( Q,\\ K,\\ V \\right) = Concat\\left( \\text{head}_{1},...\\text{head}_{h} \\right)\\ W^{O}\\]\n\nWhere $Q,\\ K,\\ V \\in \\mathbb{R}^{n \\times d_{m}}$ are input embedding\nmatrices, $n$ is sequence length, $d_{m}$ is the embedding dimension,\nand $h$ is the number of heads. Each head is defined as:\n\n\\[\\text{head}_{i} = \\text{Attention}\\left( QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V} \\right) = \\text{softmax}\\left\\lbrack \\frac{QW_{i}^{Q}\\left( KW_{i}^{K} \\right)^{T}}{\\sqrt{d_{k}}} \\right\\rbrack VW_{i}^{V}\\]\n\nWhere $W_{i}^{Q},W_{i}^{K} \\in \\mathbb{R}^{d_{m} \\times d_{k}}$,\n$W_{i}^{V} \\in \\mathbb{R}^{d_{m} \\times d_{v}}$, and\n$W_{i}^{O} \\in \\mathbb{R}^{\\text{hd}_{v} \\times d_{m}}$\nare learned matrices and $d_{k},d_{v}$ are the hidden dimensions of the\nprojection sub-spaces. The term highlighted in red is the self-attention\nwhich is also called “context mapping matrix” denoted\n$P \\in \\mathbb{R}^{n \\times n}$. Usually, we set $d_{k}$\nand $d_{v}$ to be the same size $d$.\n\nThe Transformer uses $P$ to capture the input context for a given token,\nbased on a combination of all tokens in the sequence. However, computing\n$P$ is expensive. It requires multiplying two $n \\times d$ matrices,\nwhich is $O\\left( n^{2} \\right)$ in time and space complexity. This\nquadratic dependency on the sequence length has become a bottleneck for\nTransformers.\n\nSelf-Attention is Low Rank\n\nBased on some analysis including singular value decomposition, the\ndistributional\n\nJohnson–Lindenstrauss lemma (JL for short), and the\nEckart–Young–Mirsky Theorem they found out that the self-attention\nmatrix $P$ can be approximated to:\n\n\\[P \\approx \\sum_{i = 1}^{k}{\\sigma_{i}u_{i}v_{i}^{T}} = \\begin{bmatrix}\n\\  &amp;amp; \\  &amp;amp; \\  \\\\\nu_{1} &amp;amp; \\text{...} &amp;amp; u_{k} \\\\\n\\  &amp;amp; \\  &amp;amp; \\  \\\\\n\\end{bmatrix}\\text{diag}\\begin{Bmatrix}\n\\sigma_{1} &amp;amp; \\text{...} &amp;amp; \\sigma_{k} \\\\\n\\end{Bmatrix}\\begin{bmatrix}\n\\  &amp;amp; v_{1} &amp;amp; \\  \\\\\n\\  &amp;amp; \\vdots &amp;amp; \\  \\\\\n\\  &amp;amp; v_{k} &amp;amp; \\  \\\\\n\\end{bmatrix}\\]\n\nwhere $\\sigma_{i}$ , $u_{i}$ and $v_{i}$ are the $i$ largest singular\nvalues and their corresponding singular vectors. This performs the\nself-attention is $O\\left( \\text{nk} \\right)$ time complexity.\n\nIn the paper, they optimized the operation from\n$O\\left( \\text{nk} \\right)$ to $O\\left( n \\right)$ by projecting two\nlinear projection matrices $E_{i},F_{i} \\in \\mathbb{R}^{n \\times k}$\nwhen computing key and value. So, the self-attention mechanism becomes:\n\n\n    \n\n\nThe method included a lot of maths and a lot of theorems. It didn’t make\nsense to me.\n\nTO BE CONTINUED!\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Very Deep Transformer",
        "url"       : "/machine-translation/Very_Deep_Transformer",
        "date"      : "18/08/2020",
        "content": "Using a simple yet effective initialization technique that stabilizes\ntraining, researchers at Microsoft Research were able to build very deep\nTransformer\nmodels with up to 60 encoder layers. These models were explored in this\npaper published in 2020: Very Deep Transformers for Neural Machine\nTranslation. The official code\nfor this paper can be found in the following GitHub repository:\nexdeep-nmt.\n\n\n    \n\n\n\n  Note:\nI suggest reading the\nTransformer\npost first before going on especially the part about “Layer\nNormalization”.\n\n\nThe capacity of a neural network influences its ability to model complex\nfunctions. Very deep neural network models have proved successful in\ncomputer vision such as ResNet-101 and Inception networks. In NMT,\nresearchers from Google have shown in this paper: Training Deeper\nNeural Machine Translation Models with Transparent\nAttention (published in 2018)\nthat it is difficult to train deep Transformers whose encoder depth is\nincreased beyond 12 layers as shown in the following table:\n\n\n    \n\n\nIn the previous table, * indicates that a model failed to train. As we\ncan see, Transformers beyond 12 layers all failed to train. And that’s\ndue to gradient vanishing; since the error signal needs to traverse\nalong the depth of the encoder.\n\nThat’s why models with “Transparent” attention were able to train.\n“Transparent Attention” behaves akin to creating trainable weighted\nresidual connections along the encoder depth, allowing the dispersal of\nerror signal simultaneously over encoder depth and time.\n\nIn this paper, they are re-investigating the deeper Transformer models\nbut with a new initialization technique called ADMIN which remedies the\nproblem. This enables training Transformers that are significantly deep.\n\nADMIN Initialization\n\nThe ADMIN initialization technique was proposed in 2020 by researchers\nfrom Microsoft and published in this paper: Understanding the\ndifficulty of training\ntransformers. This technique\nreformulates the layer-normalization equation. First, let’s recap the\nlayer normalization formula used in the\nTransformer\nmodel:\n\n\\[x_{i} = \\text{LayerNom}\\left( x_{i - 1} + f\\left( x_{i - 1} \\right) \\right)\\]\n\nWhere $f$ represents either the attention function or the feed-forward\nsub-layer. This process repeats $2 \\times N$ times for a $N$-layer\nencoder and $3 \\times M$ times for a $M$-layer decoder. ADMIN\nreformulates this equation by using a constant vector $\\omega_{i}$ that\nis element-wise multiplied to $x_{i - 1}$ in order to balance the\ncontribution against $f\\left( x_{i - 1} \\right)$:\n\n\\[x_{i} = \\text{LayerNom}\\left( x_{i - 1}.\\omega_{i} + f\\left( x_{i - 1} \\right) \\right)\\]\n\nADMIN initialization method is effective in ensuring that training does\nnot diverge, even in deep networks. It involves two phases:\n\n\n  \n    Profiling Phase: At the profiling phase, we follow these steps:\n\n    \n      \n        We randomly initialize the model parameters and we set $\\omega_{i} = 1$.\n      \n      \n        Then, we and perform one step forward pass.\n      \n      \n        Then, compute the variance of the residual output at each layer:\n      \n    \n  \n\n\n\\[V\\text{ar}\\left\\lbrack f\\left( x_{i - 1} \\right) \\right\\rbrack\\]\n\n\n  \n    Training Phase: At the training phase, we follow these steps:\n\n    \n      We fix $\\omega_{i}$ to be:\n    \n  \n\n\n\\[\\omega_{i} = \\sqrt{\\sum_{j &amp;lt; i}^{}{V\\text{ar}\\left\\lbrack f\\left( x_{i - 1} \\right) \\right\\rbrack}}\\]\n\n\n  \n    Then, train the model like normal.\n  \n  \n    After training is finished, $\\omega_{i}$ can be removed to recover the\nstandard Transformer architecture.\n  \n\n\nThe following figure shows the learning curve of 60L-12L Transformer\nwhen initialized with the default initialization once and with ADMIN\nonce. As we can see, the default initialization has difficulty\ndecreasing the training perplexity; its gradients hit NaN, and the\nresulting model is not better than a random model.\n\n\n    \n\n\nExperiments\n\nExperiments were conducted using Transformers with 512-dim word\nembedding, 2048 feed-forward model size, and 8 heads on standard WMT’14\nEnglish-French (36 Million) dataset using 40k subword vocabulary, and\nEnglish-German (4.5 Million) dataset using 32k subword vocabulary. They\nused max tokens of 3584 in each batch. They used RAdam optimizer with\ntwo configurations:\n\n\n  \n    French-English: 8000 warm-up steps, 50 max epochs, and 0.0007 as\nlearning rate.\n  \n  \n    German-English: 4000 warm-up steps, 50 max epochs, and 0.001 as\nlearning rate.\n  \n\n\nThe following table shows the test results on WMT’14 benchmarks, in\nterms of TER (T↓), METEOR (M↑), and BLEU. ∆ shows difference in BLEU\nscore against baseline 6L-6L. As we can see, 60L-12L ADMIN outperforms\nall other models and achieves new state-of-the-art benchmark results on\nWMT14 English-French (43.8 BLEU and 46.4 BLEU with back-translation) and\nWMT14 English-German (30.1 BLEU):\n\n\n    \n\n\nThe following figure shows the BLEU score over multiple sentence length\nranges of 6L-6L default Transformer versus 60L-12L ADMIN transformer\nwhich indicates that Very Deep Transformer shows progress over all\nsentence lengths.\n\n\n    \n\n\nSame results can be seen in the following figure when considering the\nword frequency. As we can see, Very Deep Transformers improve\ntranslation of low frequency and high frequency words as well:\n\n\n    \n\n\nAlso, they experimented with different number of encoder and decoder\nlayers and results are shown in the following table where (+) means the\nrow outperforms the column, (-) means under-performs, and (=) means no\nstatistically significant difference.\n\n\n    \n\n\nThe pairwise comparison of models shown that deeper encoders are more\nworthwhile than deeper decoders.\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Hallucination",
        "url"       : "/machine-translation/hallucination",
        "date"      : "14/04/2021",
        "content": "As NMT systems are built on deep learning methodology which means that\nthey exhibit both the strengths and weaknesses of the approach. For\nexample, NMT systems make the best use of very large datasets but on the\nother hand they are poorly understood. For example, in many commercial\ntranslation systems, entering repeated words many times occasionally\nresults in strange translations like this one from Vice’s blog\npost:\n\n\n    \n\n\nThis phenomenon was named “Hallucination” by researchers at Google AI\nwhen writing their paper: Hallucinations in Neural Machine\nTranslation published in 2018. They\ncalled it “Hallucination” since the output is untethered from the input. The\npaper “The Curious Case of Hallucinations in Neural Machine Translation\n” published in 2021 by Microsoft has\ncategorized the “Hallucination” phenomenon into two categories:\n\n\n  \n    Hallucinations under Perturbations (HP)\n  \n  \n    Natural Hallucinations (NH)\n  \n\n\nHallucination Under Perturbation (HP)\n\nThis type of hallucination was first discussed by Hallucinations in\nNeural Machine Translation\npaper which was published in 2018 by Google AI. In this paper, the\nresearchers were able to reproduce hallucination by just adding a single\ntoken to the input source sentence. They called this process\n“perturbation”. And they defined the hallucination as a translation\nof a perturbed input sentence that has almost no words in common with\nthe translation of the unperturbed sentence. Hallucination Under\nPerturbation can be categorized into four types:\n\n\n  Grammatically correct output that bears no relation to the input\ntext:\n\n\n\n    \n\n\n\n  Ungrammatical output with oscillatory structure:\n\n\n\n    \n\n\n\n  Output that remains largely in the source language:\n\n\n\n    \n\n\n\n  Output that jumps to the end of the sequence:\n\n\n\n    \n\n\nUsing the GNMT\nmodel with a single layered bidirectional LSTM in the encoder and two\nlayered unidirectional LSTM in the decoder plus the attention mechanism,\nBPE encoded vocabulary, and the following algorithm, they were able to\nproduce hallucination in $73\\%$ of sentences in WMT De→En 2016 test set\nusing a greedy decoding and $48\\%$ using beam search decoding:\n\n\n    \n\n\nThe algorithm goes as follows:\n\n\n  \n    First, they split tokens into several types: common (100 most common\ntokens), rare (100 least common tokens), mid-frequency tokens\n(randomly sampled 100 tokens from the remaining tokens), and\npunctuation tokens.\n  \n  \n    Then, they used an adjusted BLEU score to evaluate the model’s\nhypotheses. They adjusted the BLEU score by re-weighting the n-grams\nin the BLEU score computation to favor having any words in common\nbetween the two sentences (1.0 for one-grams and 0.8 for bi-grams\nand disregarded other n-grams).\n  \n  \n    According to the paper, poor translations tend to have adjusted BLEU\nscore $\\leq 0.09$. So, they discarded them and kept good\ntranslations. The following are few examples on how the adjusted\nBLEU scores change by changing the translation quality:\n  \n\n\n\n    \n\n\n\n  \n    Then, each source sentence in the dataset -that had good translation- is\nperturbed by inserting a token at one of the following positions:\nbeginning, end,\nsecond to the end, or randomly in the\nmiddle.\n  \n  \n    Then, the model tries to translate this perturbed sentence. If the\nadjusted BLEU score of the new translation is $\\leq 0.01$, this\nsentence is considered a “hallucination”.\n  \n\n\nThey studied the effect of different hyper-parameters (canonical model with\ngreedy decoding, beam search decoding, 512 &amp;amp; 1024 hidden units, 16k &amp;amp; 32k BPE\ncodes) respectively affected hallucination percentages. They found that\nbeam search and increasing the vocabulary size to 32K BPE codes did\nsignificantly lower the mean percentage of hallucinations; that’s why they\nhave a red star ☺.\n\n    \n\n\nThey also studied how different types of perturbations (using common tokens\nonly, using mid-frequency tokens only, using rare tokens only, at the beginning\nof the sentence, at the end of the sentence, at the second-end of the sentence,\nrandomly in the middle of the sentence) respectively impacted the hallucination\npercentage of the canonical model. As seen from the following figure,\nadding a perturbing token to the beginning of the input\nsequence produces the least hallucinations.\n\nReduce HP\n\nLater in the same paper, they investigated the effect of different\nmethodologies on the hallucination percentage. They used the following\ntechniques along with the canonical GNMT model introduced earlier:\n\n\n  \n    DO: Dropout regularization of $0.1$ in all feed-forward layers.\n  \n  \n    L2E: L2 regularization on embeddings.\n  \n  \n    L2R: L2 regularization on recurrent weights.\n  \n  \n    L2: L2 regularization on all weights\n  \n  \n    DA: Data augmentation on the training data by perturbing all\ntraining sentences. This doubled the training set.\n  \n  \n    TDIS: “Tied Decoder with Initial State” is a dynamical\nregularization method where the initial state of the decoder was\ntied to last step of the encoder.\n  \n  \n    CFN: “Chaos-free network” is a dynamical regularization method\nwhich cannot produce chaos. This was done by replacing the LSTM\ncells with CFN cells with 256 hidden units.\n  \n\n\n\n  Note:\nAll L2 regularization used weighting hyper-parameters of $1 \\times 10^{- 4}$\nand $1 \\times 10^{- 5}$.\n\n\nAccording to the following graph, the canonical model with the augmented\ntraining data (DA) has the least percentage of\nhallucination. On the other hand, the CFN model resulted in a\nsignificant increase in the hallucination percentage:\n\n\n    \n\n\nSince Data augmentation (DA) resulted in the least hallucination percentage,\nthey studied it further with different perturbation methods. They found out\nthat DA model would be less prone to hallucinate when perturbed with\ntypes of tokens or positions it had been trained against.\n\n\n    \n\n\nAnd according to the previous graph, the most reliable way to produce\nhallucination is to append a common token at the beginning of the source\nsentence.\n\n\n  Very Important Note:\nThey studied the Transformer\n model (TR)\nimplemented in the Tensor2Tensor\nlibrary with tiny-configuration (2 hidden layers, 128 hidden size, 512 filter\nsize, and 4 heads) and had a greedy BLEU score of 17.5, which is a little lower\nthan our GNMT models. They found out that the transformer model hallucinates\nsignificantly less than the GNMT canonical model. However, it still can be\nperturbed to hallucinate on average $15\\%$ of the time.\n\n  \n   \n\n\n\nReason Behind HP\n\nIn the “Hallucinations in Neural Machine\nTranslation” paper, the\nresearchers didn’t provide an answer to “why this perturbation generates\nhallucination?”. Luckily, the paper “The Curious Case of Hallucinations\nin Neural Machine Translation”\npublished in 2021 has a hypothesis that explains the reason which is\n“Samples that have high memorization values in the NMT model\nare most likely to generate hallucinations when perturbed”.\n\nTo validate this hypothesis, they used the Memorization Value Estimator\n(MVE) algorithm proposed by this paper: “What neural networks memorize\nand why: Discovering the long tail via influence\nestimation”\nto calculate the memorization values of sentences. The MVE algorithm can\nbe seen down below where $S$ is IWSLT-2014 De-En dataset with $n = 160k$\nsamples, $A$ is a Transformer model, $t = 10$, $M$ is BLEU:\n\n\n    \n\n\nThen, they perturbed the highest sentences that values into a certain\nrange of memorization values. And Since, each input sentence can appear\nin the hallucinated samples multiple times, they reported both Unique\nand Total number of Hallucinations generated:\n\n\n    \n\n\nThe previous figure shows that as the memorization values increase, the number\nof unique (Unique HP) as well as total hallucinations (Total HP) keeps\nincreasing as well, demonstrating a strong positive correlation\nbetween hallucination frequency and memorization values.\n\nNatural Hallucination (NH)\n\nNatural Hallucinations is the type of hallucination that happens to an\nunperturbed input source sequence where the NMT model tend to generate\ntranslations that are severely inadequate. Natural hallucinations can be\ncategorized into two categories:\n\n\n  Detached Hallucinations: A fluent but completely inadequate\ntranslation.\n\n\n\n    \n\n\n\n  Oscillatory Hallucinations: A translation that contains\nrepeating n-grams.\n\n\n\n    \n\n\nReason Behind NH\n\nIn the same paper, the researchers provided a hypothesis to explain why\nNatural Hallucinations exist. They suggested that Corpus-level\nnoise patterns (invalid source-target pairs) dictate the type of natural\nhallucinations generated by the NMT model.\n\nTo validate this hypothesis, they needed to create corpus-level noised\ndata. To be able to do that, they first constructed IRS (invalid\nreference set) which is a small set of 21 invalid source-target pairs\nchosen randomly. Then, they created four different corpus-level noised\ndata; each is 21k samples of invalid source-target sentence pairs. By\ninvalid, I mean the target sentence is not a valid translation for the\nsource sentence.\n\nThe idea behind designing the following corpus-level noise patterns is\nthat they are very common in web-based corpora due to the widespread of\nautomatic mining algorithms. Also, each noise pattern challenges\nsomething different in the translation model:\n\n\n\n    \n        \n            Noise Pattern\n            Description\n            Challenges\n        \n    \n    \n        Unique-Unique (U-U)\n        21K random unique source sentences; each sentence is paired with\n        an unrelated unique random target sentence.\n        Both encoder and decoder are required to produce representations\n        that are vastly different than the one seen before in the data.\n    \n    \n        Repeat-Repeat (R-R)\n        21 unique source sentences from IRS, and pair each with unrelated\n        unique random target sentence from IRS, and repeat each such pair 1000\n        times.\n        Challenges the memorization of the model.\n    \n    \n        Repeat-Unique (R-U)\n        The same 21 random unique source sentences as RR, repeat each 1000\n        times, and pair each repeat with unrelated unique random target\n        sentence from WMT.\n        The decoder is required to generate unique translations for the\n        same sources, thereby encouraging decoder instability.\n    \n    \n        Unique-Repeat (U-R)\n        21 random unique target sentences from the IRS. Each target\n        sentence is repeated 1000 times, each repeat is paired with an\n        unrelated unique random source sentence from WMT.\n        The encoder is required to produce the same representations for\n        unique inputs.\n    \n\n\n\nThen, they trained 5 transformer models. Each model is six layers with\n$512$ embedding size, $1024$ FFN layer dimension, and $4$ attention\nheads. Training was done on the IWSLT En-De 2014 corpus (160K samples) +\ncorpus-level noised data leaving one model without noise. Then, they\nanalyzed the resulting models in terms of the generated translations on\nthree different sets:\n\n\n  \n    IWSLT: The IWSLT De-En 2014 test set.\n  \n  \n    Invalid reference set (IRS): The 21 unique detached\nsource-target sentence pairs.\n  \n  \n    Valid reference set (VRS): The same 21 source sentences as the\nIRS, but now they are paired with their valid (correct) references.\n  \n\n\n\n  Note:\nIRS set exists in four corpus-noise patterns: it is contained in the\nRR training data, its source sentences are present in the RU\ntraining data and its target sentences are present in the UR\ntraining data. The main purpose of evaluating models on IRS is to\nmeasure memorization of the overlapping source/targets.\n\n\nUsing the above three evaluation sets, they computed the following metrics:\n\n\n  \n    BLEU: The BLEU score for each evaluation set.\n  \n  \n    IRS-NH: The percentage of natural hallucinations (NH) (manually\nidentified) in the translations of the IRS.\n  \n  \n    IRS-OH: The percentage of oscillatory hallucinations (OH)\n(manually identified) in the translations of the IRS.\n  \n  \n    IRS-Repeats: The percentage of the hallucinations that exactly\nmatch a reference in the training data.\n  \n\n\nThe results can be seen in the following table where the boxes marked with ‘-‘\nare the cases that didn’t convey any useful information:\n\n\n    \n\n\nFrom the previous table, we can say the following:\n\n\n  \n    The Test-BLEU is not greatly affected by the corpus-level noise.\n  \n  \n    The IRS-BLEU indicates that the model memorized the IRS data when\nR-R noise pattern is used, which makes total sense.\n  \n  \n    The U-R and U-R noise patterns lead to high natural hallucinations.\n  \n  \n    The R-U noise pattern leads to a very high percentage of oscillatory\nhallucinations.\n  \n\n\nThe past hallucination stats were done manually. This is not scalable,\nthat’s why they proposed an automatic NH detection algorithm that can\nwork on scale. The algorithm works at the corpus-level:\n\n\n    \n\n\nThey used the past algorithm to measure the effect of Knowledge\nDistillation (KD) and Back-translation (BT) techniques on natural\nhallucinations. And they found out that:\n\n\n  \n    For both KD and BT, U-R noise pattern leads to severe amplifications\nwhile R-R causes the least hallucinations.\n  \n  \n    For KD, all noise patterns lead to increase in natural\nhallucination.\n  \n  \n    For BT, U-R and U-U noise patterns lead to large number of repeated\ngenerations.\n  \n\n\nThe official code and datasets used in the past experiments can be found in the\nfollowing GitHub repository: hallucinations.\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "REDER",
        "url"       : "/machine-translation/REDER",
        "date"      : "07/05/2021",
        "content": "REDER stands for “Reversible Duplex Transformer” which is a\nTransformer\nmodel where its both ends can simultaneously input and output a distinct\nlanguage thus enabling reversible machine translation by simply flipping\nthe input and output ends. REDER was proposed by ByteDance AI lab in\n2022 and published in their paper: Duplex Sequence-to-Sequence Learning\nfor Reversible Machine\nTranslation. The official code\nfor this paper can be found in the following GitHub repository:\nREDER.\n\n\n    \n\n\nIn this paper, they proposed an alternative approach for utilizing\nbidirectional supervisions called duplex sequence-to-sequence\nlearning which is better than the typical sequence-to-sequence learning\nwhere the encoder learns to represent the source language and the\ndecoder learns to generate the target language, and multi-task learning\nwhere the encoder learns to represent multiple language simultaneously\nwhile the decoder learns to generate multiple language simultaneously.\n\n\n    \n\n\n\n  Note:\nThe name “Duplex” comes from telecommunications and computer networking\nwhere the “simplex” communication means the communication channel is\nunidirectional while the “duplex” communication is bidirectional.\n\n\nWe should note that building duplex seq2seq networks is non-trivial; and\nthat’s because of the following reasons:\n\n\n  \n    Reversibility:\nTypical encoder-decoder networks such as Transformers are\nirreversible, i.e., one cannot just obtain its inverse function by\nflipping the same encoder-decoder network.\n  \n  \n    Homogeneity:\nThe decoder works autoregressively, while the encoder works in a\nnon-autoregressive manner.\n  \n\n\nThat’s why REDER is designed without explicit encoder and decoder division\nintroducing two solutions (Reversible Duplex Transformer layers and\nthe Symmetric Network architecture) to address the reversibility and\nhomogeneity issues respectively as we are going to see next.\n\nReversible Duplex Transformer Layer\n\nThe following figure shows the overall architecture of REDER where we\ncan see that it has two ends: the source end (left) and the target end\n(right). As illustrated, REDER is composed of a series of identical\nReversible Duplex Transformer layers. Since the network has two ends, it\ndefines a forward mapping function\n$f_{\\theta}^{\\rightarrow}:X \\rightarrow Y$ and a reverse mapping\nfunction $f_{\\theta}^{\\leftarrow}:Y \\rightarrow X$ that satisfy the\nfollowing reversibility:\n$f_{\\theta}^{\\rightarrow} = \\left( f_{\\theta}^{\\leftarrow} \\right)^{- 1}$\nand\n$f_{\\theta}^{\\leftarrow} = \\left( f_{\\theta}^{\\rightarrow} \\right)^{- 1}$.\n\n\n    \n\n\nThe Reversible Duplex Transformer layer is adopted from the Reversible\nNetworks (RevNet) where each layer is composed of two sub-layers: a\nmulti-head self-attention (SAN) and a feed-forward network (FFN) with a\nspecial reversible design to ensure a duplex behavior.\n\nGiven a sentence pair $\\left( x,y \\right)$, the inner representations of\neach layer in divided into two directions:\n\n\n  The forward direction $\\overrightarrow{H}_l$ which is the\nconcatenation of two halves\n$\\left\\lbrack \\overrightarrow{H}_l^{\\left( 1 \\right)},\\ \\overrightarrow{H}_l^{\\left( 2 \\right)} \\right\\rbrack$:\n\n\n\\[{\\overrightarrow{H}}_{l} = F_{l}\\left( \\left\\lbrack {\\overrightarrow{H}}_{l - 1}^{\\left( 1 \\right)},\\ {\\overrightarrow{H}}_{l - 1}^{\\left( 2 \\right)} \\right\\rbrack \\right) = \\left\\lbrack {\\overrightarrow{H}}_{l}^{\\left( 1 \\right)},\\ {\\overrightarrow{H}}_{l}^{\\left( 2 \\right)} \\right\\rbrack\\]\n\n\\[{\\overrightarrow{H}}_{l}^{\\left( 1 \\right)} = {\\overrightarrow{H}}_{l - 1}^{\\left( 1 \\right)} + \\text{SAN}\\left( {\\overrightarrow{H}}_{l - 1}^{\\left( 2 \\right)} \\right)\\]\n\n\\[{\\overrightarrow{H}}_{l}^{\\left( 2 \\right)} = {\\overrightarrow{H}}_{l - 1}^{\\left( 2 \\right)} + \\text{FFN}\\left( {\\overrightarrow{H}}_{l}^{\\left( 1 \\right)} \\right)\\]\n\n\n  The backward direction $\\overleftarrow{H}_l$ which is the\nconcatenation of two halves\n$\\left\\lbrack \\overleftarrow{H}_l^{\\left( 1 \\right)},\\ \\overrightarrow{H}_l^{\\left( 2 \\right)} \\right\\rbrack$:\n\n\n\\[{\\overleftarrow{H}}_{l - 1} = F_{l}^{- 1}\\left( \\left\\lbrack {\\overleftarrow{H}}_{l}^{\\left( 1 \\right)},\\ {\\overleftarrow{H}}_{l}^{\\left( 2 \\right)} \\right\\rbrack \\right) = \\left\\lbrack {\\overleftarrow{H}}_{l - 1}^{\\left( 1 \\right)},\\ {\\overleftarrow{H}}_{l - 1}^{\\left( 2 \\right)} \\right\\rbrack\\]\n\n\\[{\\overleftarrow{H}}_{l - 1}^{\\left( 2 \\right)} = {\\overleftarrow{H}}_{l}^{\\left( 2 \\right)} + FFN\\left( {\\overleftarrow{H}}_{l}^{\\left( 1 \\right)} \\right)\\]\n\n\\[{\\overleftarrow{H}}_{l - 1}^{\\left( 1 \\right)} = {\\overleftarrow{H}}_{l}^{\\left( 1 \\right)} + \\text{SAN}\\left( {\\overleftarrow{H}}_{l - 1}^{\\left( 2 \\right)} \\right)\\]\n\n\n  Note:\nThe attention mechanism used in this paper is a relative self-attention\nproposed in the “Self-attention with relative position\nrepresentations” paper,\ninstead of the original one proposed in the Transformer paper.\n\n\nSymmetric Network\n\nAs discussed earlier, using reversible duplex transformer layers solves\nthe reversibility problem. Now, let’s see how to solve the Homogeneity\nproblem. To achieve homogeneous computations, the model must satisfy the\nfollowing cycle consistency:\n\n\\[\\forall x \\in X:f_{\\theta}^{\\leftarrow}\\left( f_{\\theta}^{\\rightarrow}\\left( x \\right) \\right) = x,\\ \\ \\ \\ \\ \\ \\ \\ \\forall y \\in Y:f_{\\theta}^{\\rightarrow}\\left( f_{\\theta}^{\\leftarrow}\\left( y \\right) \\right) = y\\]\n\nOne solution is to make the network symmetric which can be done as\nfollows: given an network of $L$ layers, the layers starting from $1$\ntill $\\frac{L}{2}$ will be in reverse form, whereas the layers starting\nfrom $\\frac{L}{2} + 1$ to $L$ be in regular form:\n\n\\[f_{\\theta}^{\\rightarrow}\\left( x \\right) \\triangleq F_{1}^{- 1} \\circ \\text{...} \\circ F_{\\frac{L}{2}}^{- 1} \\circ F_{\\frac{L}{2} + 1} \\circ \\text{...} \\circ F_{L}\\left( x \\right)\\]\n\n\\[f_{\\theta}^{\\leftarrow}\\left( y \\right) \\triangleq F_{L} \\circ \\text{...} \\circ F_{\\frac{L}{2} + 1} \\circ F_{\\frac{L}{2}}^{- 1} \\circ \\text{...} \\circ F_{1}^{- 1}\\left( y \\right)\\]\n\nAnd this property means that the REDER model works in a fully\nnon-autoregressive fashion in both reading and generating sequences.\nSpecifically, given an input sequence\n$x = \\left( x_{1},\\ …x_{n} \\right)$, the i-th element of REDER’s input\n$x_{i}$ is the concatenation of two copies of the embedding of $x_{i}$\nas shown below:\n\n\\[{\\overrightarrow{H}}_{0,i} = \\left\\lbrack {\\overrightarrow{H}}_{0,i}^{\\left( 1 \\right)},\\ {\\overrightarrow{H}}_{0,i}^{\\left( 2 \\right)} \\right\\rbrack = \\left\\lbrack e\\left( x_{i} \\right),\\ e\\left( x_{i} \\right) \\right\\rbrack\\]\n\nOnce the forward computation is done, the concatenation of the output of\nthe model \n$\\left\\lbrack {\\overrightarrow{H}}_{L,i}^{\\left( 1 \\right)},\\ {\\overrightarrow{H}}_{L,i}^{\\left( 2 \\right)} \\right\\rbrack$\nserves as the representations of target translation after a softmax\noperation is performed to measure the similarity between itself and the\nconcatenated embedding of ground-truth reference\n$\\left\\lbrack e\\left( y_{i} \\right),\\ e\\left( y_{i} \\right) \\right\\rbrack$\nto obtain the prediction probability:\n\n\\[p\\left( y_{i} \\middle| x;\\ \\theta \\right) = \\text{softmax}\\left( \\frac{1}{2}{\\left\\lbrack e\\left( y_{i} \\right),\\ e\\left( y_{i} \\right) \\right\\rbrack\\ }^{T}.\\left\\lbrack {\\overrightarrow{H}}_{L,i}^{\\left( 1 \\right)},\\ {\\overrightarrow{H}}_{L,i}^{\\left( 2 \\right)} \\right\\rbrack \\right)\\]\n\nObjective Function\n\nGiven a parallel dataset\n$\\mathcal{D}_{x,y} = \\left( x^{\\left( n \\right)},\\ y^{\\left( n \\right)} \\right)_{n = 1}^{N}$\n,the final objective of REDER is to minimize the following objective\nfunction:\n\n\\[\\mathcal{L}\\left( \\theta;\\mathcal{D}_{x,y} \\right) = \\sum_{n = 1}^{N}\\left( - \\log\\left( p_{\\text{ctc}}\\left( y^{\\left( n \\right)} \\middle| x^{\\left( n \\right)};\\theta \\right) \\right) - log\\left( p_{\\text{ctc}}\\left( x^{\\left( n \\right)} \\middle| y^{\\left( n \\right)};\\theta \\right) \\right) + \\lambda_{\\text{fba}}\\mathcal{L}_{\\text{fba}}\\left( x^{\\left( n \\right)} \\middle| y^{\\left( n \\right)};\\theta \\right) + \\lambda_{\\text{fba}}\\mathcal{L}_{\\text{fba}}\\left( y^{\\left( n \\right)} \\middle| x^{\\left( n \\right)};\\theta \\right) + \\lambda_{\\text{cc}}\\mathcal{L}_{\\text{cc}}\\left( x^{\\left( n \\right)};\\theta \\right) + \\lambda_{\\text{cc}}\\mathcal{L}_{\\text{cc}}\\left( y^{\\left( n \\right)};\\theta \\right) \\right)\\]\n\nWhere:\n\n\n  \n    $p_{\\text{ctc}}\\left( y^{\\left( n \\right)} \\middle| x^{\\left( n \\right)};\\theta \\right)$\nis the CTC loss of the forward direction, while\n$p_{\\text{ctc}}\\left( x^{\\left( n \\right)} \\middle| y^{\\left( n \\right)};\\theta \\right)$\nis the CTC loss of the backward direction.\n  \n  \n    $\\mathcal{L}_{\\text{fba}}\\left( x^{\\left( n \\right)} \\middle| y^{\\left( n \\right)};\\theta \\right)$\nis the forward-backward agreement of the forward direction, while\n$\\mathcal{L}_{\\text{fba}}\\left( y^{\\left( n \\right)} \\middle| x^{\\left( n \\right)};\\theta \\right)$\nis the forward-backward agreement of the backward direction. This\nloss function is calculated via the following formula knowing that\n$sg()$ is the stop-gradient operation:\n  \n\n\n\\[\\mathcal{L}_{\\text{fba}}\\left( x \\middl| y;\\theta \\right) = \\frac{1}{L}\\sum_{l = 1}^{L}{1 - \\cos\\left( {\\overrightarrow{H}}_{l},\\ \\text{sg}\\left( {\\overleftarrow{H}}_{l} \\right) \\right)}\\]\n\n\n  $\\mathcal{L}_{\\text{cc}}\\left( x^{\\left( n \\right)};\\theta \\right)$\nis the cycle consistency loss of the forward direction, while\n$\\mathcal{L}_{\\text{cc}}\\left( y^{\\left( n \\right)};\\theta \\right)$\nis the cycle consistency of the backward direction. This loss\nfunction is calculated via the following formula:\n\n\n\\[\\mathcal{L}_{\\text{cc}}\\left( x;\\theta \\right) = distance\\left( x,\\ f_{\\theta}^{\\leftarrow}\\left( f_{\\theta}^{\\rightarrow}\\left( x \\right) \\right) \\right)\\]\n\n\n  $\\lambda_{\\text{fba}}$ and $\\lambda_{\\text{cc}}$ are the\ncoefficients of the auxiliary losses.\n\n\n\n  Note:\nTraining REDER is done in a a two-stage training setting, where we first train\nREDER without using any auxiliary losses until a predefined number of updates,\nand then activate the additional losses and continue training the model until\nconvergence.\n\n\nExperiments\n\nIn the paper, they evaluated REDER on two standard translation\nbenchmarks: WMT14 English↔German (4.5M training pairs), and WMT16\nEnglish↔Romanian (610K training pairs). REDER was implemented using\nfairseq framework and it consists\nof $12$ stacked layers. The number of head is $8$, the model dimension\nis $512$, and the FFN is $2048$. For regularization, they used a dropout\nrate of $0.1$ and L2-regularization of $0.01$ and label smoothing with\n$\\epsilon = 0.1$. Also, they both $\\lambda_{\\text{fba}}$ and\n$\\lambda_{\\text{cc}}$ to $0.1$ for all experiments. All models were\ntrained for 300K updates with a batch size of approximately 64K tokens.\nFor scoring, they average the best 5 checkpoints to obtain the final\nmodel.\n\nThe following table shows the a comparison between REDER and other\nexisting models knowing that all NAT models were trained with Knowledge\nDistillation (KD), also “MTL” stands for “multitask learning” and “BT”\nstands for “back-translation”. (row 4 ∼ row 9) employ greedy decoding\nwhile (row 10 ∼ row 14) employ beam search decoding with beam size of\n20.\n\n\n    \n\n\nFrom the previous table we can see:\n\n\n  \n    REDER achieves competitive results compared with strong NAT\nbaselines.\n  \n  \n    Duplex learning has more potential than multitask learning and\nback-translation.\n  \n  \n    REDER performs on par with autoregressive Transformer\n  \n\n\nLike other NAT approaches, they found out that REDER heavily relies\non knowledge distillation. The following table shows that the\naccuracy of REDER without KD significantly drops:\n\n\n    \n\n\nTo examine whether REDER can generalize to distant languages, they\nconducted experiments on WMT20 English↔Japanese dataset. The\nfollowing table shows that REDER can achieve very close results\ncompared with Auto-regressive Transformer (AT) in such a large-scale\nscenario with distant languages:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Luna: Linear Attention Mechanism",
        "url"       : "/machine-translation/Luna",
        "date"      : "03/06/2021",
        "content": "Luna stands for “Linear Unified Nested Attention” which is a novel\nattention mechanism that yields linear time and space complexity as\nopposed to standard attention mechanism proposed in the\nTransformer\narchitecture that yields quadratic time and space complexity. Luna was\nproposed by FAIR in 2021 and published in the paper under the same name:\n“Luna: Linear Unified Nested\nAttention”. The official code\nfor this paper can be found in the following GitHub repository:\nfairseq-apollo.\n\n\n    \n\n\nAs compared to other attention mechanism proposed by different models,\nLuna achieves competitive or even better performance, while acquiring\nprominent gains of efficiency in both speed and memory as shown in the\nfollowing figure:\n\n\n    \n\n\nAttention Recap\n\nBefore getting into Luna details, let’s first recap the attention\nmechanism. The traditional attention mechanism is a function of two\nsequences: the query sequence $X \\in \\mathbb{R}^{n \\times d}$ with\nlength $n$ and the context sequence $C \\in \\mathbb{R}^{m \\times d}$ with\nlength $m$. And it outputs one sequence $Y \\in \\mathbb{R}^{n \\times d}$\nwith the same length as the query $X$:\n\n\\[Y = \\text{Attention}\\left( X,\\ C \\right) = \\text{softmax}\\left( \\frac{XW^{Q}\\left( CW^{K} \\right)^{T}}{\\sqrt{d}} \\right)CW^{V}\\]\n\nWhere $d$ is the embedding dimension, and\n$W^{Q},W^{K},W^{V} \\in \\mathbb{R}^{d \\times d}$ are three learnable\nparameters that project the input sequences into the space of query, key\nand value matrices: $Q = XW^{Q},\\ K = CW^{K},\\ V = CW^{V}$ respectively.\nEach head in the multi-head attention mechanism has different learnable\nparameters.\n\n\n  Note:\nIn self-attention mechanism, $X = C$, where both come from either the\nencoder or the decoder. In cross-attention, $X$ comes from the encoder\nand $C$ comes from the decoder.\n\n\nThe matrix\n$\\text{softmax}\\left( \\frac{XW^{Q}\\left( CW^{K} \\right)^{T}}{\\sqrt{d}} \\right) \\in \\mathbb{R}^{n \\times m}$\nis called the “attention matrix” which specifies the alignment\nscores between every pair of tokens in sequences of queries $X$ and\ncontexts $C$. Calculating the attention matrix takes\n$O\\left( \\text{nm} \\right)$ time and space, which is quadratic with\nrespect to the sequence length and becomes a significant bottleneck when\nprocessing long sequences.\n\n\n    \n\n\nThe other two key components of Transformer, besides attention, are\nfeed-forward networks (FFN) and layer normalization. Each Transformer\nlayer can be expressed as:\n\n\\[X_{A} = \\text{LayerNorm}\\left( \\text{Attention}\\left( X,\\ C \\right) + X \\right)\\]\n\n\\[X&#39; = \\text{LayerNorm}\\left( \\text{FFN}\\left( X_{A} \\right) + X_{A} \\right)\\]\n\nWhere $X’$ is the output of the transformer layer. Here, we used the\noriginal post-layer normalization architecture which places layer\nnormalization after the residual connection.\n\nLuna Attention\n\nThe key idea behind Luna attention is to decouple the attention function\nmentioned above into two nested attention operations, both of which have\nlinear efficiency:\n\n\n    \n\n\n\n  Pack Attention: Which packs the context sequence\n$C \\in \\mathbb{R}^{m \\times d}$ into a fixed-length sequence\n$Y_{P} \\in \\mathbb{R}^{l \\times d}$ with a fixed length $l$ using\nthe standard attention function with $P \\in \\mathbb{R}^{l \\times d}$\nas a query sequence (gonna explain where $P$ comes from in a\nsecond):\n\n\n\\[Y_{P} = \\text{Attention}\\left( P,\\ C \\right)\\]\n\n\n  Unpack Attention: unpacks the sequence\n$Y_{P} \\in \\mathbb{R}^{l \\times d}$ back to the length of the\noriginal query sequence $Y_{X} \\in \\mathbb{R}^{n \\times d}$ using\nthe same standard attention function:\n\n\n\\[Y_{X} = \\text{Attention}\\left( X,\\ Y_{P} \\right)\\]\n\nThe complexity of pack attention and the unpack attention is\n$O\\left( \\text{lm} \\right)$ and $O\\left( \\ln \\right)$ respectively which\nis linear with respect to $m$ and $n$ respectively.\n\nNow, the question is “how to get $P \\in \\mathbb{R}^{l \\times d}$?”. At\nthe first Luna layer, $P$ is created as a learnable positional\nparameter. At other following layers, $P$ is calculated via the\nfollowing formula:\n\n\\[P^{+} = \\text{LayerNorm}\\left( Y_{P} + P \\right)\\]\n\nNow, Luna layer, shown in the previous layer, is composed of the\nfollowing:\n\n\\[Y_{X},\\ Y_{P} = \\text{LunaAttention}\\left( X,\\ P,\\ C \\right)\\]\n\n\\[X_{A},\\ P_{A} = \\text{LayerNorm}\\left( Y_{X} + X \\right),\\ \\text{LayerNorm}\\left( Y_{P} + P \\right)\\]\n\n\\[X&#39;,\\ P&#39; = \\text{LayerNorm}\\left( \\text{FFN}\\left( X_{A} \\right) + X_{A} \\right),\\ P_{A}\\]\n\nExperiments\n\nTo evaluate Luna on sequence-to-sequence modeling, they evaluated it on\nWMT’14 English-German (EN→DE) machine translation dataset using BPE\nvocabulary of 37K subwords. The Luna models was closely following the\narchitecture of Transformer-base: 6 encoder and decoder layers with 8\nattention heads and model size of 512 and hidden size of 2048.\n\nUnlike the Transformer-base, Luna was trained using Apollo optimizer\nwith learning rate of 0.1, $\\beta = 0.9$ , and $\\epsilon = 1e^{- 4}$.\nFor learning rate scheduling, they applied linear warm up the first 1000\nsteps. After learning rate warm up, they decayed the learning rate at\nthe 300,000 and 450,000 steps by decay rate 0.1. Gradient clips with 1.0\nwere applied. And the dropout ratio are set to 0.1. The weight decay\nrate was set to $1e^{- 8}$.\n\nThe following table presents Luna results in comparison with Transformer\nmodels trained using Adam and Apollo optimizers along with Random\nFeature Attention (RFA) model. We can see that, Luna achieves similar\nresults to the Transformer model. Also, we note that Luna with softplus\nactivation function consistently outperforms ELU.\n\n\n    \n\n\n\n  Note:\nWhen saying Luna-16, we mean Luna where $l = 16$. Here, softplus and ELU\nactivation functions were used instead of the softmax in the attention\nmechanism.\n\n\nTo evaluate the effectiveness of Luna on long sequences, they trained\nLuna on the Long Range Arena (LRA) benchmark which consists of five\ntasks, each designed for the purpose of evaluating Transformer models\nunder the long-context (from 1K to 8K tokens). The following table shows\nthat Luna outperforms baseline models on three out of five tasks and\nperforms comparably with the best performed model on the other two\ntasks:\n\n\n    \n\n\nAlso, Luna was pre-training on Masked Language Modeling (MLM) objective\nand then fine-tuned on Natural Language Understanding downstream tasks\nand was found out to have very similar results in comparison with\nstate-of-the-art models such as\nBERT and\nRoBERTa:\n\n\n    \n\n\nLuna-128 (16GB) was pre-trained on 16GB of monolingual data collected\nfrom BookCorpus and English Wikipedia which is the same data used with\nBERT. Luna-128\n(160GB) was pre-trained on 160GB of monolingual data collected from the\nsame sources in addition to CC-News, OpenWebText, and Stories which is\nthe same data used with\nRoBERTa. The\nfollowing are the hyper-parameters used for pre-training Luna-128 (16GB\nand 160GB):\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Dr. NMT",
        "url"       : "/machine-translation/DrNMT",
        "date"      : "01/08/2021",
        "content": "DrNMT stands for “Discriminative Reranking for Neural\nMachine Translation” which is a re-ranking framework created by\nFacebook AI in 2021 and published in this paper: Discriminative\nReranking for Neural Machine\nTranslation. The\nofficial implementation for this paper can be found in this GitHub\nrepository:\nDrNMT.\n\nA re-ranking model is a model that is able to take a list of hypotheses\ngenerated by a baseline model and rank them based on a desired metric.\nIn this paper, DrNMT takes as input both the source sentence as well as\na list of hypotheses to output a ranked list of a desired metric, e.g.\nBLEU, over the n-best list.\n\nArchitecture\n\nDrNMT is a transformer architecture which takes as input the\nconcatenation of the source sentence $x$ and one of the hypothesis\nsentences generated by a NMT model $u \\in \\mathcal{U}(x)$. It outputs\nhigher scores for hypotheses of better quality based on a metric\n$\\mu(u,r)$ such as BLEU, where quality is measured with respect to a\nreference sentence $r$.\n\nThe architecture includes also position embeddings and language\nembeddings, to help the model represent tokens that are shared between\nthe two languages. The final hidden state corresponding to the start of\nsentence token $\\left\\langle s \\right\\rangle$ is a one hidden\nfeed-forward network with $d$ hidden units with\n$\\text{tanh}$ activation. It serves as the joint representation for\n$(x, u)$:\n\n\n    \n\n\nGiven a source sentence $x$, $n$ of hypotheses generated from Trans, a\nreference sentence $r$, and an evaluation metric $\\mu$, DrNMT parameters\n$\\theta$ are learned by minimizing the KL-divergence over the training\ndataset according to the following formula:\n\n\\[\\mathcal{L}\\left( \\theta \\right) = - \\sum_{j = 1}^{n}{p_{T}\\left( u_{j} \\right)\\text{.log}\\left( p_{M}\\left( u_{j} \\middle| x;\\theta \\right) \\right)}\\]\n\n\\[p_{M}\\left( u_{i} \\middle| x;\\theta \\right) = \\text{softmax}\\left( o_{i}\\left( u_{i} \\middle| x;\\theta \\right) \\right) = \\frac{\\exp\\left( o_{i}\\left( u_{i} \\middle| x;\\theta \\right) \\right)}{\\sum_{j = 1}^{n}{\\exp\\left( o_{j}\\left( u_{j} \\middle| x;\\theta \\right) \\right)}}\\]\n\n\\[p_{T}\\left( u_{i} \\right) = \\text{softmax}\\left( \\frac{\\mu\\left( u_{i},\\ r \\right)}{T} \\right) = \\frac{\\exp\\left( \\frac{\\mu\\left( u_{i},\\ r \\right)}{T} \\right)}{\\sum_{j = 1}^{n}{\\exp\\left( \\frac{\\mu\\left( u_{j},\\ r \\right)}{T} \\right)}}\\]\n\nWhere $T$ is the temperature to control the smoothness of the\ndistribution. In practice, we apply a min-max normalization on $\\mu$\ninstead of using $T$ so that the best hypothesis scores 1 and the worst\n0.\n\nResults\n\nIn the paper, they experimented on four different language pairs:\nGermanEnglish (De-En), English-German (En-De), English-Tamil (En-Ta) and\nRussian-English (Ru-En). The following table shows the number of\nsentences in each dataset used in the experiments after pre-processing:\n\n\n    \n\n\nThey trained vanilla Transformer models using the bitext data to\ngenerate the n-best hypothesis list. To alleviate overfitting,\nBack-Translation data was generated from beam decoding with beam size\nequal to 5. They also used pre-trained XLM-R as the transformer-part of\nthe DrNMT. Experiments on the four WMT directions show that DrNMT yields\nimprovements of up to 4 BLEU over the beam search output:\n\n\n    \n\n\nThe following are the models used in the previous table:\n\n\n  \n    beam (fw): Feed-forward transformer with beam decoding.\n  \n  \n    beam (fw) + MLM: This is the same as beam (fw) with\npre-trained masked language model (MLM) added. This technique was\nproposed by this paper: Masked Language Model\nScoring. This takes a pre-trained\nmasked language model (MLM) on the target side, and iteratively\nmasks one word of the hypothesis at the time and aggregates the\ncorresponding scores to yield a score for the whole hypothesis.\nThen, this score is combined with the score of the forward model to\nrerank the n-best list.\n  \n  \n    beam (fw) + MLM: This is the same as beam (fw) + MLM where\nMLM is tuned on our target side monolingual dataset.\n  \n  \n    DrNMT: is the model described before.\n  \n  \n    DrNMT + NCD: This is the same as DrNMT with noisy channel\ndecoding (NCD), a technique proposed in this paper: Simple and\nEffective Noisy Channel Modeling for Neural Machine\nTranslation.\n  \n  \n    Oracle: The oracle is computed by selecting the best hypotheses\nbased on BLEU with respect to the human reference. Of course, the\noracle may be not achievable because of uncertainty in the\ntranslation task.\n  \n\n\nIn the paper, they also examined the effect of training the reranker\nwith different sizes of the n-best list. And they found out that;\nas the size of the n-best list during test time increases, the\nperformance of all rerankers and NCD improve. The following\nfigure shows the performance of DrNMT on De-En validation sets from\nfour re-rankers trained with 5, 10, 20 and 50 hypotheses,\nrespectively:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Hinted Back-Translation",
        "url"       : "/machine-translation/HintedBT",
        "date"      : "09/09/2021",
        "content": "HintedBT is a family of techniques that provides hints through tags\non the source side and target side to the\nBack-translation\nmechanism to improve the effectiveness of the provided monolingual data.\nThese techniques were proposed by Google Research in 2021 and published\nin their paper: HintedBT: Augmenting Back-Translation with Quality\nand.\n\nBack-translation\nis one such widely used data augmentation technique in which synthetic\nparallel data is created by translating monolingual data in the target\nlanguage to the source language using a baseline system. However, in\norder to get high quality parallel back-translated (BT) data, we either\nneed a high quality target→source translation model or a high quality\nparallel (bitext) data. Both are limited when it comes to low-resource\nlanguages.\n\nTo overcome that, existing methods either use all BT data available which leads\nto low-quality translation models, or use various cleaning techniques to filter\nout lower quality BT data which reduces the amount of data available for\ntraining low-resource languages even more.\n\nHintedBT provides the solution of this problem by providing two\ndifferent techniques that can be combined or used independently:\n\n\n  \n    Quality Tagging\n  \n  \n    Translit Tagging\n  \n\n\n\n    \n\n\nQuality Tagging\n\nQuality Tagging approach uses all the BT data by utilizing quality\ninformation about each instance by using multiple tags on the source\nsentence to hint of the quality of the BT pair. For each sentence pair,\nthe quality was determined by the following steps:\n\n\n  \n    Use LaBSE to\ncompute sentence embeddings of the two sentences. LaBSE is a\nBERT-based language agnostic cross-lingual model.\n  \n  \n    Compute the cosine similarity between these source and target\nembeddings.\n  \n  \n    This score is treated as the quality score\n  \n\n\n\n  Note:\nThis method can be seen as an extension to the Tagged\nBT\napproach.\n\n\nNow, we know how they calculated the quality of a sentence pair. Next, we need\nto know how they designed the binning mechanism. By binning mechanism, I mean\nhow they divided all sentence pairs into bins where each bin has a bunch of\nsentence pairs. In the paper, they designed the binning mechanism by using:\n\n\n  \n    Equal Volume Binning: They calculated the quality score of the\n$N$ sentence-pairs, then they sorted them by their quality score,\nand finally divided them into $k$ equally sized groups.\n  \n  \n    Four bins: They experimented with different number of bins (see\nexperiments) and found out that four or\nthree bins provide the best performance.\n  \n\n\nThe following figure shows a sentence-pair after applying Quality Tagging; the\ntags indicate the bin index prepended to the source sentence:\n\n\n    \n\n\n\n  Note:\nThey also experimented using Equal Width Binning (to divide the quality\nscore range) instead instead of Equal Volume Binning (to divide the data\npoints range) and found out that the former leads to unbalanced data which isn’t\npreferable.\n\n\nTranslit Tagging\n\nTranslit Tagging approach uses all the BT data by adding a tag to the\ntarget sentence indicating whether this sentence pair needs either\ntranslation or translation + transliteration\n. Hence the name “Translit”. If the target was fully translated,\nthe method will prepend &amp;lt;Txn&amp;gt; to the target sentence. Otherwise, it will\n&amp;lt;Both&amp;gt; to the target sentence as seen below:\n\n\n    \n\n\n\n  Note:\nTransliteration is when you sound a word of one language using the\nscripts of other languages. We do that all the time when writing our\nnames in other language; like “أحمد” will be written in English as “Ahmed”. In\nthe following example, we can see the difference between translation and\ntranslation + transliteration:\n\n  \n    \n\n\n\nTransliteration is a big issue especially in low-resource languages.\nAccording to the following table found in the paper, more than 60% of\nthe sentence-pairs in three different low-resource languages contain\nboth translation + transliteration:\n\n\n    \n\n\nThis huge percentage of translation + transliteration in the datasets\nputs a pressure on the trained model to identify implicitly which source\nwords should be translated to the target language and which need to be\ntransliterated. In this paper, they did that by using the following\nsteps:\n\n\n  \n    For each word in the source sentence, they used FST (Finite State\nTransducer) transliteration models from this\npaper to generate 10\nEnglish (i.e., the target language) transliterations.\n  \n  \n    If any of these transliterations are present in the corresponding\ntarget, they would categorize this pair as &amp;lt;Both&amp;gt;. As you might\nhave guessed, it only needs one word to prepend this tag.\n  \n  \n    If not, they would categorize it as &amp;lt;Txn&amp;gt;.\n  \n\n\n\n  Note:\nThey experimented prepending the translit tag to source sentence but it didn’t\nperform as we are going to discuss later in the experiments.\n\n\nExperiments\n\nIn this paper, all experiments were performed on three low-resource\nlanguages: Hindi→English (hi→en), Gujarati→English (gu→en), and\nTamil→English (ta→en). The training/dev/test stats of the data used can\nbe seen in the following table:\n\n\n    \n\n\nAll three models used in these experiments were standard Transformer\narchitecture with 6 layers on the encoder and the decoder for hi→en and\n4 layers on the decoder for both gu→en and ta→en. The dimension of\ntransformer layers, token embeddings and positional embeddings is\n$1024$, the feed-forward layer dimension is $8192$, and number of\nattention heads is $16$.\n\nFor training, we use the Adafactor optimizer with $\\beta_{1} = 0.9$ and\n$\\beta_{1} = 0.98$, and the learning rate is varied with warm-up for\n40,000 steps followed by decay as in the original paper for 300k step\nusing a batch size of 3k across all models and tokenize the source and\ntarget using WordPiece tokenization.\n\nBaselines\n\nIn the paper, they presented five baseline models:\n\n\n  \n    bitext: Model trained only on bitext data.\n  \n  \n    bitext + full-BT: Model trained on bitext data and an additional\n23M back-translated pairs.\n  \n  \n    bitext + Iterative-BT: Model trained with two iterations of\nback-translation (in the forward and reverse directions).\n  \n  \n    bitext + tagged-full-BT: Model trained on bitext data using\ntagged Back-Translation data. A tag is added to the source in every\nsentence pair to help the model distinguish between natural (bitext)\nand synthetic (BT) data.\n  \n  \n    bitext + LaBSE topk-BT: Model trained on bitext data and topk\nbest quality BT pairs. Quality is estimated using LaBSE scores, and\nthey grid-searched with at least 6 LaBSE threshold values and choose\nthe one which gives the best BLEU on the dev set. The chosen\nthreshold yields 20M BT sentences for hi→en, 10M for gu→en and 5M\nfor ta→en.\n  \n\n\n\n    \n\n\nResults\n\nThe following table shows the comparison between different variations of\nHintedBT and baseline models. As we can see, HintedBT provides the best\nresults on the three languages. Also, quality tagging provides similar\nresults to topk-BT and it’s in face more efficient than topk-BT in terms\nof computational resources:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Dataset-Length Bias",
        "url"       : "/machine-translation/Data_Length_Bias",
        "date"      : "13/09/2021",
        "content": "Neural Machine Translation (NMT) is known to suffer from a beam-search\nproblem: after a certain point, increasing beam size causes an overall\ndrop in translation quality. This effect is especially in long\nsentences. A factor that strongly contributes to the quality degradation\nwith large beams is dataset-length bias which means that NMT\ndatasets are strongly biased towards short sentences.\n\nTo mitigate this issue, some researchers from Yandex in 2021 proposed a\nnew data augmentation technique called “Multi-Sentence Resampling” or\nMSR for short in their paper: Multi-Sentence Resampling: A Simple\nApproach to Alleviate Dataset Length Bias and Beam-Search\nDegradation. This technique\nextends the training examples by concatenating several sentences from\nthe original dataset to make a long training example. The official\nimplementation of this technique can be found on the Yandex official\nGitHub Repository: msr.\n\nMSR\n\nMSR stands for “Multi-Sentence Resampling” which augments a dataset such\nthat each training example consists of 1 to N sentences randomly chosen\nfrom a dataset and concatenated one after another preserving the order\nof sentence. The following is the full algorithm:\n\n\n    \n\n\nAs shown in the following example, we have a dataset of just three short\nsentences (on the left). Using MSR with N=3 will create a new dataset\nwhere each sentence is either 1 or 2 or 3 sentences long. MSR\nconcatenates sentences together as shown on the right table which\nincreases the average length of the dataset.\n\n\n    \n\n\n\n  Note:\n\n  \n    \n      MSR performed the augmentation process on both the source-side and\n  the target-side of the data.\n    \n    \n      Knowing the average length of examples in a dataset $L$, the average\n  length of examples in the new dataset can be approximately\n  calculated as:\n    \n  \n\n\\[avg\\_ new\\_ length \\cong \\sum_{n = 1}^{N}\\frac{\\text{L.n}}{N} = L.\\frac{N + 1}{2}\\]\n\n\nThe following figure illustrates how the train examples length\ndistribution changes in IWSLT17 Fr-En dataset for N from 2 to 5. With\ngrowing N distributions become more flatten for lengths presented in the\ntest set.\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "BiT: Bidirectional Training",
        "url"       : "/machine-translation/BiT",
        "date"      : "16/09/2021",
        "content": "BiT stands for “Bidirectional Training” which is a simple and effective\npre-training strategy for neural machine translation. BiT was proposed\nby The University of Sydney in collaboration with Peking University and\nJD Explore Academy in 2021 and published in this paper: Improving\nNeural Machine Translation by Bidirectional\nTraining.\n\nThe motivation behind this strategy is that when human learn foreign\nlanguages, knowing both directions $x_{i} \\rightarrow y_{i}$ and\n$y_{i} \\rightarrow x_{i}$ may help human easily master the bilingual\nknowledge. Simply put, they proposed using a system trained on\nbi-directional data as an initialization for a unidirectional system.\n\nSpecifically, they proposed pre-training NMT models using bilingual data\ncreated by reconstructing the training samples from\n($\\overrightarrow{B}:\\ \\text{source} \\rightarrow \\text{target}$) to\n($\\overset{\\overleftrightarrow{}}{B}:source + target \\rightarrow target + source$)”,\nwhere the training data was doubled as shown in the following formula:\n\n\\[\\overset{\\overleftrightarrow{}}{B} = \\left\\{ \\left( x_{i},\\ y_{i} \\right) \\cup \\left( y_{i},\\ x_{i} \\right) \\right\\}_{i = 1}^{N}\\]\n\nWhere the pre-training objective is:\n\n\\[\\overset{\\overleftrightarrow{}}{\\mathcal{L}}\\left( \\theta \\right) = \\overrightarrow{\\mathcal{L}}\\left( \\theta \\right) + \\overleftarrow{\\mathcal{L}}\\left( \\theta \\right)\\]\n\n\\[\\overset{\\overleftrightarrow{}}{\\mathcal{L}}\\left( \\theta \\right) = \\underset{\\theta}{\\arg\\max}\\left( \\log\\left( p\\left( y \\middle| x;\\theta \\right) \\right) \\right) + \\underset{\\theta}{\\arg\\max}\\left( \\log\\left( p\\left( x \\middle| y;\\theta \\right) \\right) \\right)\\]\n\nPre-training lasts for 1/3 of the total training steps and fine-tuning\nis performs on the required direction $\\overrightarrow{B}$ with the rest\nof 2/3 training steps.\n\nExperiments\n\nIn this paper, they tried five translation datasets whose data sizes\nrange from 160K to 38M sentence-pairs as shown in the following table.\nAll language pairs were trained on Transformer-BIG except IWSLT14 En↔De\nand WMT16 En↔Ro; they were trained on Transformer-BASE because of their\nextremely small data size. Also, the performance was measured by\naveraging the model’s last 10 checkpoints to avoid stochasticity.\n\n\n    \n\n\nThe previous table show that BiT achieves significant improvements over\nstrong baseline Transformer in 7 out of 10 directions, and the rest of 3\ndirections also show promising performance. This demonstrates the\neffectiveness and universality of BiT.\n\nTo dispel the doubt that BiT works also on distant language pairs, they\ntrained two models on Zh↔En and Ja→En language pairs. The following\ntable shows that BiT significantly and incrementally improves the\ntranslation quality in all cases.\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Machine Translation",
        "title"     : "Scaling Transformer",
        "url"       : "/machine-translation/scaling_transformer",
        "date"      : "16/09/2021",
        "content": "Scaling model sizes, datasets and the total computation budget has been\nidentified as a reliable approach to improve generalization performance\non several machine learning tasks. Here, we are going to discuss a paper\ncalled “Scaling Laws for Neural Machine\nTranslation” published by Google\nResearch in 2021 where the researchers study the effect of scaling the\ntransformer depths on the performance.\n\nExperiments\n\nAll experiments in this paper were performed using a series of\nTransformer\nnetworks where the model dimension to $1024$, feed-forward layer\ndimension to $8192$, number of attention heads to $16$, attention hidden\ndimension to $1024$, and varying layers as shown in the following figure\nknowing that the baseline is a 12-layer encoder 12-layer decoder\nTransformer model:\n\n\n    \n\n\nAll models were trained with a cross-entropy loss and Adafactor\noptimizer. All models were trained for $500k$ training steps with a\nfixed batch-size of $500k$ tokens. All models were regularized using a\ndropout rate of $0.1$, label smoothing of $0.1$, and a gradient clipping\nof $10$ to improve the training stability.\n\nTraining Data\n\nAll results -in this paper- were reported on two language pairs,\n(English→German) and (German→English) using an in-house web-crawled\ndataset with around 2.2 billion sentence pairs for both translation\ndirections. A sentence-piece vocabulary of size $32k$ was used for\ntraining all models. This dataset provides a large enough training set\nto ensure the dataset size is not a bottleneck in the model performance.\n\nEvaluation Data\n\nTo evaluate these models, they used a variety of test sets covering\ndifferent domains such as Web-Domain,\nNews-Domain, Wikipedia, and\nPatents. The news-domain test sets come from the\nWMT2019 evaluation campaign (newstest2019) while the other test sets are\ninternal test sets.\n\nThroughout the paper, they refer to a certain data as either\n“source-original” or “target-original”, here is the difference:\n\n\n  \n    Source-original: means that the source\nsentences have been crawled from the web while the reference translations\nwere generated later by professional translators.\n  \n  \n    Target-original: means that the target\nsentences have been crawled from the web while the reference\ntranslations were generated later by professional translators.\n  \n\n\nScaling Effect on Loss\n\nIn the paper, the researchers were able to find a formula that could\ncapture the effect of scaling the encoder/decoder layers on the\nperformance of a certain test set. Given the English→German web-domain\ntest set, they trained multiple models with different encoder/decoder\nlayers and measured the perplexity of each model on the test set. The\nfollowing graph shows the performance of the encoder-scaling and\ndecoder-scaling on source-original (left) and target-original (right):\n\n\n    \n\n\nAs we can see, the dashed line of each case fits the points perfectly\n(variance $R^{2} = 99.4$). These dashed lines were creating using the\npower law of the form:\n\n\\[\\widehat{L}\\left( N \\right) = \\alpha N^{- p} + L_{\\infty}\\]\n\nWhere $N$ is the total number of parameters outside of embedding /\nsoftmax layers and $\\left( \\alpha,\\ p,\\ L_{\\infty} \\right)$ are the\nfitted parameters of the power law which change based on the\ndata.\n\nNow, given a Transformer model of $\\overline{N}_e$ encoder layers\nand $\\overline{N}_d$ decoder layers trained on a certain dataset,\nthe following formula predicts the new performance on the same dataset\nwhen the encoder layers become $N_e$ and the decoder layers become\n$N_d$:\n\n\\[\\widehat{L}\\left( N_{e},\\ N_{d} \\right) = \\alpha\\left( \\frac{\\overline{N}_e}{N_{e}} \\right)^{p_{e}}\\left( \\frac{\\overline{N}_d}{N_{d}} \\right)^{p_{d}} + L_{\\infty}\\]\n\nWhere the parameters\n$\\left( \\alpha,\\ p_{e},\\ p_{d},\\ L_{\\infty} \\right)$ are achieved using\nthe following algorithm that uses scipy.optimize.least_squares()\nfunction for curve fitting while using &#39;soft_l1&#39; loss option which is\na popular option to have some robustness to outliers:\n\ndef func(p, x, y):\n    &quot;&quot;&quot;\n    Fitting a bi-variate scaling law.\n    p: A 1-D array of dim 4, corresponding to alpha, p_e, p_d, l_inf.\n    x: A matrix of dimension n \\times 2. First column encoder params,\n    second col decoder params.\n    y: A 1-D array of log-perplexity of dim n.\n    &quot;&quot;&quot;\n    x_e = NE_bar / x[:,0]\n    x_d = ND_bar / x[:,1]\n    return p[0] * np.power(x_e, p[1]) * np.power(x_d, p[2]) + p[3] - y\n\n\ndef fit_model(x, y, f_scale):\n    X = x.to_numpy().copy()\n    y = y.to_numpy().copy()\n    if np.isnan(X).any() or np.isnan(y).any():\n        raise ValueError(&#39;Data contains NaNs&#39;)\n    if len(y.shape) &amp;gt; 1 or y.shape[0] != X.shape[0]:\n        raise ValueError(&#39;Error in shapes&#39;)\n    p0 = np.zeros((4 ,))\n    p0[0] = 0.2 # alpha\n    p0[1] = 0.4 # p_e\n    p0[2] = 0.6 # p_d\n    p0[3] = 1.0 # l_inf\n    fit = least_squares(func, p0, loss=&#39;soft_l1&#39;, f_scale=f_scale,\n        args=(X, y), max_nfev=10000, bounds=(0, 10))\nreturn fit\n\n\n\n  Note:\nThe parameter $\\alpha$ corresponds to the maximum loss reduction\ncompared to the baseline model, while the parameter $L_{\\infty}$\ncorresponds to the irreducible loss of the data.\n\n\nThe above formula captures the scaling behavior of the Transformer NMT\nmodels over a certain dataset. The following figure shows different\nvalues of the encoder exponent and the decoder exponent over multiple\ntest sets:\n\n\n    \n\n\nAs we can see, the decoder exponents $p_{d}$ were observed to be larger\nthan the encoder exponents $p_{e}$. As a result, when improving the\ntest loss is concerned, it is much more effective to scale the decoder\nrather than the encoder. This is contrary to the usual practice\nwhere many practitioners train NMT models with deep encoders and shallow\ndecoders.\n\nNow to a very important question, “what is the optimal Transformer size\ngiven a certain dataset?” The paper proposed the following formula which\npredicts the optimal size for both the encoder and the decoder:\n\n\\[N_{e}^{*} = \\frac{p_{e}}{p_{e} + p_{d}}B,\\ \\ \\ \\ \\ \\ \\ \\ N_{d}^{*} = \\frac{p_{d}}{p_{e} + p_{d}}B\\]\n\nWhere $B$ is the total number of parameters you can afford in your\norganization. In addition, when optimally scaling the model, the scaling\nlaw reduces to:\n\n\\[\\alpha^{*} \\equiv \\alpha\\left( \\frac{\\overline{N}_e\\left( p_{e} + p_{d} \\right)}{p_{e}} \\right)^{p_{e}}\\left( \\frac{\\overline{N}_d\\left( p_{e} + p_{d} \\right)}{p_{e}} \\right)^{p_{d}}\\]\n\n\n  Important Note:\nIn the paper, they found out that symmetrically scaling the encoder and\ndecoder layers, which yields $\\frac{N_{d}}{N} \\approx 0.55$, is barely\ndistinguishable from the optimal scaling scheme.\n\n\nScaling Effect on Quality\n\nThey examined the effects of scaling on the output quality as measured\nby BLEU score. The following figure presents the co-evolution of BLEU\nscore and cross-entropy loss throughout the training for all of our\nmodels.\n\n\n    \n\n\nDepending on the construction of the test sets, two different empirical\nbehaviors emerge:\n\n\n  On target-original test sets, larger models are able to\nimprove (lower) the test loss which was accompanied with consistent\nimprovements (increases) in BLEU score. In fact, the following\nsimple power law can capture the relation:\n\n\n\\[BLEU = c_{B}L^{- p_{B}}\\]\n\n\n  \n    On source-original test sets, larger models\nconsistently achieve better (lower) test losses, however, beyond a certain\nthreshold, BLEU scores begin to deteriorate.\n  \n  \n    Also, a careful look at the left-subplots brings up another\ninteresting trend. At similar values of the test loss,\nencoder-scaled models result in better generation quality\ncompared to decoder-scaled models. This findings agrees with\nprevious work that relied on encoder-scaling when optimizing for\nBLEU.\n  \n\n\n"
      },
    
  
  
    
    
  
  
    
    
      
      
      {
        "collection": "Multilingual NMT",
        "title"     : "Multilingual Google&#39;s NMT",
        "url"       : "/multilingual-nmt/Multilingual_GNMT",
        "date"      : "14/11/2016",
        "content": "GNMT stands for “Google Neural Machine Translation” which is a\nbilingual machine translation architecture that was\ndiscussed before in this post:\nGNMT. Here, we\nare going to discuss how they extended the bilingual nature of the GNMT\nmodel to be multilingual. The Multilingual GNMT architecture, as seen in\nthe following figure, was proposed in 2016 by the Google Research team\nand published in this paper: Google’s Multilingual Neural Machine\nTranslation System: Enabling Zero-Shot\nTranslation. The official code\nfor this paper can be found in the TensorFlow’s official GitHub repository:\nTensorFlow/GNMT.\n\n\n    \n\n\nThe researchers of this paper have found out that this architecture,\nwhich is used mainly for bilingual translation, can be extended to\nmultiple languages using a simple method that doesn’t change anything in\nthe architecture. The method they used was just to add an artificial\ntoken to the input sequence to indicate the required target\nlanguage. That’s it!! All other parts of the system like\nencoder, decoder, attention mechanism, and shared WordPiece vocabulary\nstayed exactly the same.\n\n\n  Note:\nThis paper was the first paper to validate the use of true multilingual\ntranslation using a single encoder-decoder model. That’s why some\nconsider it the founding basis for Multilingual NMT systems.\n\n\nArchitecture\n\nThe multilingual GNMT model architecture is identical to the\nGNMT model as\nshown below with only one addition which is to add an artificial token\nat the beginning of the input sentence to indicate the required target\nlanguage. In this example, the token $\\left\\langle 2es \\right\\rangle$\nindicates that the target sentence is in Spanish. Also, the source\nsentence is reversed as a processing step.\n\n\n    \n\n\nTo be able to make use of multilingual data within a single system, they\nproposed adding an artificial token at the beginning of the input\nsentence to indicate the target language. For instance, to translate to\nSpanish (from any language), the $\\left\\langle 2es \\right\\rangle$ token\nshould be added to the beginning of the source sentence like so:\n\n\\[\\left\\langle 2es \\right\\rangle\\ Hello,\\ how\\ are\\ you? \\rightarrow Hola,\\ ¿cómo\\ estás?\\]\n\nInstead of:\n\n\\[Hello,\\ how\\ are\\ you? \\rightarrow Hola,\\ ¿cómo\\ estás?\\]\n\n\n  Very Important Note:\nWhen training, they didn’t specify the source language. Not specifying\nthe source language is simpler, and can handle input with code-switching\n(as discussed later. Also, context provides enough language\nevidence to produce the correct translation when there are two words\nwith the same spelling in two different source languages that have\ndifferent meanings.\n\n\nData\n\nIn this paper, they trained the model on publicly-available datasets\nalong with Google-internal production datasets. The datasets used for\nmost of the experiments performed in this paper were:\n\n\n  \n    WMT’14 English(En)→French(Fr).\n  \n  \n    WMT’14 English→German(De).\n  \n  \n    Internal dataset English↔Japanese(Ja).\n  \n  \n    Internal dataset English↔Korean(Ko).\n  \n  \n    Internal dataset English↔Spanish(Es).\n  \n  \n    Internal dataset English↔Portuguese(Pt).\n  \n\n\nThey used a shared WordPiece model of all the source and target data\nwith $32,000$ word pieces. They used newstest2014 and newstest2015 as\ntest sets. They used a combination of newstest2012 and newstest2013 as\nthe development set.\n\nResults\n\nThe training protocols used here are mostly identical to the\nGNMT. The only\nthing changed was that they used larger batch sizes with a slightly\nhigher initial learning rate to speed up the convergence of these\nmodels.\n\n\n  Note:\nIn the paper, they compared uniform batching (each batch has one language-pair) and mixed batching (each batch has more than one language- pair) and they found out that mixed batching significantly improve efficiency of the serving system.\n\n\nIn training, they applied the multilingual GNMT in several different\nconfigurations: Many-to-One, One-to-Many, and Many-to-Many.\nAnd to measure the influence of varying amounts of training data per\nlanguage pair, they explored two strategies:\n\n\n  \n    Oversampling: where they over-sampled the data from all language\npairs to be of the same size as the largest language pair\n  \n  \n    No Oversampling: where they mix the data as is without any\nchange.\n  \n\n\nMany-to-One\n\nHere, the multilingual GNMT have multiple source languages and a single\ntarget language. Since there is only a single target language, no\nadditional source token is required. In this part, they performed three\nsets of experiments:\n\n\n  They combined German→English and French→English to train a\nmultilingual GNMT and compared it to two single bilingual models\ntrained independently.\n\n\n\n    \n\n\n\n  They combined Japanese→English and Korean→English (with oversampling)\nand compared it to two single bilingual models trained independently.\n\n\n\n    \n\n\n\n  They combined Spanish→English and Portuguese→English (with\noversampling) and compared it to two single bilingual models\ntrained independently.\n\n\n\n    \n\n\nAll results presented above shows that multilingual GNMT outperforms the\nbaseline single systems despite the fact that all of the models have the\nsame architecture and number of parameters.\n\nOne-to-Many\n\nHere, the multilingual GNMT have a single source language and multiple\ntarget languages. Here, they prepended the input with an additional\ntoken to specify the target language. As before, they performed three\nsets of experiments:\n\n\n  They combined English→German and English→French to train a\nmultilingual GNMT and compared it to two single bilingual models\ntrained independently.\n\n\n\n    \n\n\n\n  They combined English→Japanese and English→Korean(with\noversampling) and compared it to two single bilingual models\ntrained independently.\n\n\n\n    \n\n\n\n  They combined English→Spanish and English→Portuguese (with\noversampling) and compared it to two single bilingual models\ntrained independently.\n\n\n\n    \n\n\nAll results presented above shows that multilingual GNMT outperforms the\nbaseline single systems in some cases but not always.\n\n\n  Note:\nBased on the first table, The model with no oversampling achieves better\nresults on the larger language (English→French) compared to the smaller\none (English→German).\n\n\nMany-to-Many\n\nHere, the multilingual GNMT have multiple source languages and multiple\ntarget languages. Here, they prepended the input with an additional\ntoken to specify the target language. As before, they performed three\nsets of experiments:\n\n\n  They combined English↔German and English↔French to train a\nmultilingual GNMT and compared it to four single bilingual models\ntrained independently.\n\n\n\n    \n\n\n\n  They combined English↔Japanese and English↔Korean(with\noversampling) and compared it to four single bilingual models\ntrained independently.\n\n\n\n    \n\n\n\n  They combined English↔Spanish and English↔Portuguese (with\noversampling) and compared it to four single bilingual models\ntrained independently.\n\n\n\n    \n\n\nThe results presented above show that the multilingual production models\nwith the same model size and vocabulary size as the single language\nmodels are quite close to the baselines. The average relative loss in\nBLEU score across all experiments is only approximately $2.5\\%$.\n\nLarge-scale Model\n\nIn this part, they tried to combine 12 language pairs having a total of\n$12*255M = 3B$ parameters into a single multilingual model. To achieve\nthat, they tried a range of multilingual models starting from $255M$\n(the same size as a single GNMT) up to $650M$ parameters. The following\ntable shows the results knowing that all models were over-sampled:\n\n\n    \n\n\nThe table above shows that multilingual models are on average worse than\nthe bilingual models and the average difference gets smaller when going\nto larger multilingual models. However, the largest multilingual model\nstill has about five times less parameters than the combined single\nmodels which requires roughly 1/12-th of the training time (or computing\nresources).\n\nZero-shot Translation\n\nZero-shot Translation is when the model is able to translate between\nlanguage pairs that has never seen during training. Some call it\n“Implicit Bridging”; unlike “Explicit Bridging” where we translate the\nsource language to a bridge language (usually English) and then to the\ntarget language.\n\n\n    \n\n\nTo demonstrate the zero-shot performance, they build two multilingual\nmodels:\n\n\n  \n    Model trained on (Portuguese→English and English→Spanish)... 2\npairs.\n  \n  \n    Model trained on (Portuguese↔English and English↔Spanish)... 4\npairs.\n  \n\n\nThe following table shows the comparison of phrase-based machine translation\n(PBMT) system, Bilingual NMT, and Multilingual NMT on Portuguese→Spanish\ntranslation:\n\n\n    \n\n\nFrom the previous table, we can see that:\n\n\n  \n    NMT system outperforms the PBMT system.\n  \n  \n    Both Model 1 and Model 2 can perform zero-shot translation with\nreasonable quality and Model 2 outperforms Model 1 by up to 3 BLEU\npoints while having the same number of parameters.\n  \n  \n    The model at the last row (f) achieves that highest BLEU. This model\ncan’t be called zero-shot since it is created by incrementally\ntraining Model 2 with a small amount of true Portuguese→Spanish\nparallel data.\n  \n\n\n\n  Note:\nThe zero-shot translation works better when the source and target\nlanguages are from the same family; like Portuguese and Spanish.\nThey tried zero-shot learning (implicitly bridged) with Spanish and\nJapanese and the result was worse as shown below:\n\n  \n    \n\n\n\nThe model at the last row had the best performance which raises an\nimportant question: what is the best way to use additional parallel\ndata when you have multilingual GNMT? To answer this question, they\ntrained a multilingual GNMT model on English↔{Belarusian(Be),\nRussian(Ru), Ukrainian(Uk)} languages which they called\n“Zero-shot”, then they tried to add additional\nRu↔{Belarusian(Be), Ukrainian(Uk)} parallel data in two different\nways:\n\n\n  \n    Training a new multilingual model with all available parallel data\nmixed equally. They called this “From-scratch”.\n  \n  \n    Incrementally training the multilingual model on the additional\nparallel data. They call this “Incremental”.\n  \n\n\nThe following table shows that incrementing the zero-shot models with a small\namount of additional parallel data achieves almost the same results as\ntraining the model from-scratch:\n\n\n    \n\n\nCode-Switching\n\nCode-switching is the process of forming a sentence in more than one\nlanguage as shown in the following figure where we can see a sentence\nformed by mixing English and Hindi. Interestingly, multilingual GNMT can\nhandle code-switching sentences despite the fact that no such\ncode-switching samples were present in the training data.\n\n\n    \n\n\nI would love to see some examples of how the model performed on\ncode-switching samples, but sadly they didn’t provide any. No problem,\nthough! It still one of the best papers I have ever read. Respect!\n"
      },
    
      
      
      {
        "collection": "Multilingual NMT",
        "title"     : "Massively MNMT",
        "url"       : "/multilingual-nmt/Massively_MNMT",
        "date"      : "02/07/2019",
        "content": "Massively MNMT is a multilingual many-to-many NMT model proposed by\nGoogle Research in 2019 and published in their paper: Massively\nMultilingual Neural Machine\nTranslation. Massively MNMT is a\nstandard Base-Transformer with 6 layers in both the encoder and the\ndecoder. To enable many-to-many translation, the authors added a\ntarget-language prefix token to each source sentence.\n\nThe main question in this paper is how well a single NMT model can scale\nto support a very large number of language pairs. To answer this\nquestion, the model was trained under two training settings:\n\n\n  \n    Low-resource Setting\nLanguage-pairs have limited training examples.\n  \n  \n    High-resource Setting:\nLanguage-pairs have plenty of training examples.\n  \n\n\nIn these settings, they created three different massively MNMTs with\nexactly the same architecture:\n\n\n  \n    Many-to-many model:\nTrained using data from and to English (116 directions).\n  \n  \n    One-to-many model:\nTrained using data from English (58 directions).\n  \n  \n    Many-to-one model:\nTrained using data to English (58 directions).\n  \n\n\nFor model fine-tuning, they created a development set by uniformly sampling\nfrom a concatenation of all the individual language pair development sets,\nresulting in 13k development examples.\n\n\n  Note:\nIn this paper, the baseline models in the following tables are\nbilingual models, each trained for a specific language-pair.\n\n\nLow-resource Setting\n\nIn this setting, we are going to evaluate the performance of the\nMassively MNMT over 59 languages with few training examples gathered\nfrom TED talks\nparallel corpus. This dataset is highly imbalanced, with language pairs\nincluding between 214k to 3.3k sentence pairs for training as shown in\nthe following table:\n\n\n    \n\n\nIn this setting, the model dimension is set at 512, hidden dimension\nsize of 2048 and 8 attention heads. The model’s 93M trainable parameters\nwere trained using the inverse square root learning rate schedule with\nlearning rate set at 3 and 40k warm-up steps. The vocabulary used in\nthis setting is 32k sub-words.\n\nTo evaluate the three models, they used the following four languages:\nAzerbeijani (Az), Belarusian (Be), Galician (Gl) and Slovak (Sk) as they\npresent an extreme low-resource case with as few as 4.5k training\nexamples for Belarusian-English. The following two tables show the test\nBLEU score:\n\n\n    \n\n\nThe previous left table shows that the many-to-many model\noutperforms all other models when translating into English. This is\nsurprising as it uses the same X→En data as the many-to-one model.\nOne possible explanation is that the many-to-one model overfits the\nEnglish side of the corpus since the English sentences are overlapping\nacross the different language pairs, making it much easier for the model\nto memorize.\n\nOn the other hand, the previous right table shows an opposite trend; the\nmany-to-many model performs worse than the one-to-many model.\nThis quality degradation may be due to the English-Centric setting:\nsince most of the translation directions the model is trained on are\ninto English, this leaves less capacity for the other target languages.\n\nHigh-resource Setting\n\nIn this setting, they scaled the number of languages to 103 (shown\nbelow) and the number of examples per language-pair to 940k on average.\nThe 13 languages colored in red had less than 1M examples, while the\nrest had exactly 1M. This data is not publicly available:\n\n\n    \n\n\nIn this setting, the model dimension is set at 1024, hidden dimension\nsize of 8192 and 16 attention heads. The model’s 473.7M trainable\nparameters were trained using the inverse square root learning rate\nschedule with learning rate set at 3 and 40k warm-up steps. The\nvocabulary used in this setting is 64k sub-words.\n\nTo evaluate the three models, they used 10 languages different\ntypological families: Arabic (Ar), Hebrew (He), Romance – Galician\n(Gl), Italian (It), Romanian (Ro), German (De), Dutch (Nl), Belarusian\n(Be), Slovak (Sk) and Azerbaijani (Az) and Turkish (Tr). The following\ntwo tables show the test BLEU score:\n\n\n    \n\n\nThe former two tables show that the many-to-many model outperforms\nbilingual baseline models. This shows that many-to-many models can work\nwell in realistic settings with millions of training examples, 102\nlanguages and 204 jointly trained directions to-and-from English.\n\nThe first table shows that the many-to-one model here performs\nbetter than the many-to-many model. This shows that the previous\nresult in the low-resource setting was due to overfitting in the\nmany-to-one model.\n\nThe second table shows the same trend as the low-resource setting where\nthe one-to-many model outperforms the many-to-many model. Again,\nthis advantage may be due to the one-to-many model handling a smaller\nnumber of tasks while not being biased towards English in the target\nside like the many-to-many model.\n\nMultilinguality Vs Performance\n\nThey tried to better understand the trade-off between the number of\nlanguages involved and the translation accuracy while keeping the model\nfixed. So, they created four additional English-Centric datasets,\ncontaining 5, 25, 50 and 75 languages. They made sure that the 25\nlanguage subset contains the 5 language subset, the 50 language subset\ncontains the 25 language subset and so on. Then, they trained the same\ntransformer model used for the high-resource setting (with 473.7M\nparameters) on each of these subsets and measure the performance in two\nmanners:\n\n\n  Supervised Manner:\nWhere they evaluated the model based on translation directions that\nit was trained on. They found out that the more languages you add,\nthe less performance you get knowing that the gap isn’t that big as\nshown below:\n\n\n\n    \n\n\n\n  Zero-shot Manner:\nWhere they evaluated the model based on translation directions that\nit has never seen. They found out that by adding more languages, the\nmodel is forced to create a more generalized representation to\nbetter utilize its capacity, which improves zero-shot performance.\nAnd the balance between capacity and generalization is best in the\nmid range (50-to-50 model):\n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Multilingual NMT",
        "title"     : "Google&#39;s M4 Model",
        "url"       : "/multilingual-nmt/M4",
        "date"      : "11/07/2019",
        "content": "M4 stands for “Massively Multilingual, Massive Machine Translation”\nwhich is a multilingual NMT model that is trained on over 25 billion\nparallel sentences in 103 languages. This model was proposed by Google\nAI in 2019 and published in their paper: Massively Multilingual Neural\nMachine Translation in the Wild: Findings and\nChallenges.\n\nM4 is a Transformer-big model with a single encoder and decoder shared\nacross all the language pairs. In order to guide the translation with\nthe intended target language, a target language token is prepended to\nevery source sequence to be translated. M4 demonstrates effective\ntransfer learning ability, significantly improving translation quality\nof low-resource languages, while keeping high-resource language\ntranslation quality on-par with competitive bilingual baselines. Up to\nthis point in history, this model is the largest multilingual NMT system\nin terms of the number of languages considered.\n\nThe aim of this paper is to highlight key challenges and open problems\non the way to building a real-world massively multilingual translation\nsystem. All experiments done in this paper were done using the standard\ntransformer architecture implemented in the Google’s open-source\nLingvo framework.\n\nData\n\nThe data used to train M4 is an in-house corpus generated by crawling\nand extracting parallel sentences from the web. This corpus contains\nparallel documents for 102 languages, to and from English, containing a\ntotal of 25 billion sentence pairs. The following is the full list of\nall languages used in this dataset:\n\n\n    \n\n\nThe following figure illustrates the data distribution across language\npairs for all 204 language pairs. The x-axis indicates the language pair\nindex, and the y-axis depicts the number of training examples available\nper language pair on a logarithmic scale. Dataset sizes range from 35k\nfor the lowest resource language pairs to 2 billion for the largest.\n\n\n    \n\n\nThis was the data for training. To minimize the data imbalance on the\nvalidation/test sets, they created validation and test sets as multi-way\naligned datasets containing more than 3k and 5k sentence pairs\nrespectively for all languages.\n\nBilingual Baselines\n\nFor high-resource language pairs, they used Transformer-Big setup which\ncontains around 375M parameters. For most medium and low resource\nlanguages, they used Transformer-Base setup. Also, they used shared\nsource-target sentence-piece model vocabulary with 32k tokens and\ndropout as a regularizer. All models were trained with Adafactor with\nmomentum factorization, a learning rate schedule of (3.0, 40k), and a\nper-parameter norm clipping threshold of 1.0. For Transformer Base\nmodels, they used a learning rate schedule of (2.0, 8k).\n\nThe following figure summarizes BLEU score (y-axis) of individual\nbilingual models on all 204 supervised language pairs. Languages\n(x-axis) are arranged in decreasing order of available training data\nfrom left to right. Each dot represents the BLEU score of one bilingual\nmodel and a trailing average is used to show the trend.\n\n\n    \n\n\nFor a more clarified results, the following table shows the average BLEU\nscore of bilingual models over different groups of languages. High 25\nrefers to the top 25 languages by dataset size (left-most portion of the\nprevious figure), while low 25 refers to the bottom 25 (right-most\nportion of the previous figure) and mid-52 is the middle 52 languages\n(the ones in the middle of the previous figure):\n\n\n    \n\n\nTransfer-Inference Tradeoff\n\nIn this section, we are going to dicuss the effect of data imbalance\nacross languages through the lens of transfer and interference. Transfer\nis the ability to use the knowledge learned from high-resource languages\nto improve the performance over low-resource languages. Inference is the\neffect of learning new languages over the old ones. Two desired\ncharacteristics of any universal machine translation model which are:\n\n\n  \n    Maximum positive transfer to low-resource languages.\n  \n  \n    Minimum interference (negative transfer) for high-resource\nlanguages.\n  \n\n\nIn the paper, they measured the effect of transfer-inference of M4\nagainst bilingual baselines by training a single Transformer-Big\nwith a shared vocabulary of 64k tokens with 0.1 dropout probability\nand the same values as the bilingual baselines used for other\nhyper-parameters. Also, they used a batch-size of 4M tokens and they\nfollowed two strategies when sampling:\n\n\n  All the available training data is combined as it is. For a given\nlanguage pair $l$ with $D_{l}$ parallel sentences, the probability\nof the sample being from language $l$ using this strategy is:\n\n\n\\[p_{l} = \\frac{D_{l}}{\\sum_{k}^{}D_{k}}\\]\n\n\n  Use temperature sampling to up-sample low-resource languages. For a\ngiven language pair $l$ with $D_{l}$ parallel sentences, the\nprobability of the sample being from language $l$ using this\nstrategy is:\n\n\n\\[p_{l} = \\left( \\frac{D_{l}}{\\sum_{k}^{}D_{k}} \\right)^{\\frac{1}{T}}\\]\n\nThe following figure shows the effect of sampling strategy on the\nperformance of multilingual models. On the axis, languages are arranged\nin decreasing order. On the y-axis, BLEU score is reported relative to\nthose of the bilingual baselines. Also, two multilingual models were\ntrained: one to translate from any language to English (Any→En) and the\nother to translate English to any language (En→Any):\n\n\n    \n\n\nFrom this graph, we can find out the following:\n\n\n  \n    Following temperature sampling (green) maximizes transfer and beats\nbilingual baselines by large margins, especially in the Any→En\ndirection (right-most portion). However, this also has significantly\nhurt the performance on high resource languages (left-most portion).\n  \n  \n    On the other hand, sampling based on the true data distribution\n(strategy (i)) retains more performance on high resource languages,\nat the cost of sacrificing performance on low resource languages.\n  \n  \n    Also, the transfer-interference tradeoff is more pronounced in the\nAny→En direction than that for the En→Any direction.\n  \n\n\nIn the paper, they tried different values for the $T$ in the\ntemperature sampling and noticed that $T = 5$ improves performance\non the high resource languages for both translation directions\ncompared to $T\\  = \\ 100$, while also retaining high transfer\nperformance on low resource languages. However, performance on high\nand medium resource languages still lags behind their bilingual\nbaselines by significant margins:\n\n\n    \n\n\nTo highlight the exact effect of interference with increasing\nmultilinguality, they trained three additional multilingual models\non a growing subset of 10, 25, and 50 languages. The specific\nlanguages were chosen to get a mixed representation of data size,\nscript, morphological complexity and inter-language relatedness.\n\nThe following figure shows the effect of different languages on the\ntranslation performance of multilingual models with a temperature\nsampling of $T = 5$. Language were sorted in decreasing order of\navailable training data from left to right. Each point represents a\ncertain language group of 10 languages; for example each blue point\nrepresents a certain group of 10 languages.\n\n\n    \n\n\nThe figure clearly highlights how performance degrades for all\nlanguage pairs, especially the high and medium resource ones, as the\nnumber of tasks (languages) grows.\n\nMultilinguality\n\nAs we have seen in the past experiments, multilinguality has a positive\nimpact on low-resource languages. To understand how multilinguality\naffects languages when translating to or from English, they inspected\nthree different settings:\n\n\n  \n    En→Any model: Translating from English.\n  \n  \n    Any→English model: Translating to English.\n  \n  \n    All→All model: Translating from any language to any language.\n  \n\n\nThey noticed that the Any→En model achieves huge improvements over\nbilingual baselines for all low-resource languages (rightmost\nportion of the right figure). On the other hand, the En→Any model\nhas lesser deterioration in the performance on high resource\nlanguages (leftmost portion of the left figure), while the\nperformance on low resource languages does not improve by much\n(rightmost portion of the left figure).\n\n\n    \n\n\nThis difference in performance between the transfer for Any→En and\nEn→Any can be explained as the Any→En model can be considered as a\nmulti-domain model where each source language constitutes a\nseparate domain. And the En→Any model can be considered as a\nmulti-task model with each target language representing a\nseparate task. This suggests that multilingual models might be\nmore amenable to transfer across input domains than transfer across\ntasks.\n\nAnother strong indicator of transfer in multilingual models is the\nquality on zero-shot translations. The following table shows the the\nzero-shot performance on selected language pairs. As we can see,\nzero-shot performance for most language pairs increases as we move\nfrom the 10 language model to the 102 language model:\n\n\n    \n\n\nVocabulary\n\nIn the paper, they constructed shared vocabulary among all languages\nusing Sentence Piece Model (SPM) to handle out-of-vocabulary tokens and\nremove complexity that may arise from language specific pre-processing.\nThe following table compares the quality of two models trained using\nvocabularies of size 32k and 64k:\n\n\n    \n\n\nFrom the table, we can see that the model with the smaller 32k token\nvocab does noticeably worse on high resource languages when translating\nin both directions, and on Any→En translation in all resource settings.\nOn the other hand, the smaller vocab model performs marginally better\nwhen translating into low resource languages on En→Any . For other\nmedium resource languages, increased vocabulary size appears to be\nbetter on all directions.\n\nArchitecture Capacity\n\nThe quality of any neural network is largely dependent on its\narchitecture and its capacity. The following figure shows the effect of\nincreasing capacity on the performance of multilingual models. The plots\ncorrespond to three different models with temperature sampling of\n$T = 5$, shared vocabulary of 64k tokens and 4M batch size:\n\n\n  \n    Blue: Transformer-Big setup which means 6 layers (400M param),\nfeed-forward hidden dimensions set to 4096, 16 attention heads and\nan attention hidden dimension set to 1024.\n  \n  \n    Green: 12 layer wide model (1.3B param), feed-forward\nhidden dimensions set to 16384, 32 attention heads and an attention\nhidden dimension set to 2048.\n  \n  \n    Red: 24 layer deep model (1.3B param), feed-forward\nhidden dimensions set to 4096, 16 attention heads and an attention\nhidden dimension set to 1024.\n  \n\n\n\n    \n\n\nFrom the past figure we can find out the following:\n\n\n  \n    Bigger models improves performance by significant amounts on the\nhigh resource languages, when compared to the Transformer-big\nbaseline (blue curve).\n  \n  \n    The deep model (red curve) beats both, the baseline (blue) and the\nequivalent capacity wide model, by significant margins on most of\nthe language pairs.\n  \n  \n    Unlike the wide model (green curve), the deep model (red curve) does\nnot overfit in low resource languages and, in fact, significantly\nenhances transfer to low resource languages on the Any→En\ntranslation tasks.\n  \n  \n    The wide model (green curve) while significantly improving\nperformance on the high resource languages, fails to show similar\ngains in the low resource setting.\n  \n  \n    While the deep model (red curve) shows great performance\nimprovements, it comes bundled with high decoding latency, a\nsignificantly larger computational footprint, and trainability\nconcerns including vanishing/exploding gradients, early divergence,\nill-conditioned initial conditions etc.\n  \n\n"
      },
    
      
      
      {
        "collection": "Multilingual NMT",
        "title"     : "Analyzing M4 using SVCCA",
        "url"       : "/multilingual-nmt/Analyzing_M4",
        "date"      : "05/09/2019",
        "content": "Multilingual Neural Machine Translation (MNMT) models have yielded large\nempirical success in transfer learning settings. However, these\nblack-box representations are poorly understood, and their mode of\ntransfer remains elusive. This paper “Investigating Multilingual NMT\nRepresentations at Scale”\npublished by Google in 2019 attempted to understand MNMT representations\n(specifically Google’s\nM4 model) using\nSingular Value Canonical Correlation Analysis (SVCCA). Google’s\nunofficial code for the SVCCA framework can be found on their official\nGitHub repository: google/svcca.\n\nSVCCA Recap\n\nAs mentioned earlier, SVCCA stands for “Singular Value Canonical\nCorrelation Analysis” which is a technique to compare vector\nrepresentations in a way that is both invariant to affine\ntransformations and fast to compute. This framework was proposed by\nGoogle Brain in 2018 and published in this paper: SVCCA: Singular\nVector Canonical Correlation Analysis for Deep Learning Dynamics and\nInterpretability. The following\nis how SVCCA works.\n\nFor a given dataset of $n$ examples\n$X = \\left\\{ x_{1},\\ …\\ x_{n} \\right\\}$,\n$z_{i}^{l} \\in \\mathbb{R}^{n}$ is defined as a vector of the neuron $i$\nvalues of layer $l$ in a neural network across all examples. Note\nthat this is a different vector from the often-considered &quot;layer\nrepresentation&quot; vector of a single point. Within this\nformalism, the Singular Vector Canonical Correlation Analysis (SVCCA)\nproceeds as follows:\n\n\n  SVCCA takes as input two (not necessarily different) sets of neurons\n(typically layers of a network):\n\n\n\\[l_{1} = \\left\\{ z_{1}^{l_{1}},\\ ...z_{n}^{l_{1}} \\right\\},\\ \\ \\ \\ \\ l_{2} = \\left\\{ z_{1}^{l_{2}},\\ ...z_{n}^{l_{2}} \\right\\}\\]\n\n\n  First, SVCCA performs a Singular Value Decomposition (SVD) of each\nsubspace to get smaller dimension while keeping $99\\%$ of the\nvariance.\n\n\n\\[{l&#39;}_{1} = SVD\\left( l_{1} \\right) = \\left\\{ {z&#39;\\ }_{1}^{l_{1}},\\ ...{z&#39;\\ }_{n_{1}}^{l_{1}} \\right\\},\\ \\ where\\ n_{1} &amp;lt; n\\]\n\n\\[{l&#39;}_{2} = SVD\\left( l_{2} \\right) = \\left\\{ {z&#39;\\ }_{1}^{l_{2}},\\ ...{z&#39;\\ }_{n_{2}}^{l_{2}} \\right\\},\\ \\ where\\ n_{2} &amp;lt; n\\]\n\n\n  Then, SVCCA uses the Canonical Correlation Analysis (CCA) to\nlinearly transform ${l’}_1$ and ${l’}_2$ to be as aligned as\npossible. CCA is a well established statistical method for\nunderstanding the similarity of two different sets of random\nvariables. It does that by linearly transform (${l’}_1$ and\n${l’}_2$) vectors to another vectors (${\\widetilde{l}}_1$ and\n${\\widetilde{l}}_2$) where the correlation\n$corr = \\left\\{ \\rho_1,\\ …\\rho_{\\min\\left( n_1, n_2 \\right)} \\right\\}$\nis maximized:\n\n\n\\[{\\widetilde{l}}_{1} = W_{1}{l&#39;}_{1} = \\left\\{ {\\widetilde{z}}_{1}^{l_{1}},\\ ...{\\widetilde{z}\\ }_{n_{1}}^{l_{1}} \\right\\}\\]\n\n\\[{\\widetilde{l}}_{2} = W_{2}{l&#39;}_{2} = \\left\\{ {\\widetilde{z}}_{1}^{l_{2}},\\ ...{\\widetilde{z}\\ }_{n_{2}}^{l_{2}} \\right\\}\\]\n\n\n  With these steps, SVCCA outputs pairs of aligned directions,\n(${\\widetilde{z}}_i^{l_1}$ , ${\\widetilde{z}}_i^{l_2}$) and\nhow well they correlate, $\\rho_i$.\n\n\nNow that we have reviewed what SVCCA is all about; let’s get back to\nthe paper and see how they used SVCCA. In the paper, they applied\nSVCCA on the sentence-level of the hidden representation of\nGoogle’s M4 model\naveraging over the sequence time-steps.\n\nMore concretely, given a batch of size $B$ sentences of max length\n$T$ in a certain language, the hidden representation of any layer of\nthis MNMT model will be a tensor of $(B \\times T \\times C)$\ndimension where $C$ is the model’s dimension. Applying an average\npooling operation on the time-steps (sentence length) will result in\na matrix of $(B \\times C)$ dimension. This is equivalent to assuming\nthat every token in a sentence from language A is equally likely to\nbe aligned to each token in an equivalent sentence in language B.\n\nMNMT Learns Language Similarity\n\nUsing the top-layer of the encoder of Google’s\nM4 model, they have\nclustered all languages together based on their SVCCA similarities.\nThen, they used the “Laplacian Eigenmaps” algorithm implemented in\nscikit-learn as\nsklearn.SpectralEmbedding\nmethod to visualize these similarities across all languages found in the\ndataset that was used to train Google’s\nM4. This dataset is an\nEnglish-centric dataset containing sentences in 103 languages. The\nfollowing figure is the result:\n\n\n    \n\n\nFrom analyzing these similarities, they have observed remarkable\nfindings:\n\n\n  The resulting clusters are grouped not only by the language-family\n(e.g. Slavic), but also by branches within the language-family (e.g.\nSouth Slavic), branches within those branches (e.g. Western\nSubgroup), and dialects within those (e.g. Serbo-Croatian). The\nfollowing figure is a visualization of the Slavic languages:\n\n\n\n    \n\n\n\n  Clustering captures the linguistic similarity\nbetween languages (sharing similar grammatical properties) and the\nlexical similarity (having the same alphabet/scripts and thus\nsharing many subwords). However, the the lexical similarity gets\nweaker as we move up on the encoder level. The following figure\nshows the representations of the Turkic and Slavic languages; within\neach family there are languages that are written in Cyrillic and\nRoman alphabets. As you can see languages that use Cyrillic-scripts\n(blue) are clustered together according to the first encoder layer’s\nrepresentation. However, they get closer to the Roman-scripts\nlanguages (orange) at the last encoder layer’s representation.\n\n\n\n    \n\n\n\n  \n    Similarity evolves across encoder/decoder Layers. For each layer,\nthey first computed the pair-wise similarity between all pairs of\nlanguages. Then, they aggregated these score into a distribution and\nrepresented in the following figures.\n\n    \n      \n        As seen in the X-to-English (left) figure:\n\n        \n          \n            On the encoder side: the similarity between the source\nlanguages (X) increase as we move up the encoder layer,\nsuggesting that the encoder attempts to learn a common\nrepresentation for all source languages. However,\nrepresentations at the last layer of the encoder are far\nfrom being perfectly aligned.\n          \n          \n            On the decoder side: as the decoder incorporates more\ninformation from the source language (X), representations\nof the target (En) diverge. This is in line with some\nfindings that show that the translated text is predictive\nof the source language.\n          \n        \n      \n      \n        For English-to-Any (right) figure: We observe a similar trend\nwhere representations of the source language (En) diverge as\nwe move up the encoder. Same as it happens with the decoder of\nthe left figure.\n      \n    \n  \n\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Multilingual NMT",
        "title"     : "mBART",
        "url"       : "/multilingual-nmt/mBART",
        "date"      : "22/01/2020",
        "content": "mBART stands for “Multilingual Bidirectional Auto-regressive\nTransformer” which is a multilingual NMT model proposed by FacebookAI in\n2020 and published in their paper: “Multilingual Denoising Pre-training\nfor Neural Machine Translation”.\nThe official code for this paper can be found in the fairseq GitHub\nrepository:\nmbart.\nmBART is the first method for pre-training a complete\nsequence-to-sequence model by denoising full texts in multiple\nmonolingual data, while previous approaches have focused only on the\nencoder/decoder.\n\nThe whole idea behind mBART is to apply the BART architecture to\nlarge-scale monolingual corpora across many languages where the input\ntexts are noised by masking phrases and permuting sentences. This will\ncreate a universal language model that is able to denoise this input\ntext which makes translation from one language to another achievable.\n\n\n    \n\n\nPre-training\n\nThe data used for pre-training mBART is monolingual data of 25 different\nlanguages collected by common crawl. The table below\nshows the size of the collected data.\n\n\n    \n\n\nThe model used in this paper is a standard sequence-to-sequence\nTransformer architecture from fairseq\nrepository with:\n\n\n  \n    12 layers of encoder and 12 layers of decoder. The model’s dimension\nis 1024 on 16 heads (∼ 680M parameters).\n  \n  \n    They included an additional layer-normalization layer on top of both\nthe encoder and decoder, which they found stabilized the training.\n  \n  \n    The model was trained for 500 steps using Adam optimizer with\n0.000001 learning rate and 0.98 beta2 and linear learning rate\ndecay. The training took around 2.4 weeks despite using 256 Nvidia\nV100 GPUs.\n  \n  \n    They started the training with dropout 0.1 and reduced it to 0.05 at\n250K steps and 0 at 400K steps.\n\n    Regarding the encoder:\n  \n  \n    For each instance, they packed as many consecutive sentences as\npossible sampled from the corresponding corpus of a certain\nlanguage &amp;lt;LID&amp;gt;, until either it hits the document boundary or\nreaches the 512 max token length.\n  \n  \n    Sentences in the instance are separated by the end of sentence\n(&amp;lt;/S&amp;gt;) token.\n  \n  \n    The language ID token is appended at the end of the instance.\n  \n\n\n\n    \n\n\nRegarding the decoder:\n\n\n  The decoder input is the original text with one position offset\nwhich is the language id symbol &amp;lt;LID&amp;gt; as it’s used as the\ninitial token to predict the sentence.\n\n\n\n    \n\n\nFollowing the BART paper, they uses two types of noise:\n\n\n  \n    Text Infilling: They masked around 35% of the words in each\ninstance by random sampling a span length according to a Poisson\ndistribution (λ = 3.5).\n  \n  \n    Sentence Permutation: They also permuted the order of sentences\nwithin each instance.\n  \n\n\nFine-tuning\n\nWe fine-tune our multilingual pre-trained models on a single pair of\nparallel data, feeding the source language into the encoder and decoding\nthe target language. Note that they used monolingual data for\npre-training and parallel corpus for fine-tuning. In the paper, they\nused different pre-trained models to compare between while fine-tuning:\n\na.  BART-En/Ro: Baseline model for just one pair (English -\n    &amp;gt; Romanian).\n\nb.  mBART25: Uses all 25 languages.\n\nc.  mBART06: Uses just 6 European languages [Ro (Romanian), It\n    (Italian), Cs (Czech), Fr (French), Es (Spanish) and En(English)].\n\nd.  mBART02: They used 4 different versions of this model; one for\n    -&amp;gt; each pair of the following pairs: ( English-German,\n    -&amp;gt; English-Romanian, English-Italian).\n\ne.  Random: don’t know exactly :D.\n\nFor all these models, they trained using 0.3 dropout, 0.2 label\nsmoothing, 2500 warm-up steps, 3e−5 maximum learning rate with a\nmaximum of 40K training updates for all low and medium resource\npairs and 100K for high resource pairs. The final models are\nselected based on validation likelihood. For decoding, we use\nbeam-search with beam size 5 for all directions.\n\nAll models use the same vocabulary. Not all tokens will frequently\noccur in all pre-training corpora, but later experiments show that\nthis large vocabulary can improve generalization in multilingual\nsettings even for unseen languages.\n\nThe following table shows a comparison between mBART25 and Random\nbaseline on low/medium-resource parallel corpora. Using mBART25\nweights shows gains on all the low and medium resource pairs. While\nfine-tuning fails in extremely low-resource setting such as En-Gu,\nwhich only have roughly 10k instances:\n\n\n    \n\n\nThe following table shows a comparison between mBART25 and Random\nbaseline on high-resource parallel corpora in the direction of En\n--&amp;gt; X. As you can see, there weren’t consistent gains, and\npre-training slightly hurts performance when &amp;gt;25M parallel sentence\nare available as if they wash out the pre-trained weights\ncompletely:\n\n\n    \n\n\n\n  Note:\nIn the paper, they tried using back-translation with low-resource\nlanguage and mBART25 and it did improve.\n\n\nAnalysis\n\nIn my opinion, this is one of the best parts about this paper where they\ntry to answer the most common questions regarding their paper:\n\n\n  \n    How many languages should you pre-train on?\nWhen monolingual data is plentiful, pre-training\npre-training on multiple languages slightly hurts the final\nresults (&amp;lt;1 BLEU). On the other hand, when monolingual data is\nlimited, pre-training on more languages helps.\n  \n  \n    Is pre-training essential for using mBART?\nWithout any pre-training, mBART tends to overfit and perform much\nworse than the baseline.\n  \n  \n    How many pre-training steps are needed?\nAfter just 25K steps, pre-trained models outperform the\nbest baseline. The models keep improving by over 3 BLEU for the\nrest of steps and have not fully converged after 500K steps.\n  \n  \n    Is fine-tuning helps with unseen pre-trained languages?\nSurprisingly, yes! They performed an experiment where they used\nmBART02 and mBART06 to translate En-Ar (English-Arabic), En-De\n(English-German), and En-DI (English-Dutch) where they weren’t in\nthe pre-training data. And these two models showed competitive\nresults compared with mBART25. This result suggests that the\npre-trained Transformer layers learn universal properties of\nlanguage that generalize well even with minimal lexical overlap.\n  \n  \n    How does mBART behave with unseen Source or Target languages in\nthe pre-training?\nIf both sides are unseen, the performance (in terms of difference\nfrom mBART25) is worse than where at least one language is seen\nduring pre-training. Fine-tuning unseen languages on source side\nis more difficult than the target side. Although mBART06\noutperforms mBART02 by a margin on when the source side is\nmissing.\n  \n\n"
      },
    
      
      
      {
        "collection": "Multilingual NMT",
        "title"     : "CRISS",
        "url"       : "/multilingual-nmt/CRISS",
        "date"      : "16/06/2020",
        "content": "CRISS stands for “Cross-lingual Retrieval for Iterative\nSelf-Supervised Training” which was created by FacebookAI in\n2020 and published in this paper: “Cross-lingual Retrieval for\nIterative Self-Supervised\nTraining. The official code for\nthis paper can be found in the fairseq GitHub repository:\ncriss.\n\nCRISS is a novel self-supervised training technique that iteratively\ntrains a multilingual NMT model on mined bi-text data with the help a\ncross-lingual model. The cross-lingual language model used here is a\npre-trained mBART model as shown below:\n\n\n    \n\n\nAs see from the previous figure, CRISS consists of different components:\n\n\n  \n    Cross-lingual language model (pre-trained mBART).\n  \n  \n    Bi-text Mined data.\n  \n  \n    Multilingual NMT (standard Seq2Seq Transformer Model).\n  \n\n\nPre-trained mBART\n\nmBART is used with CRISS because it’s able to form a language-agnostic\nsentence representation. To understand the language agnosticity of\nmBART, they performed the following study on the TED58 dataset which\ncontains multi-way translations of TED talks in 58 languages.\n\nFor each language pair, they encoded both the source and target\nsentences using mBART to obtain two sentence representations. They did\nthat for the whole dataset. Then, for each sentence in the source\nlanguage, then found the closest sentence in the target language using\ncosine similarity. And they found out that mBART was able to retrieve\nthe correct translation with 57% accuracy on average as shown in the\nfollowing figure:\n\n\n    \n\n\nThe high retrieval accuracy suggests that the pre-trained mBART model is able\nto generate language agnostic representation that are aligned at the semantic\nlevel in the vector space. Moreover this cross-lingual representation can be\nimproved by a relatively small amount of parallel data of just one direction.\nAs we can see from the following figure, the sentence retrieval accuracy\nwas improved for all language directions by 27% (absolute) on average\nafter fine-tuning it on just Japanese-English parallel dataset (223, 000\nsentence pairs in the IWSLT17).\n\n\n    \n\n\n\n  Note:\nThis is a very important discover! This means that in order to train a\nmultilingual model that can translate between N languages, we don’t need\nto fine-tune it all $N(N - 1)$ language pairs, but only a subset of\nthem.\n\n\nBi-text Mining\n\nTo mine bi-text data without supervised signals, they proposed the\nfollowing algorithm which uses monolingual data of two languages as an\ninput and a pre-trained MNMT model. The full algorithms can be seen\nbelow:\n\n\n    \n\n\nAs we can see, the algorithm is pretty self-explanatory with three\nadditional functions used. Let’s understand them one-by-one:\n\n\n  \n    Embed():\nThis function takes a sentence and a model and returns the sentence\nrepresentation of the given sentence resulting from the give model.\n  \n  \n    KNN():\nThis is the K-Nearest-Neighbor algorithm. This function takes as\ninput three variables: the vector representation of the source\nsentence, the dataset of the target language, and an integer k. And\nit returns the nearest k vectors from the given dataset to the given\nsentence representation.\nFAISS tool is used here\nfor fast searching.\n  \n  \n    score():\nThis function takes as an input two sentence representations\nand returns the score based on the margin function which is\nconsidered cosine score normalized by average distances. The margin\nscoring function is defined below:\n  \n\n\n\\[\\text{score}\\left( x,y \\right) = \\frac{\\cos\\left( x,y \\right)}{\\sum_{z \\in N_{x}}^{}\\frac{\\cos\\left( x,z \\right)}{2k} + \\sum_{z \\in N_{y}}^{}\\frac{\\cos\\left( z,y \\right)}{2k}}\\]\n\nUsing this algorithm enables us to create bi-text data in an\nunsupervised manner. Now, let’s get to how to use this mined bi-text\ndata to train MNMT model.\n\nMultilingual Training\n\nThe following algorithm shows how to perform a multilingual training.\nFirst, we simply augment each mined pair $\\left( x,y \\right)$ of\nsentences by adding a target language token at the beginning of y to\nform a target language token augmented pair $\\left( x,y’ \\right)$. Then,\nwe aggregate all mined pairs $\\left\\{ \\left( x,y \\right)’ \\right\\}$\nof the mined language pairs into a single data set to train a standard\nseq2seq machine translation transformer models from the pre-trained\nmBART model.\n\n\n    \n\n\nUnsupervised MT\n\nCRISS was evaluated on unsupervised neural machine translation\nbenchmarks that cover both low resource and high resource language\ndirections. For English-French, they used WMT’14, for English-German and\nEnglish-Romanian, they used WMT’16 test data, and for English-Nepali and\nEnglish-Sinhala, they used Flores test set.\n\nAs shown in the following table, the CRISS model overperforms\nstate-of-the-art in 9 out of 10 language directions:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Multilingual NMT",
        "title"     : "mBART-50",
        "url"       : "/multilingual-nmt/mBART-50",
        "date"      : "02/08/2020",
        "content": "In this part, we are going to discuss this paper: Multilingual\nTranslation with Extensible Multilingual Pretraining and\nFinetuning (the official code:\nmbart)\nwhich doubles the number of languages in\nmBART to suppor\nmultilingual machine translation models of 50 languages without loss of\nperformance. Also, This paper tried to fix some of the issues found in\nmBART such as:\n\n\n  \n    mBART was pre-trained using noising functions like “Text infilling”\nand “sentence permutation” and it was fine-tuned on machine\ntranslation on bitext. As you can see, fine-tuning on bitext to\ntranslate from one language to another does not leverage the full\ncapacity of the multilingual pre-training.\n  \n  \n    mBART was only trained on 25 languages, so fine-tuning to translate\non a model not part of these 25 languages is not possible.\n\n    In this paper, they propose multilingual fine-tuning (ML-FT) on\nmBART that leads to doubling the supported languages without loss of\nperformance on the original 25 languages and without starting from\nscratch. This sallows languages to be added flexibly, while\npreserving the performance.\n  \n\n\nMultilingual Fine-tuning\n\nMultilingual fine-tuning (ML-FT) is proposed as a replacement to the\nbilingual fine-tuning used earlier in the mBART paper. So, instead of\ntraining a model from language i to language j, a model is trained to\ntranslate N languages to N other languages which creates one model\ncapable of translating many languages to many other languages, which has\nefficiency and storage maintenance benefits.\n\nTo perform multilingual fine-tuning, they collected bitexts of different\nlanguage pairs $\\left( i,\\ j \\right)$ into a collection\n$\\mathcal{B}_{i,j} = \\left( \\left( x_{i},\\ y_{i} \\right) \\right)$\nfor each direction of any two languages $i$ and $j$. Each bitext pair\n$\\left( x_{i},\\ y_{i} \\right)$ is augmented by adding a language token\nat the beginning. And since the training dataset sizes are imbalanced as\ndifferent languages have different quantities of bitext, they trained\nthe model with temperature up-sampling, which up-samples lower resource\npairs:\n\n\\[p_{i,\\ j} \\propto \\left( \\frac{\\left| \\mathcal{B}_{i,\\ j} \\right|}{\\sum_{i,j}^{}\\mathcal{B}_{i,\\ j}} \\right)^{\\frac{1}{T}}\\]\n\nTo measure the effect of the multilingual fine-tuning method over mBART,\ncompared it with three strong baselines:\n\n\n  \n    Bilingual from Scratch (BL-Scratch):\nThey trained bilingual translation models with standard Transformer\nmodels for translation into and from English to 49 languages.\n  \n  \n    Bilingual Fine-tuning (BL-FT):\nBilingual finetuning adapts the mBART model into bilingual machine\ntranslation models by training for longer on translation bitext. For\neach language direction, they fine-tuned for 40K updates.\n  \n  \n    Multilingual from Scratch (ML-Scratch):\nThey trained 3 different multlilingual models from scratch for 500K\nupdates and through different batch sizes, learning rates, and\nupsampling temperature for best performing. These 3 models are:\n\n    \n      \n        Many-to-one (N → 1): It encodes N languages and decodes to\nEnglish.\n      \n      \n        One-to-Many (1 → N): It encodes English and decoded to N\nlanguages.\n      \n      \n        Many-to-Many (N ↔ N): It encodes and decodes N languages\nusing English as a pivot language (L1 → English → L2).\n      \n    \n  \n\n\nFirst, they evaluated mBART on the original 25 languages using different\nfine-tuning methods. The following results are the improvement in BLEU compared\nto BL-Scratch which show that ML-FT has consistently stronger results in the\nMany-to-one setting. However, in the one-to-Many setting BL-FT is better:\n\n\n    \n\n\nThen, they extended the bitext data to another 25 languages forming a dataset\nof 50 languages (ML 50 Benchmark) and they extended mBART embedding layers with\nrandomly initialized vectors for the extra set of 25 language tokens. The\nfollowing results are the improvement in BLEU compared to BL-Scratch which\nshow the effect as before which is ML-FT has consistently stronger results in\nthe Many-to-one setting:\n\n\n    \n\n\nThese results show that ML-FT is the best fine-tuning methods\nfor multilingual neural machine translation systems no matter\nthe number of languages supported.\n\nML50 Benchmark\n\nTo demonstrate the impact of multilingual fine-tuning on additional\nlanguages, the published created the ML50 Benchmark. ML50 standardizes\nthe training and evaluation schemes across 50 different languages, from\nextremely low resource languages like Xhosa and Gujarati to high\nresource languages like French and German. The full list of languages is\nshown in the following table:\n\n\n    \n\n\nThe languages surrounded by a red box are the new 25 languages added to\nthe benchmark. As we can see, there are five categories to the 50\nlanguages based on the amount of available training data.\n\nmBART-25 Vs mBART-50\n\nIn this part, we are going to discuss if adding additional languages to\nmBART is harmful for performance on the original 25 languages knowing\nthat the model remains the same size. The following figure show a\ncomparison between mBART-25 and mBART-50 over the original 25 languages\nwhen doing bilingual fine-tuning (BL-FT):\n\n\n    \n\n\nWe can see that the performance is almost exactly the same with both\nmodels, indicating that the number of languages can be doubled without\nloss of performance.That’s outstanding!! This means that we can extend\nmBART to other languages without loss of performance in the original\nlanguages.\n\nTo put things into perspective, the original mBART was trained for 2.5\nweeks on 256 Nvidia V100 GPUs on only 25 languages. There are hundreds\nof different languages in the world, so restarting pre-training from\nscratch to add any new language would be difficult. These results show\nthat extending mBART to another set of languages is possible using\nML-FT.\n"
      },
    
      
      
      {
        "collection": "Multilingual NMT",
        "title"     : "MTL: Multi-task Learning",
        "url"       : "/multilingual-nmt/MTL",
        "date"      : "06/10/2020",
        "content": "MTL stands for “Multi-task Learning” and it is a framework that jointly\ntrains multilingual neural machine translation (MNMT) models on bitext\ndata and monolingual data. The bitext data is used for the translation\ntask while the monolingual data is used for the denoising language\nmodeling tasks. This framework was proposed by Microsoft in 2020 and\npublished in their paper: Multi-task Learning for Multilingual Neural\nMachine Translation.\n\nRemember that multi-task learning (MTL) trains the model on several\ntasks to improve generalization performance. In this paper, the\ntranslation task is the main task combined with two denoising language\nmodeling tasks that help improving the quality of the translation task.\nThe two denoising tasks are:\n\n\n  Masked Language Model (MLM):\nThis task was first introduced in the\nBERT model\nwhere tokens are randomly masked and fed into the model and the\nmodel attempts to predict the masked tokens based on their context.\nBERT was an encoder-only architecture. To adapt this idea to the\nencoder-decoder architecture, they added an additional output layer\n(LM Head) during training which was dropped during inference.\n\n\n\n    \n\n\n\n  \n    Denoising Auto-encoding (DAE):\nThis task was first introduced in this paper back in 2008\nwhere the target-side sentence is fed to the encoder along with the\nLanguage ID and the decoder attempts to reconstruct the original\nsentence. In this paper, they introduced three different types of\nnoise functions:\n\n    \n    \n\n\n    \n      \n        Text Infilling: Same as\nmBART\nmodel, they randomly sampled text spans from the input with\nspan lengths drawn from a Poisson distribution\n($\\lambda = 3.5$) and replaced all words in each span with a\nsingle masking token.\n      \n      \n        Word Drop &amp;amp; word blank: They randomly sampled words from the\ninput sentence which were either removed or replaced with\nblanking tokens for each token position.\n      \n      \n        Word Swapping (Shuffling): Following this\npaper, they slightly\nshuffled the input sentence where a word at position $i$ can’t\nbe further than either $i - k$ or $i + k$, and $k = 3$ is the\nmaximum swapping distance.\n      \n    \n  \n\n\nIn the training process, the two self-learning objectives are combined with the\ncross-entropy loss for the translation task:\n\n\\[\\mathcal{L} = \\mathcal{L}_{\\text{MT}} + \\mathcal{L}_{\\text{MLM}} + \\mathcal{L}_{\\text{DAE}}\\]\n\n\n  Note:\nA language ID symbol $\\lbrack LID\\rbrack$ of the target language is\nappended to the input sentence in the translation and DAE tasks.\n\n\nDynamic Noise Ratio\n\nTraining algorithms perform better when starting with easier tasks. And\ngradually moving to harder ones and increasing the learning difficulty\ncan potentially help avoid saturation. Therefore, the researchers of\nthis paper has proposed a Dynamic Noise Ratio parameter that can\nbalance the difficulty level of MLM and DAE tasks by using a noising\nratio $R$ as a function of the training epochs $k$:\n\n\\[R\\left( k \\right) = \\min\\left( R_{m},\\ \\left( k - 1 \\right)\\frac{R_{m} - R_{0}}{M} + R_{0} \\right)\\]\n\nWhere $R_{0}$ and $R_{m}$ are the lower and upper bound for the noising\nratio respectively and $M$ is the number of warm-up epochs.\n\n\n  Note:\nIn case of MLM, the noising ratio $R$ refers to the masking ratio in\nMLM; the masking ratio is how many sequences will be masked. In BERT,\nthe masking ratio was 15%. In the case of DAE, the noising ratio $R$ is\nthe length of the blank span that needs to be filled.\n\n\nDynamic Temperature Sampling\n\nOne serious yet common problem for MNMT is data imbalance across\ndifferent languages. Training the model with uniform data distribution\nwould starve the low-resource language pairs since they are less\nprobable to be chosen. One approach to solve that issue is a\ntemperature-based sampling . Temperature sampling is an\neffective heuristic to up-sample the the probability of low-resource\npairs. It works like so; for language pair $l$ with bitext corpus\n$\\left| D_{l} \\right|$, the probability of sampling an instance of the\nsame language pair is:\n\n\\[p_{l} \\propto \\left( \\frac{\\left| D_{l} \\right|}{\\sum_{k}^{}\\left| D_{k} \\right|} \\right)^{\\frac{1}{T}}\\]\n\nWhile this sampling method improves translation quality for low-resource\nlanguages, performance gradually decreases for high resource languages.\nTo get over that, the researchers of this paper proposed a dynamic\ntemperature data sampling method that samples more high-resource\nlanguage pairs in the early stage of training and gradually shift more\nattention to the low-resource languages as shown in the following\nfunction where the temperature $T$ is a function of the number of\ntraining epochs $k$ and $N$ is the number of warm-up epochs. Also, they\nadded a lower-bound $T_{0}$ and a higher-bound $T_{m}$ for the\ntemperature.\n\n\\[T\\left( k \\right) = \\min\\left( T_{m},\\ \\left( k - 1 \\right)\\frac{T_{m} - T_{0}}{N} + T_{0} \\right)\\]\n\nFrom the past equation the sampling temperature starts from a smaller\nvalue $T_{0}$, resulting in sampling leaning towards true data\ndistribution and gradually increases low-resource languages more to\navoid them getting starved.\n\nThe following graph shows the performance gain of data sampling\nstrategies compared to the standard temperature sampling ($T = 5$) which\nclearly shows that dynamic temperature sampling has better performance\non low-resource language pairs. All results are reported as ∆ BLEU\nrelative to the corresponding bilingual baseline on validation sets:\n\n\n    \n\n\nData\n\nAs said earlier, the MTL framework jointly trains multilingual neural\nmachine translation (MNMT) model on bitext data and monolingual data.\nThe bitext data is used for the multilingual translation task and the\nmonolingual data is used for the denoising tasks.\n\nBitext data training data comes from the WMT corpus; they concatenated\nall resources except WikiTitles provided by WMT of the latest available\nyear and filtered out the duplicated pairs and pairs with the same\nsource and target sentence.\n\nThen, they tokenized all data with the SentencePiece model forming a\nvocabulary shared by all the source and target languages with $32k$\ntokens for bilingual models ($16k$ for Hi and Gu) and $64k$ tokens for\nmultilingual models. For validation, they randomly sampled $1,000$\nsentence pairs from each individual validation set and concatenated them\nto construct a multilingual validation set.\n\nThe following figure shows a list of 10 languages ranked by the size of\nthe bitext corpus translating to/from English:\n\n\n    \n\n\nRegarding the monolingual data, they mainly used data from\nNewsCrawl after applying a series\nof filtration rules to remove the low-quality sentences, including\nduplicated sentences, sentences with too many punctuation marks or\ninvalid characters, sentences with too many or too few words, etc. Then,\nthey randomly select $5M$ filtered sentences for each language. For\nlow-resource languages without enough sentences from NewsCrawl, they\nleveraged data from CCNet.\n\nExperiments\n\nRegarding the multilingual models, they used the\nfairseq implementation of the\nTransformer-big setting with a 6-layer encoder and decoder. The\ndimensions of word embeddings, hidden states, and non-linear layer were\nset as $1024$, $1024$ and $4096$ respectively, the number of heads for\nmulti-head attention was set as $16$.\n\nFor the low-resource bilingual models (Tr, Hi, and GU), they used a\nsmaller model setting with 3-encoder and decoder layers, $256$ embedding\nand hidden dimension to avoid overfitting and acquire better\nperformance.\n\nAll models were optimized with Adam with $\\beta_{1} = 0.9$,\n$\\beta_{2} = 0.98$. They set the learning rate schedule as the standard\ntransformer paper with initial learning rate $5 \\times 10^{- 4}$. Label\nsmoothing was adopted with $0.1$. And during inference, they used beam\nsearch with a beam size of $5$ and length penalty $1.0$.\n\nThe following table shows the BLEU scores of 10 languages → English\ntranslation with bilingual, X→En (many-to-English MNMT) and X→X\n(many-to-many MNMT) systems. The languages are arranged from\nhigh-resource (left) to low-resource (right):\n\n\n    \n\n\nThe following table shows the same as the previous one but when\nconsidering English → 10 languages translation:\n\n\n    \n\n\nThe results from the past two tables show that models trained with\nmultitask learning (+MTL) significantly outperform the multilingual and\nbilingual baselines demonstrating the effectiveness of the proposed\nframework.\n\n\n  Note:\nBT means “Back-translation” where they used target-to-source bilingual\nmodels to back translate the target-side monolingual sentences into the\nsource domain for each language pair. They used the same monolingual\ndata for back-translation as the multi-task learning.\n\n\nThey further evaluated the proposed approach on zero-shot translation of\nnon English-Centric language pairs. The following table shows that MTL\nframework significantly improves the zero-shot translation quality of\nthe X→X system especially for low-resource language pairs, further\ndemonstrating the effectiveness of the proposed approach:\n\n\n    \n\n\n\n  Note:\nFor the pivoting method, the source language was translated into\nEnglish first, and then translated into the target language.\n\n\nAt the end, the researchers of this paper compared the MTL framework\nwith mBART, the\nstate-of-the-art multilingual pre-training method for NMT. They\npre-trained mBART on\nCC25\ncorpus and fine-tuned it on the same bitext training data used in MTL.\nAs shown in the following figure, MTL outperforms mBART on all language\npairs:\n\n\n    \n\n\nThis suggests that in the scenario of NMT, jointly training the model\nwith MT task and self-supervised learning tasks could be a better task\ndesign than the separated pre-training and fine-tuning stages.\n\nAlso, It is worth noting that mBART is utilizing much more monolingual\ndata; for example, it uses 55B English tokens and 10B French tokens,\nwhile this approach is using just 100M tokens each. This indicates that\nMTL is more data efficient.\n"
      },
    
      
      
      {
        "collection": "Multilingual NMT",
        "title"     : "mRASP",
        "url"       : "/multilingual-nmt/mRASP",
        "date"      : "07/10/2020",
        "content": "mRASP stands for “multilingual Random Aligned Substitution Pre-training”\nwhich is a pre-training method for multilingual NMT models proposed by\nByteDance AI Lab in 2020 and published in their paper: “Pre-training\nMultilingual Neural Machine Translation by Leveraging Alignment\nInformation”. The official code\nfor this paper can be found on this GitHub repository:\nmRASP.\n\nmRASP is actually a standard Transformer-large architecture with 6-layer\nencoder and 6-layer decoder. The model dimension is 1,024 on 16 heads\nwith replacing ReLU (Rectified Linear Unit) with GeLU (Gate Linear Unit)\nas activation function on feed forward network. It also uses learned\npositional embeddings.\n\n\n    \n\n\nThe key idea in mRASP is its novel technique of RAS (Random Aligned\nSubstitution) pre-training, which brings words and phrases with similar\nmeanings across multiple languages closer in the representation space\nachieving a a common initial multilingual NMT model that can be later\nfine-tuned on any language pair.\n\nPre-training via RAS\n\nGiven a parallel sentence $\\left( x^{i},\\ x^{j} \\right)$ in two\ndifferent languages $L_{i}$ and $L_{j}$, RAS (Random Aligned\nSubstitution) randomly replaces a word $x_{t}^{i}$ at index $t$ in the\nsource language $i$ to a another word $d_{i,k}\\left( x_{t}^{i} \\right)$\nin a different random language $L_{k}$ using\nMUSE dictionary\nwhich is basically a look-up table trained in an unsupervised fashion\nthat is able to translate $x_{t}^{i}$ word in language $L_{i}$ to\n$d_{i,k}\\left( x_{t}^{i} \\right)$ word in language $L_{k}$ where\n$d_{i,k}$ is the dictionary translating function.\n\nAs we can see in the following figure, the words “singing” and “dancing”\nwere replaced by “chanter” and “danser” which have the same meaning in\nFrench. For RAS, they used the top 1000 words in dictionaries and only\nsubstituted words in source sentences. Each word is replaced with a\nprobability of $30\\%$ according to the En-X bilingual dictionaries. And\nif one word has multiple replacements, they randomly select one\nsubstitution from all candidates To address polysemy.\n\n\n    \n\n\nWith these replacement, the original bilingual pair\n$\\left( x^{i},\\ x^{j} \\right)$ will construct a code-switched\nsentence pair $\\left( C\\left( x^{i} \\right),\\ x^{j} \\right)$.\nConsidering a parallel dataset $\\mathcal{D}_{i,j}$ of language\npair $\\left( L_{i},L_{j} \\right)$ and $\\theta$ is the\nparameter of mRASP, the pre-training loss is defined as:\n\n\\[\\mathcal{L}^{\\text{pre}} = \\sum_{i,j \\in \\mathcal{D}}^{}{\\mathbb{E}_{\\left( x^{i},x^{j} \\right)\\sim\\mathcal{D}_{i,j}}\\lbrack - \\log P_{\\theta}\\left( x^{i} \\middle| C\\left( x^{j} \\right) \\right)\\rbrack}\\]\n\nIn pre-training phase, mRASP was trained using Adam optimizer with and\nlinear decay scheduling with $\\epsilon = 10^{- 8},\\ \\beta_{2} = 0.98$. A\nwarm-up step of 4000 is used. And the model was pre-trained for a total\nof 150000 steps. Also, a subword vocabulary of 64,808 tokens were\ncreated using BPE.\n\n\n  Note:\nTo distinguish from different translation pairs, they simply added\nlanguage tokens. For instance, the following En→Fr sentence “How are\nyou? → Comment vas tu?” is transformed to “&amp;lt;en&amp;gt; How are you? →\n&amp;lt;fr&amp;gt; Comment vas tu?”.\n\n\nPC32\n\nPC32 stands for “Parallel Corpus 32” and this is the data created for\ntraining mRASP containing 32 English-Centric Parallel data of 197\nmillion pair of sentences. This dataset was collected from various\nsources: ted,\nwmt,\neuroparl,\nparacrawl,\nopen-subtitles,\nqed. The following table summaries\nthe number of sentences of English-X where X denotes languages involved:\n\n\n    \n\n\nFine-tuning\n\nFor fine-tuning, they collected 14 pairs of parallel corpus to simulate\ndifferent scenarios. The collected data was divided into four\ncategories:\n\n\n  \n    Extremely low resource (&amp;lt;100K): such as En-Be (Belarusian), En-My\n(Burmese), En-Af (Afrikaans), and En-Eo (Esperanto).\n  \n  \n    Low resource(&amp;gt;100k and &amp;lt;1M): such as En-He (Hebrew), En-Tr\n(Turkish), En-Ro (Romanian), and En-Cs (Czech).\n  \n  \n    Medium resource (&amp;gt;1M and &amp;lt;10M): such as En-Ar (Arabic), En-Et\n(Estonian), En-Bg (Bulgarian), and En-De (German).\n  \n  \n    Rich resource (&amp;gt;10M): such as En-Zh (Chinese), and En-Fr (French).\n\n    mRASP model was fine-tuned on the target language pairs. They\napplied a dropout rate of 0.3 for all pairs except for rich resource\nwith 0.1. They carefully tuned the model, setting different learning\nrates and learning scheduler warm-up steps for different data scale.\nFor inference, we use beam-search with beam size 5.\n  \n\n\nExperiments\n\nTo better quantify the effectiveness of the proposed pre-training\nmethod, they compared mRASP with a direct model which is a randomly\ninitialized model. The following table shows that mRASP obtains better\nresults on different language-pairs:\n\n\n    \n\n\nTo illustrate the generalization of mRASP, they also conducted\nexperiments on translation directions that haven’t been seen in the\npre-training phase. The following table shows that mRASP obtains\nsignificant gains for each category for different scales of datasets,\nindicating that even trained with exotic languages, with pre-training\ninitialization, the model still works reasonably well.\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Multilingual NMT",
        "title"     : "cMNMT: Complete MNMT",
        "url"       : "/multilingual-nmt/cMNMT",
        "date"      : "20/10/2020",
        "content": "cMNMT stands for “Complete Multilingual Neural Machine Translation”\nwhich is a multilingual NMT model proposed by Google Research in 2020\nand published in their paper: “Complete Multilingual Neural Machine\nTranslation”. Multilingual Neural\nMachine Translation models are called complete when they are\ntrained for all possible source-target pairs. The following figure shows\nthe difference between the data of an English-Centric MNMT (left) and a\ncomplete MNMT (right) on a six-languages dataset:\n\n\n    \n\n\nThe publishers proposed this model as they noticed that MNMT models are\ncommonly trained on a joint set of bilingual corpora which is acutely\nEnglish-Centric (English either as the source or target language). While\ndirect data between two languages that are non-English is explicitly\navailable at times but not used. That’s why they proposed a new way\ncalled “Multi-way Aligned Data” that can make use of these unused\ndirections.\n\nMulti-way Aligned Data\n\nMulti-way Aligned Data is the method proposed by the researchers to\nalign training examples from different language pairs when either their\nsource or target sides are identical. So, if you have X↔Y parallel data\nand Z↔Y parallel data, then you can align them to get X↔Y↔Z as shown in\nthe following example where X is “German”, Y “English” and Z “Spanish”:\n\n\n    \n\n\nIn the paper, they used five more languages beside English (en) from the\npublic WMT datasets:\n\n\n  \n    Czech (cs): Data collected from WMT 2018.\n  \n  \n    French (fr): Data collected from WMT 2014.\n  \n  \n    German (de): Data collected from WMT 2014.\n  \n  \n    Spanish (es): Data collected from WMT 2013.\n  \n  \n    Russian (ru): Data collected from WMT 2018.\n  \n\n\nAs you might now, some of the datasets provided in these benchmarks\nare multi-way parallel by construction. As shown in the following\ntable, the vast majority (123 million) of the examples do only have\ntranslations into two languages while 10,000 sentences have\ntranslations in all 6 languages.\n\n\n    \n\n\nUsing this data, the researchers were able to construct non-English\nbilingual training examples by pairing the non-English sides of two\ntraining examples with identical English translations.\n\nModel\n\ncMNMT is a standard transformer-big model size that uses a vocabulary of\n64,000 subword units trained for 500,000 updates using an average batch\nsize of around 33,000 sentences (∼1 million tokens). Due to the data\nimbalance across languages, they used temperature-sampling to up-sample\nlow-resource language pairs as seen in the following equation where $p$\nis a language pair and $D\\left( p \\right)$ is the size of the available\nparallel data:\n\n\\[p_{p} = \\left( \\frac{D\\left( p \\right)}{\\sum_{q}^{}{D\\left( q \\right)}} \\right)^{\\frac{1}{T}}\\]\n\nTo study the performance of this model, it was compared with several\nbaselines on the newstest2013 dataset; All bilingual baselines used a\nvocabulary of 32,000 subwords, while all multilingual baselines used a\nvocabulary of 64,000 subword units. All multilingual models were trained\nfor 500,000 updates while bilingual models were trained for 400,000\nsteps as they converged earlier using a batch size of around 8,000\nsentences (∼260,000 tokens):\n\n\n  Bilingual models:\nThey trained two bilingual baselines (using either transformer-base\nor transformer-big) for each language pair. They experimented with\nseveral dropout rates and found that $dropout = 0.1$ works best for\ntransformer-base while $dropout = 0.3$ works best for\ntransformer-big. As can be seen from the following two tables, the\ntranslation quality of the non-English language pairs is far behind\nthe ones for English-centric pairs:\n\n\n\n    \n\n\n\n  MNMT based on English-centric data:\n**They trained a multilingual NMT model on the original WMT\nEnglish-centric training data. All non-English language pairs are\nunseen during training and BLEU scores measure zero-shot\nperformance. The following table shows that non-English language\npairs are consistently lower than the ones for English-Centric\npairs:\n\n\n\n    \n\n\n\n  Bridging non-English language pairs:\nFor the bridging approach, the source sentence cascades\nthrough the source→English and English→target systems to generate\nthe target sentence. The following table shows that the BLEU scores\nfor all non-English pairs are higher compared to all previous\nbaselines:\n\n\n\n    \n\n\nThe bridging process has several limitations:\n\n\n  \n    Translation errors accumulate in the pipeline.\n  \n  \n    Decoding time gets doubled since inference has to be run twice.\n  \n  \n    Bridging through a morphologically low language (i.e. English),\nimportant information could be lost (i.e. gender).\n  \n\n\nThe following table compares the performance of cMNMT with the best baseline\nmodel (the bridging one). As you can see, the BLEU scores for the non-English\nlanguage pairs go up from at least 1.4 BLEU to 5.0 BLEU. And when comparing our\ncMNMT model to the English-centric baseline, we can see an average BLEU\nincrease of 14.6 BLEU for all nonEnglish language pairs:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Multilingual NMT",
        "title"     : "M2M-100",
        "url"       : "/multilingual-nmt/M2M-100",
        "date"      : "21/10/2020",
        "content": "M2M stands for “Many-to-Many” which is a multilingual NMT model using\nmany-to-many datasets. The model was created by Facebook AI in 2020 and\npublished in their paper: “Beyond English-Centric Multilingual Machine\nTranslation”. The official code\nfor this paper can be found on the official FairSeq repository:\nm2m_100\n\nSince multilingual NMT models are hungry for data and the most\nresourceful language currently is English, most of the existing\nmultilingual NMT models use data either translated from or to English.\nThis English-Centric bias in the data results in models not reflective\nof how people use translation and empirically leads to lower performance\nfor non-English translation directions. M2M model is considered a true\nMany-to-Many multilingual translation model that can translate directly\nbetween any pair of 100 languages.\n\nOur focus on non-English-Centric models brings gains of more than 10\nBLEU when directly translating between non-English directions while\nperforming competitively to the best single systems of WMT.\n\nArchitecture\n\nM2M is basically a a large Transformer sequence-to-sequence model, which is\ncomposed of two modules: 12-layer encoder and 12-layer decoder. It contains 8192 Feed-Forward-Network size and 1024 embedding dimension. The weight matrices of\nthe input and output embeddings. The total parameter count is 1.2B.\n\nThe encoder takes the sequence of tokens\n$W = \\left( w_{1},\\ …w_{S} \\right)$ and the source language $l_{s}$\nand produces a sequence of embeddings of the same length\n$H = \\left( h_{1},\\ …h_{S} \\right)$:\n\n\\[H = \\text{encoder}\\left( W,\\ l_{s} \\right)\\]\n\nThen, the decoder is fed the sequence of embeddings $H$ with the target\nlanguage $l_{t}$ and sequentially produces the target sentence token by\ntoken $V = \\left( v_{1},\\ …v_{S} \\right)$:\n\n\\[v_{i + 1} = \\text{decoder}\\left( H,\\ l_{t},\\ v_{1}\\text{...}v_{i} \\right)\\]\n\nThe Transformer architecture has been designed for the bilingual case,\nwhere the target language is fixed. In the case of multilingual machine\ntranslation, the target language is not fixed, so they added a special\ntoken in the encoder indicating the source language $l_{s}$ and a\nspecial token in the decoder indicating the target language $l_{t}$.\n\n\n    \n\n\nThe model is trained with the Adam optimizer and warmed-up first for\n4000 updates, with label smoothing 0.1. For regularization, the dropout\nparameter is tuned between {0.1, 0.2, 0.3}. To stabilize the training of\ndeeper Transformers, it is trained with LayerDrop 0.05 and\npre-normalization.\n\nTo train M2M, they used SentencePiece as a subword tokenizer. And since\nSentencePiece produces subword units depending on their frequency in the\ntraining dataset which would favor high-resource languages, they decided\nto add monolingual data for low resource languages to compensate. Also,\nthey used a temperature sample which means that choosing language $l$\nequals $p_{l}^{\\frac{1}{T}}$. The following is the language distribution\nof the vocabulary containing 128k token:\n\n\n    \n\n\nBuilding Data\n\nThe superior performance of M2M-100 was achieved by building a\nlarge-scale Many-to-Many dataset comprising 7.5B training sentences for\n100 languages (you can find the full set of languages in page: 6 of the\npaper). We considerably reduce the complexity of this task through the\nautomatic construction of parallel corpora with a novel data mining\nstrategy that exploits language similarity to avoid mining all\ndirections. We also leverage back-translation to improve the quality of\nour model on zero-shot and low resource language pairs.\n\nCHECK THE PAPER FOR MORE DETAILS REGARDING HOW THEY MINDED THE DATA.…\nIT’S A LOT!!\n\nEvaluation Set\n\nTo cover their set of 100 languages and 2200 directions, they brought\ntogether data from a variety of sources. We describe each evaluation\ndataset below:\n\n\n  \n    WMT:\nThe majority of language pairs from WMT go through English and the\ndata is from the news domain. We consider data for 13 languages.\n  \n  \n    WAT:\nThe WAT competition covers Asian languages paired with English.\nThey used it for Burmese-English.\n  \n  \n    IWSLT:\nThe IWSLT translation competition contains data from TED talks\npaired with English translations. We use data for 4 languages.\n  \n  \n    FLORES:\nFLORES pairs two low resource languages, Sinhala and Nepali, with\nEnglish in the Wikipedia domain.\n  \n  \n    TED — The TED Talks dataset:\nIt contains translations between more than 50 languages; most of\nthe pairs do not include English. The evaluation data is n-way\nparallel and contains thousands of directions.\n  \n  \n    Autshumato:\nAutshumato is an 11-way parallel dataset comprising 10 African\nlanguages and English from the government domain.\n  \n  \n    Tatoeba:\nTatoeba Challenge covers 692 test pairs from mixed domains where\nsentences are contributed and translated by volunteers online. The\nevaluation pairs we use from Tatoeba cover 85 different languages.\n  \n\n\nEnglish-Centric Bias\n\nAs said before, English-Centric multilingual models have some bias that\nlowers their performance for non-English translation. To test that, they\ntrained the same architecture on the full 100 language Many-to-Many\ndataset and on the English part of the data using the same vocabulary.\nThey trained the model for 500K updates which corresponds to one pass\nover the entire Many-to-Many dataset and 3.5 passes on the\nEnglish-centric data.\n\nThen, they compared the performance of both models on different types of\ndirections, namely, any language to English (To English), English to any\nlanguage (From English), and all the directions not involving English\n(Non-English). And the following is the results averaging over all\ndirections:\n\n\n    \n\n\nOn the pairs including English, both models achieve similar performance.\nFor the non-English pairs, we consider two translation strategies for\nthe English-Centric model: directly translating as if the model was\ntrained on the pair – by using the corresponding language tokens – or\nby pivoting through English. The M2M model outperforms direct\ntranslation with the English-Centric model by 10.2 BLEU and when the\nEnglish-Centric model uses pivoting by 5.5 BLEU.\n\nWhile this result is not surprising, it confirms that a purely\nEnglish-Centric model has limited potential on non-English pairs, and\nthere is a fundamental need for training on Many-to-Many data.\n\nTO BE CONTINUED\n"
      },
    
      
      
      {
        "collection": "Multilingual NMT",
        "title"     : "XLM-T",
        "url"       : "/multilingual-nmt/XLM-T",
        "date"      : "31/12/2020",
        "content": "XLM-T stands for “Cross-lingual Language Modeling-Transformer” which is\na multi-lingual machine translation model proposed by Microsoft in 2020\nand published in their paper: XLM-T: Scaling up Multilingual Machine\nTranslation with Pretrained Cross-lingual Transformer\nEncoders. The official code of this\npaper can be found in Microsoft’s official GitHub repository:\nunilm/xlmt. Most\nexisting MNMTs adopt a randomly initialized Transformer backbone. In\nthis work, the researchers used an a pre-trained cross-lingual\nTransformer encoder such as\n(XLM or\nXLM-R) to\ninitialize both the encoder and decoder of the multilingual NMT model,\nand then fine-tuned it with multilingual parallel data.\n\n\n    \n\n\nIn this paper, the researchers have adopted XLM-R BASE as the pretrained\nencoder and conduct extensive experiments on multilingual machine translation\nwith 10 language pairs from WMT dataset and 94\nlanguage pairs from OPUS datasets; the\nstats of these datasets can be found on page 13 of the paper. XLM-T achieved\nsignificant and consistent gains on both high, medium and low-resource \nlanguages. The following table shows that clearly in comparison with the best\nsystem from this paper (Zhang et al., 2020)\non high/medium/low resource language pairs from OPUS-100 test sets.\n\n\n    \n\n\nInitialization Strategy\n\nXLM-R BASE model has 12-layer encoder, 6-layer decoder, 768 hidden size,\n12 attention head, and 3,072 dimensions of feed-forward layers and was\ntrained in 100 languages using more than two terabytes of filtered\nCommonCrawl data with MLM objective. In the paper, the researchers used\nit to initialize both the encoder and decoder of the Transformer-based\nmultilingual NMT model.\n\n\n  \n    Initializing cross-lingual encoder:\nTo initialize the encoder with pre-trained XLM-R, the researchers tried to\nmake the architectures consistent. To be able to do that, they did the\nfollowing:\n\n    \n      \n        They added a layer-normalization layer after the embedding layer\nand did not scale up the word embedding.\n      \n      \n        They used post layer normalization for both the attention layers\nand feed-forward layers.\n      \n      \n        They changed the activation function inside the feed-forward\nlayers to be GELU instead of ReLU.\n      \n    \n  \n\n\n\n    \n\n\n\n  Initializing cross-lingual decoder:\nThe architecture of the decoder is the same as that of the encoder, except\nthat there is a cross-attention layer after the self-attention layer.\n\n\nTo analyze the effect this initialization strategy, they conducted an\nablation study by removing the encoder initialization and decoder\ninitialization. The following table summarizes the results:\n\n\n    \n\n\nIt shows that the encoder initialization mainly contributes to the\nimprovements of X → En. Similarly, the decoder initialization mainly\nbenefits E → X. Moreover, it concludes that the encoder initialization\ncontributes to more gains than the decoder initialization for\nmultilingual NMT. Now, we have our pre-trained encoder-decoder MNMT\nmodel ready to be fine-tuned. Let’s see how fine-tuning was performed.\n\nMultilingual Fine-tuning\n\nSuppose we have $L$ languages to translate in a model. Among these\nlanguages, we have $N$ bilingual corpora, each of which contains\nparallel sentences\n$\\left\\{ \\left( x_{L_{i}}^{1},\\ x_{L_{j}}^{1} \\right),\\left( x_{L_{i}}^{2},\\ x_{L_{j}}^{2} \\right),\\ …\\left( x_{L_{i}}^{k},\\ x_{L_{j}}^{k} \\right) \\right\\}$\nbetween language $L_{i}$ and language $L_{j}$, where $k$ is the number\nof training instances. Given the corpora, we are able to train a\nmultilingual model $P_{\\theta}$ that enables the translation among\ndifferent languages. With the parallel data of $N$ language direction,\nthe model is learned with the following objective:\n\n\\[L = - \\sum_{i}^{N}{\\sum_{j \\neq i}^{N}{\\log\\left( P_{\\theta}\\left( x_{L_{i}}^{k},\\ x_{L_{j}}^{k} \\right) \\right)}}\\]\n\nA simple concatenation of all parallel data will lead to poor\nperformance on low-resource translation because of the imbalanced data.\nThe researchers in the paper used temperature sampling to up-sample\nlow-resource languages. For a given language pair $l$ with $D_{l}$\nparallel sentences, the probability of the sample being from language\n$L_{i}$ using this strategy is:\n\n\\[p_{L_{i}} = \\left( \\frac{D_{L_{i}}}{\\sum_{j}^{N}D_{L_{j}}} \\right)^{\\frac{1}{T}}\\]\n\nTo reduce over-sampling of low-resource languages in the early stage of\ntraining, they employed a dynamic temperate sampling mechanism. The\ntemperature is low at the beginning of training and is gradually\nincreased for the first several epochs as shown in the following\nformula:\n\n\\[T_{i} = \\min\\left( T,\\ T_{0} + \\frac{i}{N}\\left( T - T_{0} \\right) \\right)\\]\n\nwhere $T_{0}$ is the initial temperature, $T$ is the peak temperature,\nand $N$ is the number of warming-up epochs. In the paper, they used\n$T_{0} = 1.0$, $T = 5.0$, and $N = 5$ for all of their experiments.\n\nTraining &amp;amp; Testing\n\nThe model was trained using Adam Optimizer with $\\beta_{1} = 0.9$ and\n$\\beta_{2} = 0.98$. The learning rate is in the range of $3e^{- 4}$ to\n$5e^{- 4}$ with a warming-up step of $4,000$. Label smoothing\ncross-entropy was used with $0.1$ smoothing ratio. The dropout of\nattention layers was set to $0.0$, while being $0.1$ for the rest.\n\nThe source length and the target length were limited to be\n$256\\ $tokens. For the WMT-10 dataset, the batch size was $4,096$ and\nthe gradients were accumulated for $16$ batches. For the OPUS-100\ndataset, the batch size was $2,048$ and the gradients were updated every\n$32$ batches. During testing, they used beam search with a beam size of\n$5$ with $1.0$ as length penalty. All results below were obtained by\naveraging the last 5 checkpoints.\n\nIn this paper, they used two baselines one is bilingual and the other is\nmany-to-many; both are Transformer-big architecture with a 6-layer\nencoder and decoder and the embedding size, the hidden size, embedding\nsize and the number of attention head is 1024, 1024, and 16 respectively\nwhile the dimension of feed-forward layer is 4096. These two baselines\nwere using SentencePiece model with a vocabulary size of 64,000 tokens\nextracted from the training set.\n\nThe following table shows the BLUE score bilingual, many-to-English, and\nmany-to-many models on WMT-10. On the top are the models trained with\noriginal parallel data, while the bottom are combined with\nback-translation. The languages are ordered from high-resource (left) to\nlow-resource (right).\n\n\n    \n\n\nAnd the following table is the same as before but when the source\nlanguage is English instead of English being the target as in the\nprevious table:\n\n\n    \n\n\nThe previous two tables were showing the results on WMT-10 dataset. The\nfollowing table shows the BLEU score in comparison with the best model\nin this paper (Zhang et al.,\n2020) on high/medium/low resource\nlanguage pairs in OPUS-100 test sets. “WR” is the win ratio (%) compared\nto ref:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Multilingual NMT",
        "title"     : "mRASP2",
        "url"       : "/multilingual-nmt/mRASP2",
        "date"      : "20/05/2021",
        "content": "mRASP2 stands for “multilingual Random Aligned\nSubstitution Pre-training”. It’s mRASP2 because it’s\nan extension to the\nmRASP model\nproposed by the same lab (ByteDance AI Lab) a year earlier. mRASP2\nframework was proposed in 2021 and published in this paper: Contrastive\nLearning for Many-to-many Multilingual Neural Machine\nTranslation. The official code\nfor this paper can be found in this GitHub repository:\nmRASP2.\n\nmRASP2, as shown in the following figure, is a framework for training\nmany-to-many multilingual neural machine translation models using both\nparallel corpora and monolingual corpora. This framework is empowered by\ntwo techniques:\n\n\n  \n    mCOLT: a contrastive learning scheme for the encoder to close\nthe gap among representations of similar sentences across different\nlanguages.\n  \n  \n    Aligned Augmentation (AA): Data augmentation on both parallel\nand monolingual data to create pseudo-pairs to improve multilingual\ntranslation quality.\n  \n\n\n\n    \n\n\nThe base architecture of mRASP2 is the state-of-the-art\nTransformer.\nA little different from\nmRASP, they chose a larger\nsetting with a 12-layer encoder and a 12-layer decoder to increase the model\ncapacity. The model dimension is $1024$ on $16$ heads. To ease the training of\nthe deep model, they applied Layer Normalization for word embedding and\npre-norm residual connection for both encoder and decoder.\n\nMore formally, $D$ denotes all parallel datasets involved in training where\n$D_{i,j}$ denotes a parallel dataset of $\\left( L_{i},\\ L_{j} \\right)$ language\npair. To distinguish different languages, they added an additional language\nidentification token preceding each sentence, for both source side and target\nside.\n\nmCOLT\n\nmCOLT stands for “multilingual Contrastive Learning for\nTranslation” which is a contrastive loss function for the encoder.\nIts key idea is to minimize the representation gap of similar sentences\nof different languages and maximize that of irrelevant sentences. More\nformally, given a bilingual translation pairs\n$\\left( x^{i},\\ x^{j} \\right) \\in D$ where\n$\\left( x^{i},\\ x^{j} \\right)$ is a positive example, and\n$\\left( x^{i},\\ y^{j} \\right)$ is a negative example as $y^{j}$ is\nrandomly sampled from the same language $L_{j}$. The objective of\ncontrastive learning is to minimize the following loss:\n\n\\[\\mathcal{L}_{\\text{ctr}} = \\sum_{x^{i},x^{j} \\in D}^{}{- \\log\\left( \\frac{\\frac{e^{\\text{sim}^{+}\\left( \\mathcal{R}\\left( x^{i} \\right),\\ \\mathcal{R}\\left( x^{j} \\right) \\right)}}{t}}{\\sum_{y^{j}}^{}\\frac{e^{\\text{sim}^{-}\\left( \\mathcal{R}\\left( x^{i} \\right),\\ \\mathcal{R}\\left( y^{j} \\right) \\right)}}{t}} \\right)}\\]\n\nWhere:\n\n\n  \n    $sim()$ calculates the cosine similarity of different sentences.\n$sim +$ and $sim -$ denote positive and negative similarity\nrespectively. To simplify implementation, the negative samples are\nsampled from the same training batch.\n  \n  \n    $\\mathcal{R}\\left( x \\right)$ denotes the encoded output of an\narbitrary sentence $x$.\n  \n  \n    $t$ is the temperature. Higher temperature increases the difficulty\nto distinguish positive sample from negative ones. In the paper,\ntemperature was set to $0.1$.\n  \n\n\nNow, the training loss $\\mathcal{L}$ used for training mRASP2 is a combination\nof two loss functions; the contrastive loss $\\mathcal{L}_{\\text{ctr}}$\ndefined above and the cross entropy $\\mathcal{L}_{\\text{ce}}$:\n\n\\[\\mathcal{L} = \\mathcal{L}_{\\text{ce}} + \\lambda\\left| s \\right|\\mathcal{L}_{\\text{ctr}}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathcal{L}_{\\text{ce}} = \\sum_{x^{i},x^{j} \\in D}^{}{- \\log\\left( P_{\\theta}\\left( x^{i} \\middle| x^{j} \\right) \\right)}\\]\n\nWhere\n\n\n  \n    $\\lambda$ is the coefficient to balance the two training losses. In\nthe paper, it was set to $1.0$.\n  \n  \n    $\\left| s \\right|$ is the average sequence length since\n$\\mathcal{L}_{\\text{ctr}}$ is calculated on the sentence-level\nand $\\mathcal{L}_{\\text{ce}}$ is calculated on the token-level.\n  \n  \n    $x^{i}$ and $x^{j}$ represent sentences in language $L_{i}$ and\n$L_{j}$ respectively.\n  \n  \n    $\\theta$ is the parameter of multilingual Transformer model.\n  \n\n\nAligned Augmentation\n\nAligned Augmentation (AA) is a data augmentation technique that can be\napplied on both parallel and monolingual data in order to improve\nmultilingual translation quality. Aligned Augmentation is considered an\nextension of RAS (Random Aligned Substitution) which was proposed in\nmRASP paper.\n\nFor a bilingual sentence pair $\\left( x^{i},\\ x^{j} \\right)$ in two\nlanguages $L_{i}$ and $L_{j}$, Aligned Augmentation creates a perturbed\nsentence $C\\left( x^{i} \\right)$ by replacing aligned words from a\nMUSE synonym\ndictionary with a probability of $90\\%$ and keep them unchanged\notherwise; forming a pseudo-parallel training example\n$\\left( C\\left( x^{i} \\right),\\ x^{j} \\right)$:\n\n\n    \n\n\nFor a monolingual sentence $x^{i}$ of language $L_{i}$, Aligned\nAugmentation creates a perturbed sentence $C\\left( x^{i} \\right)$ the\nsame way as the bilingual sentence; forming a pseudo self-parallel\nexample $\\left( C\\left( x^{i} \\right),\\ x^{i} \\right)$:\n\n\n    \n\n\nNow, both a pseudo-parallel training example\n$\\left( C\\left( x^{i} \\right),\\ x^{j} \\right)$ and a pseudo\nself-parallel example $\\left( C\\left( x^{i} \\right),\\ x^{i} \\right)$\nwill be used to increase the training data and therefore boosting the\nmultilingual translation quality.\n\nExperiments\n\nIn their experiments, they used the\nTransformer\nmodel with 12 encoder layers and 12 decoder layers. The embedding size\nand FFN dimension were set to $1024$ on $16$ heads. For multilingual\nvocabulary, They followed the shared BPE vocabulary of $64,808$ tokens\nplus $59$ language tokens such as\n$\\left\\langle \\text{EN\\ id} \\right\\rangle$,\n$\\left\\langle \\text{FR\\ id} \\right\\rangle$...etc.\n\nThey also used a dropout rate of $0.1$, as well as a learning rate of\n$3e^{- 4}$ with polynomial decay scheduling and a warm-up step of $10k$.\nFor optimization, they use Adam optimizer with $\\epsilon = 1e^{- 6}$ and\n$\\beta_{2} = 0.98$. To stabilize training, they set the threshold of\ngradient norm to be $5.0$ and clip all gradients with a larger norm.\n\nFor bilingual data, they used the Parallel Corpus (PC32) proposed in the\nmRASP paper. PC32\ncontains 97.6 million parallel sentences of 32 English-centric language\npairs. The following figure shows the languages found in the corpus\nalong with the number of sentences available for each language:\n\n\n    \n\n\nFor monolingual data, they created Monolingual Corpus (MC24) of 1.01\nbillion sentences of 24 languages. MC24 is a subset of the Newscrawl\ndataset by using only those\nlanguages in PC32, plus three additional languages (Nl, Pl, Pt)\nhighlighted in red in the following figure:\n\n\n    \n\n\nIn order to balance the volume across different languages, they applied\ntemperature sampling of $T = 5$. For a given language pair $l$ with\n$D_{l}$ number of parallel sentences, the probability of the sample\nbeing from language $l$ is:\n\n\\[p_{l} = \\left( \\frac{D_{l}}{\\sum_{k}^{}D_{k}} \\right)^{\\frac{1}{T}}\\]\n\nResults\n\nThe following table shows the BLEU score of different models (bilingual,\npre-trained then fine-tuned, and multilingual) on the evaluation sets of\nWMT benchmark. As shown in the following table, mRASP2 clearly improves\nmultilingual baselines by a large margin in 10 translation directions.\n\n\n    \n\n\n\n  Note:\nm-Transformer is a many-to-many 12 layers standard transformer model\ntrained on PC32 dataset and used as a baseline. As we can see from the\npast table, this model achieves very competitive results and they\nexplained that that was due to these reasons:\n\n  \n    \n      They used a batch size of 3 million tokens. The batch size plays a\n  crucial role in the success of training multilingual NMTs.\n    \n    \n      They used gradient norm to stable the training. Without it, the\n  large scale training will collapse sometimes.\n    \n  \n\n\nThe following table shows the BLEU score of unsupervised translation on IWSLT,\nWMT, and OPUS-100 evaluations sets. These three models were trained on only\nmonolingual data and mRASP2 outperforms them by a huge margin on all\nlanguage-pairs:\n\n\n    \n\n\nThe following table shows the BLEU score of zero-shot translation on OPUS-100\nevaluations sets. As we can see, mRASP2 achieves consistent BLEU gains in\nzero-shot directions on different evaluation sets:\n\n\n    \n\n\nTo understand what contributes to the performance gain, they conducted\nanalytical experiments and reported the results in the following table:\n\n\n    \n\n\nAnd they found out the following:\n\n\n  \n    ③ performs comparably with ① in supervised and unsupervised scenarios,\nwhereas achieves a substantial BLEU improvement for zero-shot translation. This indicates that by introducing contrastive loss, we can\nimprove zero-shot translation quality without harming other\ndirections.\n  \n  \n    ① and ② perform poorly for zero-shot directions. This means\ncontrastive loss is crucial for the performance in zero-shot\ndirections.\n  \n  \n    mRASP2 further improves BLEU in ③, ④, and ⑤ especially in unsupervised\ndirections. Therefore it is safe to say that mRASP2 learns a\nbetter representation space by using monolingual data.\n  \n\n"
      },
    
      
      
      {
        "collection": "Multilingual NMT",
        "title"     : "Multilinguality in Transformers",
        "url"       : "/multilingual-nmt/multilinguality_in_transformers",
        "date"      : "31/05/2021",
        "content": "The following paper: Do Multilingual Neural Machine Translation Models\nContain Language Pair Specific Attention\nHeads? asks a very good question.\nTo answer it, the publishers tried to measure the importance of the\nself-attention heads in the encoder and the encoder-decoder attention\nheads of a many-to-one transformer. The NMT model was able to translate\nFrench, German, Italian, Spanish, and Korean sentences to English. It\nuses a variant of the Transformer-Big architecture with a shallower\ndecoder: 16 attention heads, 6 encoder layers, and 3 decoder layers on\nTED2020 dataset.\n\nDenoting $\\left| I \\right|,\\ \\left| J \\right|$ as the number of source\ntokens and/or target tokens depending on whether we looked at the\nself-attention of encoder or the encoder-decoder cross attentions, The\nmetrics used for importance are three:\n\n\n  Confidence:\nIt is the mean of its maximum attention weights.\n\n\n\\[\\text{conf}\\left( \\text{head} \\right) = \\frac{1}{\\left| I \\right|}\\sum_{i \\in I}^{}{\\max_{j \\in J}\\alpha_{i,j}}\\]\n\n\n  Variance:\nIt’s measured by how much each individual position $i$ is away from\nthe expected position $\\mu_{i}$:\n\n\n\\[\\text{var}\\left( \\text{head} \\right) = - \\sum_{i \\in I}^{}{\\sum_{j \\in J}^{}{\\alpha_{i,j}\\left( \\mu_{i} - j \\right)^{2}}}\\ \\ \\ \\ \\ \\ \\ \\ \\mu_{i} = \\sum_{j \\in J}^{}{\\text{j.}\\alpha_{i,j}}\\]\n\n\n  Coverage:\nIt measures the amount of attention a source token has received.\n\n\n\\[\\text{cov}\\left( \\text{head} \\right) = \\sum_{j \\in J}^{}\\left( \\sum_{i \\in I}^{}\\alpha_{i,j} \\right)^{2}\\]\n\nAccording to the paper, the most important heads are\nlanguage-independent as you can see in the following figure:\n\n\n    \n\n\n\n  Note:\nEven though most important heads are language-independent, in the\npaper they showed that it is possible to find the rare heads specific to\na language pair via the extensive SBS (sequential backward selection)\nprocedure.\n\n"
      },
    
      
      
      {
        "collection": "Multilingual NMT",
        "title"     : "CeMAT",
        "url"       : "/multilingual-nmt/CeMAT",
        "date"      : "17/03/2022",
        "content": "CeMAT stands for “ Conditional masked language pretraining model\nfor Machine Translation” which is a bidirectional encoder and a\nbidirectional decoder multilingual\nTransformer\nmodel with a cross-attention module for bridging them. CeMAT was\nproposed by Huawei Noah’s Ark Lab in 2022 and published in their paper:\nUniversal Conditional Masked Language Pre-training for Neural Machine\nTranslation. The official code\nfor this paper can be found in Huawei Noah’s Ark Lab official GitHub\nrepository:\nhuawei-noah/CeMAT.\n\n\n    \n\n\nBenefiting from the bidirectional decoder structure, CeMAT can provide\nunified initialization parameters not only for Autoregressive\ntranslation, but also for non-autoregressive translation (NAT) directly.\nNAT has been attracting more and more attention because of its feature\nof parallel decoding, which helps to greatly reduce the translation\nlatency.\n\n\n    \n\n\nAs seen in the following figure, CeMAT follows the\npre-training-then-fine-tuning paradigm where the model is jointly\npre-trained using Masked Language Modeling (MLM) on the encoder side and\nConditional MLM (CMLM) on the decoder side with large-scale monolingual\nand bilingual texts in many languages.\n\n\n    \n\n\nPre-training\n\nAs said earlier, CeMAT is jointly trained by MLM and CMLM on the source\nside and the target side, respectively. MLM was first proposed in\nBERT while CMLM was\nproposed by\nMask-Predict\npaper. MLM predicts masked tokens given the remaining sentence, CMLM\npredicts masked tokens given the source sentence + the remaining of\ntarget sentence.\n\nGiven a training data of $M$ language-pairs\n$D = \\left\\{ D_{1},\\ D_{2},\\ …D_{M} \\right\\}$ where\n$D_{i}\\left( m,n \\right)$ is a collection of sentence pairs in language\n$L_{m}$ and $L_{n}$, respectively. A sentence pair is denoted\n$\\left( X_{m},Y_{n} \\right) \\in D_{i}\\left( m,n \\right)$, where $X_{m}$\nis the source text in the language $L_{m}$, and $Y_{n}$ is the\ncorresponding target text in the language $L_{n}$. For monolingual\ncorpora, they create pseudo bilingual text by copying the sentence\n$\\left( X_{m},X_{m} \\right)$ and $\\left( Y_{n},Y_{n} \\right)$.\n\nTo enhance model’s pre-training, they introduced a novel two-step\nmasking strategy on both monolingual and bilingual corpora:\n\n\n  \n    Aligned code-switching &amp;amp; Masking.\n  \n  \n    Dynamic dual-masking.\n  \n\n\nAligned Code-Switching &amp;amp; Masking\n\nTo replace the source word or phrase with a new word in another\nlanguage, they use a multilingual translation dictionary provided by\nMUSE with this\nmethod which consists of three steps:\n\n\n  \n    Aligning:\nUtilize a multilingual translation dictionary to get a set\nof aligned words\n$A = \\left\\{ …,\\ \\left( x_{m}^{i},\\ y_{n}^{j} \\right),\\ … \\right\\}$.\nThe word pair $\\left( x_{m}^{i},\\ y_{n}^{j} \\right)$ denotes that\nthe $i^{\\text{th}}$ word in the source $X_{m}$ and $j^{\\text{th}}$\nword in the target $Y_{n}$ are translations of each other.\n  \n  \n    Code-Switching Replace (CSR):\nGiven an aligned word pair $\\left( x_{m}^{i},\\ y_{n}^{j} \\right)$,\nthey select a new replacement word ${\\widehat{x}}_k^i$ that is a\ntranslation of $x_m^i$ in language $L_k$. The new word\n${\\widehat{x}}_k^i$ is randomly selected from a multilingual\ndictionary.\n  \n  \n    Code-Switching Masking (CSM):\nAfter replacing $x_m^i$ with ${\\widehat{x}}_k^i$,\n$y_n^j$ is masked with a universal\n$\\left\\lbrack \\text{mask} \\right\\rbrack$ token.\n  \n\n\nThen, CeMAT will be trained to predict it in the output layers of\nthe bidirectional decoder. The following figure shows the process of\naligned code-switching &amp;amp; masking. According to the following\nexample, “dance”, “tanzen”, and “danse”. “danse” is selected to\nreplace “dance”, and “tanzen” is replaced by\n$\\left\\lbrack \\text{mask} \\right\\rbrack$.\n\n\n    \n\n\nDuring pre-training, at most $15\\%$ of the words in the sentence\nwill be performed by CSR and CSM. For monolingual data, we set this\nratio to $30\\%$. After this process, the translation sentence pair\n$\\left( X_{m},Y_{n} \\right)$ becomes\n$\\left( \\text{CSR}\\left( X_{m} \\right),\\text{CSM}\\left( Y_{n} \\right) \\right)$\nand it will be further dynamically dual-masked at random as we are\ngoing to see next.\n\nDynamic Dual Masking\n\nLimited by the\nMUSE dictionary,\nthe ratio of aligned word pairs is usually small, around 6% of the\nbilingual corpora. To further increase the training efficiency, they\nperformed dynamic dual-masking on both bilingual and monolingual data\nwhere $10\\%$ of masked tokens are replaced with a random token, $10\\%$\nremain unchanged, and $80\\%$ are replaced with\n$\\left\\lbrack \\text{mask} \\right\\rbrack$ token:\n\n\n  \n    Bilingual Data:\n\n    \n      \n        They randomly select a subset of the target words and mask them\nwith a ratio of $u \\in \\mathcal{U}\\left( 0.2,\\ 0.5 \\right)$\nsampled from a uniform distribution.\n      \n      \n        Then, they randomly select a subset of the source words and mask\nthem with a ratio of  $\\mu \\in \\mathcal{U}\\left( 0.1,\\ 0.2 \\right)$\nsampled from a uniform distribution where $\\mu \\leq u$ to force the\nbidirectional decoder to obtain more information from the encoder.\n      \n    \n  \n  \n    Monolingual Data: Since the source and target are identical\nbefore masking, they sample\n$u = \\mu \\in \\mathcal{U}\\left( 0.3,\\ 0.4 \\right)$ from a uniform\ndistribution and mask the same subset of words on both sides. This\nwill avoid the decoder directly copying the token from the source.\n  \n\n\nThe following figure shows that the word “gras” from the target\nsentence and “on” from the source sentence were dynamically masked;\nboth are highlighted with yellow.\n\n\n    \n\n\nAfter applying these two steps, we jointly train the encoder and\ndecoder on MLM and CMLM tasks. Given the masked sentence pair\n$\\left( \\text{DM}\\left( \\text{CSR}\\left( X_{m} \\right) \\right),DM\\left( \\text{CSM}\\left( Y_{n} \\right) \\right) \\right)$\nwhich will be denoted as\n$\\left( {\\widehat{X}}_m,{\\widehat{Y}}_n \\right)$ for simplicity,\nthe final training objective is formulated as follows:\n\n\\[\\mathcal{L} = - \\sum_{\\left( {\\widehat{X}}_{m},{\\widehat{Y}}_{n} \\right) \\in \\widehat{D}}^{}{\\left( 1 - \\lambda \\right)\\mathcal{L}_{\\text{MLM}} + \\lambda\\mathcal{L}_{\\text{CMLM}}}\\]\n\n\\[\\mathcal{L}_{\\text{MLM}} = \\sum_{y_{n}^{j} \\in y_{n}^{\\text{mask}}}^{}{\\log\\left( P\\left( x_{n}^{j} \\middle| {\\widehat{X}}_{m} \\right) \\right)}\\]\n\n\\[\\mathcal{L}_{\\text{CMLM}} = \\sum_{x_{m}^{j} \\in x_{m}^{\\text{mask}}}^{}{\\log\\left( P\\left( y_{n}^{j} \\middle| {\\widehat{X}}_{m},{\\widehat{Y}}_{n} \\right) \\right)}\\]\n\nWhere $x_{m}^{\\text{mask}}$ are the set of masked source words and\n$y_{n}^{\\text{mask}}$ are the set of masked target words. And and\n$\\lambda$ is a hyper-parameter to balance the influence of both tasks.\nIn the paper, it was set $\\lambda = 0.7$.\n\nFine-tuning\n\nAs said earlier, CeMAT has a bidirectional decoder which can be\nfine-tuned on either autoregressive translation or non-autoregressive\ntranslation.\n\n\n  Autoregressive Translation: In this setup, CeMAT consists of a\nbidirectional encoder and a unidirectional decoder. The encoder maps\na source sentence $X_{m}$ into hidden representations which are then\nfed into the decoder which predicts the $t^{\\text{th}}$ token in a\ntarget language $L_{n}$ conditioned on $X_{m}$ and the previous\ntarget tokens $y_{n}^{&amp;lt; t}$. The training objective of\nautoregressive translation is to minimize the negative\nlog-likelihood:\n\n\n\\[\\mathcal{L}\\left( \\theta \\right) = \\sum_{\\left( X_{m},Y_{n} \\right) \\in D\\left( m,n \\right)}^{}{\\sum_{t = 1}^{\\left| Y_{n} \\right|}{- \\log\\left( P\\left( y_{n}^{t} \\middle| X_{m},\\ y_{n}^{&amp;lt; t};\\ \\theta \\right) \\right)}}\\]\n\n\n  Non-autoregressive Translation: In this setup, CeMAT consists of\na bidirectional encoder and a bidirectional decoder which can be\nused to predict the target sequences in parallel. The training\nobjective of NAT is formulated as follows:\n\n\n\\[\\mathcal{L}\\left( \\theta \\right) = \\sum_{\\left( X_{m},Y_{n} \\right) \\in D\\left( m,n \\right)}^{}{\\sum_{y_{n}^{i} \\in y_{n}^{\\text{mask}}}^{}{- \\log\\left( P\\left( y_{n}^{t} \\middle| X_{m},\\ y_{n}^{\\backslash mask};\\ \\theta \\right) \\right)}}\\]\n\nExperiments\n\nFor pre-training, they used the English-centric multilingual parallel\ncorpora of\nPC32, and\nthen collected 21-language monolingual corpora from common\ncrawl. Then, BPE tokenization was used on the\nentire data sets after tokenization using\nMoses-decoder for most languages and\nKyTea for Japanese and\njieba for Chinese. The full statistics\nof the data used are shown in the following table:\n\n\n    \n\n\nFor pre-training, they used a\nTransformer\narchitecture with 6-layer encoder and 6-layer bidirectional decoder with\na model dimension of $1024$ and $16$ attention heads that use sinusoidal\npositional embedding with pre-norm residual connection. They pre-trained\nthe model using Adam optimizer\n($\\epsilon = e^{- 6},\\ \\beta_{1} = 0.9,\\ \\beta_{2} = 0.98$) for $300K$\nsteps with a batch size of 4096 tokens. Also, they used polynomial decay\nscheduling with a warm-up step of $10,000$.\n\nAfter pre-training, they fine-tuned the model on autoregressive\ntranslation of 8 popular language pairs (shown in the following table)\nthat can be divided into four categories according to their size:\nlow-resource ($\\left\\lbrack &amp;lt; 1M \\right\\rbrack$), medium-resource\n($\\left\\lbrack 1M,\\ 10M \\right\\rbrack$), high-resource\n($\\left\\lbrack 10M,\\ 25M \\right\\rbrack$), and extremely high-resource\n($\\left\\lbrack &amp;gt; 25M \\right\\rbrack$).\n\n\n    \n\n\nThe following table shows that CeMAT outperforms\nmBART and\nmRASP for all\nlanguage pairs but two directions. As the scale of the dataset\nincreases, the benefits of pre-training models are getting smaller and\nsmaller\n\n\n    \n\n\nThey further compare CeMAT with more existing multilingual pre-trained\nmodels on three popular translation directions, including WMT14 En→De,\nWMT16 En↔Ro. The followig table show that CeMAT obtains competitive\nresults on these languages pairs on average, and achieves the best\nperformance on En→Ro.\n\n\n    \n\n\n\n  Note:\nThe “Direct” baseline mentioned in earlier results is a\nmask-predict\nmodel.\n\n\nAnd for NAT fine-tuning, they evaluated CeMAT on three popular datasets:\nWMT14 En↔De, WMT16 En↔Ro and IWSLT14 En↔De. For a fair comparison with\nbaselines, they only used the bilingual PC32 corpora to pre-train CeMAT\nand they used knowledge distillation on WMT14 En↔De tasks. The following\ntable shows that CeMAT outperforms other multilingual models. This\nsuggests that we can use the traditional pre-training method to\nfine-tune the NAT task.\n\n\n    \n\n\nAs an ablation study, they trained CeMAT without some of the proposed\ntechniques and the following table shows that all the highest\nperformance is achieved when using all proposed techniques:\n\n\n    \n\n"
      },
    
  
  
    
    
  
  
    
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "t-SNE",
        "url"       : "/word-embedding/t-SNE",
        "date"      : "25/11/2008",
        "content": "One of the popular things to do with word embedding, is to take this\nN-dimensional data and embed it in a two-dimensional space so that we\ncan visualize them. The most common algorithm for doing this is the\nt-SNE algorithm created by Laurens van der Maaten and Geoffrey\nHinton in 2008 and published in this paper: Visualizing data using\nt-SNE.\n\nt-SNE stands for t-distributed Stochastic Neighbor Embedding which is a\nmachine learning algorithm for visualization, often used to visualize\nhigh-level representations learned by an artificial neural network,\ndeveloped .\n\n\n    \n\n\nAnd if you look at one of these embeddings, you find that words like\n“man” and “woman” tend to get grouped together, “king” and “queen” tend\nto get grouped together, and these four are the people which tends to\nget grouped together. “dog”, “cat” and “fish” are animals which can get\ngrouped together. Fruits will tend to be close to each other. Numbers\nlike “one”, “two”, “three”, “four”, will be close to each other.\n\nTO BE CONTINUED…\n"
      },
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "Word2Vec",
        "url"       : "/word-embedding/word2vec",
        "date"      : "07/09/2013",
        "content": "Word2Vec stands for “word-to-vector” is a model architecture created by\nTomáš Mikolov from Google in 2013 and published in the paper: Efficient\nEstimation of Word Representations in Vector\nSpace. This model aims at\ncomputing continuous vector representations of words from very large\ndata sets.\n\nIt uses a principle in distributional semantics that says that “the\nword’s meaning is given by the words that frequently appear close-by”.\nAnd this is the main idea behind “word2vec”. Also, this means that\nsimilar words will appear in similar context which will make their\nvectors similar and that’s a great feature in word2vec as we are going\nto see later.\n\nThe way we define these vectors is actually a trick you may have seen\nelsewhere in machine learning. We’re going to train a simple neural\nnetwork with a single hidden layer to perform a certain task, but then\nwe’re not actually going to use that neural network for the task we\ntrained it on! Instead, the goal is actually just to learn the weights\nof the hidden layer, these weights are actually the “word vectors” that\nwe’re trying to learn.\n\nSo now we need to talk about this “fake” task that we’re going to build\nthe neural network to perform, and then we’ll come back later to how\nthis indirectly gives us those word vectors that we are really after.\nWe’re going to train the neural network to be a language model. To be\nmore specific, the model should do the following; given nearby words\n(before and after), the model should predict the most probable word that\nfits. At first, it will be random but after a few iterations over the\ntraining data it will produce more appropriate results. When we say\n&quot;nearby&quot;, there is actually a &quot;window size&quot; parameter to the\nalgorithm. A typical window size might be 5, meaning 5 words behind and\n5 words ahead (10 in total).\n\nWord2vec comes in two flavors, the Continuous Bag-of-Words model (CBOW)\nand the Skip-Gram model. Algorithmically, these models are similar,\nexcept for only one thing which is the shape of the input layer and the\noutput layer. As CBOW predicts target words from source context\nwords, while the skip-gram does the inverse and predicts source\ncontext-words from the target words.\n\nThis inversion might seem like an arbitrary choice, but statistically it\nhas the effect that CBOW smooths over a lot of the distributional\ninformation (by treating an entire context as one observation). For the\nmost part, this turns out to be a useful thing for smaller datasets.\nHowever, skip-gram treats each context-target pair as a new observation,\nand this tends to do better when we have larger datasets.\n\nI know this seems difficult, so let’s see the Skip-Gram and CBOW models\nin more details.\n\nSkip-Gram\n\nSkip-Gram model is a model and how to prepare the training data. In\nSkip-Gram, we’ll train the neural network by feeding it word pairs found\nin our training documents. The below example shows some of the training\nsamples (word pairs) we would take from the sentence “The quick brown\nfox jumps over the lazy dog”. Here, we’ve used a small window size of\n$2$ just for the example. A more reasonable size would be $5$ or more.\nThe word highlighted in blue is the input word (center) and the other\ntwo on the left and right are the context words, each is called\n(target).\n\n\n    \n\n\nThe network is going to learn the statistics from the number of times\neach pairing shows up. So, for example, the network is probably going to\nget many more training samples of (“Soviet”, “Union”) than it is of\n(“Soviet”, “Apple”). When the training is finished, if you give it the\nword “Soviet” as input, then it will output a much higher probability\nfor “Union” or “Russia” than it will for “Apple”.\n\nNow, how is this all represented? First of all, we know we can’t feed a\nword just as a text string to a neural network, so we need a way to\nrepresent the words to the network. To do this, we first build a\nvocabulary of words from our training documents, let’s say we have a\nvocabulary of $10,000$ unique words. Then, we’re going to represent the\ninput word as a one-hot vector. This vector will have $10,000$\ncomponents (one for every word in our vocabulary) and we’ll place a $1$\nin the position corresponding to the word, and $0$s in all of the other\npositions.\n\nThe output of the network is a single vector (also with $10,000$\ncomponents) containing, for every word in our vocabulary, the\nprobability that a randomly selected nearby word is that vocabulary\nword. Here’s the architecture of our neural network, noting that there\nis no activation function on the hidden layer neurons, but the output\nneurons use Softmax:\n\n\n    \n\n\nWhen training this network on word pairs, the input is a one-hot vector\nrepresenting the input word and the training output is also a one-hot\nvector representing the output word. But when we evaluate the trained\nnetwork on an input word, the output vector will actually be a\nprobability distribution (i.e., a bunch of floating point values, not a\none-hot vector).\n\nFor our example, we’re going to say that we’re learning word vectors of\n$300$ features. So the hidden layer is going to be represented by a\nweight matrix of $10,000$ rows (one for every word in our vocabulary)\nand $300$ columns (one for every hidden neuron). “$300$ features” is\nwhat Google used in their published model trained on the Google news\ndataset, but we can tune this number if we want; we can try different\nvalues and see what yields the best results. As a matter of fact, the\nweights of the hidden layer after finishing training, is the word\nembedding we are looking for. This matrix is often called\n“Embedding Matrix”\n\n\n    \n\n\nSo, the end goal of all of this is really just to learn this hidden\nlayer weight matrix, the output layer we’ll just toss when we’re done!\nLet’s get back, though, to working through the definition of this model\nthat we’re going to train. The one-hot vector is almost all zeros,\nwhat’s the effect of that? If we multiply a $1 \\times 10,000$ one-hot\nvector by a $10,000 \\times 300$ matrix, it will effectively just select\nthe matrix row corresponding to the 1. This means that the hidden layer\nof this model is really just operating as a lookup table.\n\nHere’s a small example to give you a visual:\n\n\n    \n\n\nThe output layer is a Softmax regression classifier of $10,000$ neurons.\nEach output neuron has a weight vector which it multiplies against the\nword vector from the hidden layer, then it applies the Softmax\nactivation function. Here’s the formula that we are going to follow to\ntrain this neural network:\n\n\\[p\\left( \\text{target} \\middle| \\text{context} \\right) = \\frac{\\exp\\left( u_{\\text{target}}^{T}.v_{\\text{context}} \\right)}{\\sum_{i = 1}^{V}{\\exp\\left( u_{i}^{T}.v_{\\text{context}} \\right)}}\\]\n\nSuch that:\n\n\n  \n    $\\text{target}$ is the target word.\n  \n  \n    $\\text{target}$ is the context word.\n  \n  \n    $V$ is the number of words in our vocabulary.\n  \n  \n    $u_{\\text{target}}$ refers to the word vector of the target word.\nIts shape should be $d \\times 1$ where $d$ is the number of\nfeatures/dimensions ($300$ in our case). $u_{\\text{target}}^{T}$\nshape should be $1 \\times d$.\n  \n  \n    $v_{\\text{context}}$ refers to the output-layer vector (outside\nvector) of the context word. Its shape should be\n$1 \\times d$ where $d$ is the number of features ($300$ in our\ncase).\n  \n\n\nSo, as we can see each word has two representations in our Word2Vec\nmodel; one when it’s a target word and another when it’s a context word.\nHere’s an illustration of calculating the output of the output neuron\nfor an input word.\n\n\n    \n\n\nIf two different words have very similar contexts, then our model needs\nto output very similar results for these two words. So, if two words\nhave similar contexts, then our network is motivated to learn similar\nword vectors for these two words. And what does it mean for two words to\nhave similar contexts? I think we could expect that synonyms like\n“intelligent” and “smart” would have very similar contexts. Or that\nwords that are related, like “engine” and “transmission”, would probably\nhave similar contexts as well. This can also handle stemming for us –\nthe network will likely learn similar word vectors for the words “ant”\nand “ants” because these should have similar contexts. The following\nformula is cost function that we are going to use to learn our model:\n\n\\[J = \\frac{- 1}{S}\\sum_{i = 1}^{S}\\left( \\sum_{- m \\leq j \\leq m,j \\neq 0}^{}{\\log\\left( p\\left( w_{i + j} \\middle| w_{i} \\right) \\right)} \\right)\\]\n\nSuch as that:\n\n\n  \n    $J$ represents the objective function.\n  \n  \n    $S$ represents the total number of sentences that we are going to\ntrain our model upon.\n  \n  \n    $m$ is the window size.\n  \n  \n    $w_{i}$ is the context word.\n  \n  \n    $w_{j + i}$ is the target word.\n  \n  \n    $p\\left( w_{i + j} \\middle| w_{i} \\right)$ represents the probability of\nthe target word given the context word. In other words, it’s equal to:\n  \n\n\n\\[p\\left( \\text{target} \\middle| \\text{context} \\right) = \\frac{\\exp\\left( u_{\\text{target}}^{T}.v_{\\text{context}} \\right)}{\\sum_{i = 1}^{V}{\\exp\\left( u_{i}^{T}.v_{\\text{context}} \\right)}}\\]\n\nYou may have noticed that the skip-gram neural network contains a huge\nnumber of weights. For our example with $300$ features and a vocab of\n$10,000$ words, that’s $3,000,000$ weights in the hidden layer and\noutput layer each! Running gradient descent on a neural network that\nlarge is going to be slow. And to make matters better, you need a huge\namount of training data in order to tune that many weights and avoid\nover-fitting. Millions of weights times billions of training samples\nmeans that training this model is going to be a beast. The authors of\nWord2Vec addressed these issues in their second paper: “Distributed\nRepresentations of Words and Phrases and their\nCompositionality”.\n\nThere are three innovations in this second paper that made training a\nbit faster:\n\n\n  \n    Treating common word pairs or phrases as single “words” in their model.\n  \n  \n    Subsampling frequent words to decrease the number of training examples.\n  \n  \n    Modifying the optimization objective with a technique they called\n“Negative Sampling”, which causes each training sample to update\nonly a small percentage of the model’s weights. It’s worth noting\nthat subsampling frequent words and applying Negative Sampling not\nonly reduced the compute burden of the training process, but also\nimproved the quality of their resulting word vectors as well.\n  \n\n\nLet’s talk about these three modifications with more details:\n\nWord Pairs\n\nHere, we are going to explain the first innovation (or modification)\nthat the authors of Word2Vec have produced in their second paper. The\nauthors pointed out that a word pair like “Boston Globe” (a newspaper)\nor “Chicago Bulls” (a Basketball Team) has a much different meaning than\nthe individual words “Chicago” and “Bulls”. So it makes sense to treat\nthem as a single word with its own word vector representation. We can\nsee the results in their published model, which was trained on $100$\nbillion words from a Google News dataset. The addition of phrases to the\nmodel swelled the vocabulary size to $3$ million words!\n\nBut the question here is “how did they detect these word-pairs?”. The\nanswer is very straight forward. Each pass of their model only looks at\ncombinations of two words, but you can run it multiple times to get\nlonger phrases. So, the first pass will pick up the phrase “New_York”,\nand then running it again will pick up “New_York_City” as a combination\nof “New_York” and “City” and so on. The tool counts the number of times\neach combination of two words appears in the training text, and then\nthese counts are used in an equation to determine which word\ncombinations to turn into word-pair.\n\nThe equation is designed to pay attention to the word-pairs that occur\ntogether often relative to the number of individual occurrences. It also\nfavors pairs made of infrequent words in order to avoid making phrases\nout of common words like “and the” or “this is”.\n\nSubsampling Frequent Words\n\nWe have seen how training samples were created from the source text. The\nbelow example shows some of the training samples (word pairs) we would\ntake from the sentence “The quick brown fox jumps over the lazy dog.”\nusing a small window size of 2. The word highlighted in blue is the\ninput word:\n\n\n    \n\n\nBut, in this representation we have two problems with common words like\n“the”:\n\n\n  \n    When looking at word pairs, (“fox”, “the”) doesn’t tell us much\nabout the meaning of “fox”. “the” appears in the context of pretty\nmuch every word.\n  \n  \n    We will have many more samples of (“the”, …) than we need to learn\na good vector for “the”.\n  \n\n\nWord2Vec implements a “subsampling” scheme to address this. They\nimplemented an equation for calculating a probability with which to keep\na given word in the vocabulary. This probability that we cut the word is\nrelated to the word’s frequency as we can see:\n\n\\[P\\left( w_{i} \\right) = \\frac{\\left( \\sqrt{\\frac{z\\left( w_{i} \\right)}{s}} + 1 \\right) \\ast s}{z\\left( w_{i} \\right)}\\]\n\nWhere $w_{i}$ represent the word, $z\\left( w_{i} \\right)$ represents the\nword frequency ratio. For example, if the word “peanut” occurs $1,000$\ntimes in a one-billion-word corpus, then\n$z\\left( ‘peanut’ \\right) = \\frac{10^{3}}{10^{9}} = 10^{- 6}$. And $s$\nis the ‘sampling rate’ which is a hyper-parameter that controls how much\nsubsampling occurs, and the default value is $0.001$. Smaller values of\n‘sampling rate’ mean words are less likely to be kept. And finally,\n$P\\left( w_{i} \\right)$ represents the probability of keeping the word\n$w_{i}$.\n\nNow, if we have a window size of 10, and we remove a specific instance\nof “the” from our text. Then, we will train on the remaining words, as\n“the” will not appear in any of their context windows. And we’ll have 10\nfewer training samples where “the” is the input word.\n\nNegative Sampling\n\nTraining a neural network means taking a training example and adjusting\nall of the neuron weights slightly so that it predicts that training\nsample more accurately. In other words, each training sample will tweak\nall of the weights in the neural network. As we discussed above, the\nsize of our word vocabulary means that our Skip-Gram neural network has\na tremendous number of weights, all of which would be updated slightly\nby every one of our billions of training samples!\n\nNegative sampling addresses this by having each training sample only\nmodify a small percentage of the weights, rather than all of them.\nHere’s how it works:\n\n\n  When training the network on the word pair (“fox”, “quick”), we will\ngenerate another $k$ words randomly from the vocabulary. These $k$\nwords are called negative samples. The paper says that selecting\n5-20 words works well for smaller datasets, and you can get away\nwith only 2-5 words for large datasets.\n\n\n\n    \n\n\nAs we can see, here we have generated $4$ words randomly from the\nvocabulary and they are (“king”, “orange”, “box”, and “the”). As we can\nsee, the word “the” exists in the context of the word “quick”, but it’s\nok.\n\n\n  After that, we will multiply the Embedding Matrix $E$ by the one-hot\nvector of the five words producing embedding vector:\n\n\n\\[O_{\\text{fox}} \\odot E = e_{\\text{fox}},O_{\\text{quick}} \\odot E = e_{\\text{quick}},O_{\\text{king}} \\odot E = e_{\\text{king}}\\]\n\n\\[O_{\\text{orange}} \\odot E = e_{\\text{orange}},O_{\\text{box}} \\odot E = e_{\\text{box}},O_{the} \\odot E = e_{the}\\]\n\n\n  Finally, change the problem into a classification problem where the\ninput is the word-pairs; and the output is either 1 or zero.\n\n\nRecall that the output layer of our model has a weight matrix of\n$300 \\times 10,000$. So, we will just be updating the weights for our\npositive word (“quick”), plus the weights for $4$ other words that we\nwant to output 0. That’s a total of $5$ output neurons, and $1,500$\nweight values total. That’s only $0.06\\text{\\%}$ of the $3$ Million\nweights in the output layer!\n\nChoosing the “negative samples” randomly isn’t that efficient because\nthe probability of a common word like “and” to be chosen is higher than\nany other word. Instead, we use a “unigram distribution” where the\nprobability for selecting a word as a negative sample is related to its\nfrequency, with more frequent words being more likely to be selected as\nnegative samples as we can see in the following formula:\n\n\\[P\\left( w_{i} \\right) = \\frac{f\\left( w_{i} \\right)^{\\frac{3}{4}}}{\\sum_{j = 0}^{n}\\left( f\\left( w_{i} \\right)^{\\frac{3}{4}} \\right)}\\]\n\nWhere $P\\left( w_{i} \\right)$ represents the probability of a word\n$w_{i}$ to be selected as a negative sample. As we can see, each word is\ngiven a weight equal to its frequency (word count)\n$f\\left( w_{i} \\right)$ raised to the $\\frac{3}{4}$ power. The decision\nto raise the frequency to the $\\frac{3}{4}$ power appears to be\nexperiential\n\n\\[J = \\frac{1}{S}\\sum_{i = 1}^{S}\\left( \\sum_{- m \\leq j \\leq m,j \\neq 0}^{}{\\log\\left( p\\left( w_{i + j} \\middle| w_{i} \\right) \\right)} \\right)\\]\n\nCBOW\n\nWe have explained the Skip-Gram model, now let’s explore the other\nWord2Vec model which is the Continuous Bag-of-Words (CBOW) model. As we\nhave said before, the only difference between Skip-Gram and CBOW is the\nshape of the input and output layer of the network. So, if we understand\nthe Skip-Gram model then the CBOW model should be quite straight-forward\nbecause in many ways they are mirror images of each other.\n\nTo understand that model, let’s consider having the following sentence\n“the quick brown fox jumps over the lazy dog.”. Here, we are going to\nuse a small window size of $2$ just for the example. A more reasonable\nsize would be $5$ or more. The word highlighted in blue is the output\nword (target).\n\n\n    \n\n\nUnlike the Skip-Gram, the continuous bag of words model context is\nrepresented by multiple words for a given target word. For example, we\ncould use “The quick fox jumps” as context words for “brown” as the\ntarget word. This calls for a modification to the neural network\narchitecture. The modification, shown below, consists of replicating the\ninput to hidden layer connections $C$ times which represents the window\nsize of the context (four in our case):\n\n\n    \n\n\nLike the Skip-Gram, we will use a one-hot vector to encode our\nwords. This vector will have $10,000$ components (one for every word in\nour vocabulary). The output of the network is a single vector (also with\n10,000 components) containing, for every word in our vocabulary, the\nprobability that a randomly selected nearby word is that vocabulary\nword. Here’s the architecture of our neural network, noting that there\nis no activation function on the hidden layer neurons, but the output\nneurons use Softmax. Everything else will be exactly as the\nSkip-Gram model.\n"
      },
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "Sentence Embedding",
        "url"       : "/word-embedding/sentence_embedding",
        "date"      : "22/05/2014",
        "content": "Here, we are going to talk about an important issue that tried to use\nthe Word Embedding to produce a sentence embedding. By sentence\nembedding, we mean to provide a vector of $d$ length that has the\nmeaning of the sentence in a numerical form; the same form as we did\nwith word embedding.\n\n\n    \n\n\nFollowing this approach, we could be able to found similarity between\ntwo sentences that has similar meaning. And this could give us some\nsuperiority over Cosine Similarity which determines the similarity bases\non the common words. Using two sentences like “Mexico wishes to\nguarantee citizen’s safety” and “Mexico intends to avoid more violence”\nwill be not that similar using Cosine Similarity even though they are\npretty similar in the meaning. And two sentences like “Iranians Vote in\nPresidential Election” and “Keita Wins Mali Presidential Election” will\nbe very similar using cosine similarity although they are not similar in\nthe meaning.\n\nLe and Mikolov from Google, in their paper: “Distributed\nRepresentations of Sentences and\nDocuments” published in 2014,\nproposed different techniques for Sentence Embedding as we are going to\ndiscuss below:\n\nBag-of-Words (BoW)\n\nThis is the simplest method of which we can covert the word embedding\nvectors into sentence embedding vector. In this method we get the\naverage of the words vectors that form the sentence. So, the\nsentence embedding vector of “Natural Language Processing” is:\n\n\\[v\\left( \\text{Natural Language Processing} \\right) = \\frac{v\\left( \\text{Natural} \\right) + v\\left( \\text{Language} \\right) + v\\left( \\text{Processing} \\right)}{3}\\]\n\nBut this method neglects a lot of information like the sequence of the\nwords and that might give false results. So, for example the sentence\n“You are going there to teach not to play.” will have the same sentence\nembedding as “You are going there to play not to each.” even though they\nare exactly the opposite.\n\nDistributed Bag-of-Words (DBoW)\n\nLe and Mikolov from Google, in their paper “Distributed Representations\nof Sentences and Documents”\npublished in 2014, proposed an distributed bag-of-words (DBOW) which\nused only the paragraph context vector to predict the words in the\nparagraph. This simple model is analogous to the skip-gram version of\nword2vec, except the paragraph vector is used to predict all the words\nparagraph instead of using the target word to predict the context words.\nAs in the skip-gram model, DBOW is very computationally and memory\nefficient. Empirical results have shown that both DM and DBOW outperform\nbag-of-words and bag-of-n-gram models for text representations.\nFurthermore, averaging the DM and DBOW vector representations often\nyields the best performance overall.\n\n\n    \n\n\ndoc2vec\n\nThe model generates fixed-length feature representations from variable\nlength pieces of text, making it useful for application to sentences,\nparagraphs, sections, or entire documents. The key to the approach is to\nassociate every paragraph with a unique paragraph vector $u^{i}$, which\nis averaged with the word vectors $w_{j}^{i}$ of the $J$ words in the\nparagraph to yield a representation of the paragraph $p^{i}$:\n\n\\[p^{i} = u^{i} + \\sum_{j = 1}^{J}w_{j}^{i}\\]\n\nThe paragraph vector ui can be thought of acting as a memory that\nremembers word order context. During training, a sliding window of\ncontext words $C$ and the paragraph vector $p^{i}$ are used to predict\nthe next word in the paragraph context. Both paragraph vectors and word\nvectors are trained via backpropagation. While the paragraph vector is\nunique to each paragraph and shared across all contexts generated from\nthe same paragraph, the word vectors are shared across the entire\ncorpus. It is notable that the\n\n\n    \n\n\nLater Check:\n\nThere is a famous paper published by Sanjeev Arora, Yingyn Liang, and\nTengyu Ma, who are a group of researchers at Princeton, \nand they call it “A simple but Tough-to-beat Baseline for Sentence\nEmbedding”.\n"
      },
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "GloVe",
        "url"       : "/word-embedding/GloVe",
        "date"      : "25/10/2014",
        "content": "GloVe (Global Vectors for Word Representation) is a model released in\n2014 by Stanford NLP Group researchers Jeffrey Pennington, Richard\nSocher, and Chris Manning for learning word embedding and\npublished in the paper: GloVe: Global Vectors for Word\nRepresentation. The GloVe\nauthors present some results which suggest that their model is\ncompetitive with Google’s popular word2vec package.\n\nThe key idea behind GloVe model is to combine two concepts into one\nmodel; It learns word vectors by using the same idea of Word2Vec\nmodel beside examining [word co-occurrences within a text\ncorpus.\n\nCo-occurrence Matrix\n\nBefore we train the actual model, we need to construct a co-occurrence\nmatrix $X$, where a cell $X_{\\text{ij}}$ represents how often the word\n$i$ appears in the context of the word $j$ using a certain window size.\nWe run through our corpus just once to build the window-based\nco-occurrence matrix $X$ whose dimension is\n$\\left| V \\right| \\times \\left| V \\right|$. We will construct our model\nbased only on the values collected in $X$. So, assuming that our corpus\nhas only three sentences which are “I like deep learning.”, “I like\nNLP.”, and “I enjoy flying.” and assuming also that we have a window of\njust one word; then our co-occurrence matrix would be:\n\n\n    \n\n\nThere are a lot of things we should notice looking at this matrix:\n\n\n  \n    The co-occurrence matrix is symmetric, which means that the\nco-occurrence count of “I” and “like” is the same as the\nco-occurrence count of “like” and “I”.\n  \n  \n    The shape of the matrix is $V \\times V$ where $V$ is the size of the\nvocabulary which is $8$ in our case here. So, imagine using a\nmillion-word vocabulary.\n  \n  \n    There are a lot of zeros in this matrix. So, most of the used\nstorage will be just zeros.\n  \n\n\nTo be able to use this matrix in our GloVe model, there are some hacks\nthat we need to do to this matrix:\n\n\n  \n    Words like “the”, “he”, “she”, “and” …etc. are too frequent, so we\ncan limit their count to a certain number, say $100$, or just\nignore them all.\n  \n  \n    Use Pearson correlations instead of counts, then set negative values\nto $0$.\n  \n  \n    Ramped windows that count closer words more.\n  \n\n\nModel\n\nAs I said before, GloVe models combine Word2Vec model with the\nco-occurrence matrix. So, we can imagine that the GloVe model is a\nmodification of the Word2Vec model where we use the word-by-word\nco-occurrence matrix with word-vectors that we want to train just like\nthat:\n\n\\[w_{i}^{T}.w_{j} + b_{i} + b_{j} = log\\left( X_{\\text{ij}} \\right)\\]\n\nWhere\n\n\n  \n    $i$ and $j$ are the word-pairs.\n  \n  \n    $X_{\\text{ij}}$ refers to the co-occurrence count of word $i$ with word $j$.\n  \n  \n    $w_{i}$ and $w_{j}$ are the two word-vectors that we are trying to learn.\n  \n  \n    $b_{i}$ and $b_{j}$ are the biases that we are trying to learn as well.\n  \n\n\nThis formula works well, but we can weigh words differently to derive\nmore flexibility and robustness like so:\n\n\\[\\sum_{i,j = 1}^{\\left| V \\right|}{f\\left( X_{\\text{ij}} \\right)\\left( w_{i}^{T}.w_{j} + b_{i} + b_{j} - log\\left( X_{\\text{ij}} \\right) \\right)^{2}}\\]\n\nSuch as that:\n\n\n  \n    $\\left| V \\right|$ is the number of word-pairs in the vocabulary.\n  \n  \n    $f\\left( X_{\\text{ij}} \\right)$ represents a simple function that\nrestrict the values of the co-occurrence count. We can use the\nfollowing function:\n  \n\n\n\\[f\\left( X_{\\text{ij}} \\right) = \\left\\{ \\begin{matrix}\n\\left( \\frac{X_{\\text{ij}}}{x_{\\max}} \\right)^{\\alpha}\\text{if}\\ \\ X_{\\text{ij}} &amp;lt; x_{\\max} \\\\\n1\\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{otherwise} \\\\\n\\end{matrix} \\right.\\]\n\nWhere $x_{\\max}$ is called “the cutoff” usually equals to $100$. When we\nencounter extremely common word pairs (where\n$X_{\\text{ij}} &amp;gt; x_{\\max}$), this function will simply return $1$. For\nall other word pairs, it will return some weight in the range\n$\\left( 0,1 \\right)$, where the distribution of weights in this range is\ndecided by $\\alpha$ which is usually equals to $0.75$.\n\nOR we could use a simpler function like the\n$\\max\\left( X_{\\text{ij}},c \\right)$ where $c$ is a constant number\n(usually $100$).\n\n\n    \n\n\nEvaluation\n\nBefore talking about how to evaluate our model, let’s first take about\nthe hyper-parameters that can be tuned to get better and better results.\nThe following graph is from the published GloVe paper:\n\n\n  \n    First, Same hyper-parameters as Word2Vec models which are:\n\n    \n      \n        Initialization method.\n      \n      \n        Learning rate $\\alpha$.\n      \n      \n        Word vector dimension $d$. Best value for $d$ is about $300$ and\nit slightly drops-off afterwards. But, it might be different\nfor downstream tasks.\n      \n      \n        Window size $m$. As we can see in the following graph, the best\nvalue is around $8$.\n      \n      \n        Using symmetric or Asymmetric windows. Symmetric\nis using the window size for both directions (left and right).\nAsymmetric is using the window size for just one direction.\n      \n    \n  \n\n\n\n    \n\n\n\n  \n    Using either co-occurrence count or the Pearson\ncorrelations instead of counts and set negative values to\n$0$.\n  \n  \n    Training Time (no. of iterations): Usually the more … the better\n  \n\n\n\n    \n\n\nNow, how to evaluate our model? Let’s first talk about how to evaluate\nNLP tasks in general. Actually, there are two methods (Intrinsic and\nExtrinsic). Intrinsic methods try to find a mathematical formula or a\nsub-task to evaluate our model. While Extrinsic methods try to get our\nmodel into a real-life task. Both have some pros and cons:\n\n\n\n    \n        \n            \n            Intrinsic\n            Extrinsic\n        \n    \n    \n        Pros\n        \n            \n                Fast to compute.\n                Very useful when trying to get intuition about the model.\n            \n        \n        \n            \n                Very sufficient in case of good results.\n            \n        \n    \n    \n        Cons\n        \n            \n                Not enough to decide if the model is helpful or not.\n            \n        \n        \n            \n                Takes a long time to compute.\n                In the case of bad results, it is unclear if the subsystem is the problem or the intrinsic task wasn&#39;t properly performed.\n               \n        \n    \n\n\n\nSo, let’s explain this in more details… Assume that we have a\nword-vectors for a $1,000,000$ word vocabulary and we want to evaluate\nif these word-vectors are worth publishing or not?? When thinking about\nintrinsic evaluation, we might think to get just a $10,000$ words and\ntest on a word analogy task]. This is fast to compute, and we can\ntune some of the parameters to get better and better results. But,\n$10,000$ isn’t enough, we need to get the whole matrix into\nconsideration. So, we decided to use the whole $1,000,000$ words into\nthe same task (word analogy). Putting in mind that such a task might\ntake hours and hours to train and evaluate. And the results might get\nworse as the hyper-parameters that we have tuned are for the $10,000$\ntask not the whole corpus.\n\nSo, the solution is to to balance between these two methods. At first,\nwe have to start with the intrinsic method and change only one\nparameter. And don’t confirm this change unless it’s evaluated by the\nExtrinsic method.\n\n\n  Note:\nWe can use a pre-trained GloVe word embedding freely from the Stanford\nGloVe official website. We can know a lot about these pre-trained models\nfrom the name, for example the pre-trained model “glove.6B.50d”, it’s\ncalled 6B because it’s formed using 6 billion words in context, and 50d\nbecause the extracted features (dimension) are 50.\n\n\nRe-training Word Vectors\n\nWe can always use pre-trained word vectors whether it’s Word2Vec\nembeddings or GloVe embeddings, but what about use these pre-trained\nword vectors as the initial weights of our model?? Is it a good idea??\n\nActually, that’s a great question and the answer is not that simple. But\nluckily there is a rule-of-thumb that we can use… if the training\ndataset is quite small, then it’s so much better if we don’t train our\nmodel using these pre-trained vectors. And if the training dataset is\nvery large, it may work better to train the model using the pre-trained\nvectors. And that’s because if the dataset is quite small, some of the\nword vectors will get better values but not all of them, so the model\nloses generalization.\n\nLet’s see how… assume that we have a small data corpus where the word\n“TV” and “telly” are mentioned and the word “television” is not; then\nwhen training the model using these small dataset, the model changes the\n“TV” and “telly” word vectors according to their context. But what about\nthe word “television&quot;?? It won’t change, which makes its vector less\nsimilar to the vectors of “TV” and “telly” putting in mind that these\nthree words are the same. So, their pre-trained vectors are pretty\nsimilar. That’s why when training a model with small dataset some of the\nwords are left out, and similar word-vectors becomes less similar.\n"
      },
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "sense2vec",
        "url"       : "/word-embedding/sense2vec",
        "date"      : "19/11/2015",
        "content": "One of the limits of word2vec is polysemy, which means that one word\ncould have multiple meanings or senses. For example, the word “bank”\ncould be a verb meaning “do financial work” or a noun meaning “financial\ninstitution”. And this problem is known in NLP by the name of\n“word-sense disambiguation”. In 2015, Andrew Trask proposed a model in\nhis paper: Sense2Vec - A Fast And Accurate Method For Word Sense\nDisambiguation In Neural Word\nEmbeddings. called “sense2vec”.\n\nSense2vec is a simple method to achieve word-sense disambiguation that\nleverages supervised labeling such as part-of-speech. The sense2vec\nmodel can learn different word senses of this word by combining a\nsingle-sense embedding model with POS labels as shown in the following\nfigure:\n\n\n    \n\n\nGiven a corpus, sense2vec will create a new corpus for each word for\neach sense by concatenating a word with its POS label. The new corpus is\nthen trained using word2vec’s CBOW or skip-gram to create word\nembeddings that incorporate word sense (as it relates to their POS\nusage) as shown in the following figure:\n\n\n    \n\n\nTO BE CONTINUED…\n"
      },
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "De-biasing Word Vectors",
        "url"       : "/word-embedding/de-biasing",
        "date"      : "21/07/2016",
        "content": "In this paper: Man is to Computer Programmer as Woman is to Homemaker?\nDebiasing Word Embeddings published in\n2016, the researchers examined the gender biases that can be reflected in a\nword embedding and explore some algorithms for reducing the bias.\n\nFirst, what does biasing of word vectors really mean? OK, let’s put it\nthat way. There are some words that relate only the females like\n“actress, waitress, mother, aunt, …” and there are some words that\nrelate only the males like “actor, waiter, father, uncle, …”. And\nthere some other words that can relate to the two genders like\n“programmer, developer, assistant, doctor, …”.\n\nSurprisingly, most of the word embedding that are out there are biasing\ntowards a certain gender due to the context they were mentioned at.\nLet’s see the “glove.6B.50d.txt” for example, there are some words that\nrelate to the female more that the male like “lipstick, arts,\nliterature, doctor, receptionist, fashion, …”. And let’s be honest,\nsome of them make perfect sense like “lipstick” for example, but there\nare also some of them that don’t make any sense like “doctor” and\n“receptionist” which must be gender unspecific.\n\nWe&#39;ll see how to reduce the bias of these vectors, using an algorithm\ndue to Boliukbasi 2016. Note that some word pairs such as\n&quot;actor&quot;/&quot;actress&quot; or &quot;grandmother&quot;/&quot;grandfather&quot; should remain\ngender specific, while other words such as &quot;receptionist&quot; or\n&quot;technology&quot; should be neutralized, i.e. not be gender-related. You\nwill have to treat these two types of words differently when de-biasing.\n\nImplicit Association Test\n\nImplicit Association Test is the test used to check if two sets of words\nare biased towards a certain topic or not. And that’s how it works;\nassume that you have two sets of words that you want to check the bias\nbetween them (they are called attributes):\n\n\n  \n    X: {male, man, boy, brother, he, him, his, son}.\n  \n  \n    Y: {female, woman, girl, sister, she, her, hers, daughter}.\n  \n\n\nNow, let’s assume that we have another two sets of words that represent\ntwo opposing topics (they are called target words):\n\n\n  \n    A: {math, algebra, geometry, calculus, equations, numbers}.\n  \n  \n    B: {poetry, art, dance, literature, novel, symphony, drama}.\n  \n\n\nAnd this can be done via the following formula:\n\n\\[s\\left( X,Y,A,B \\right) = \\sum_{x \\in X}^{}s\\left( x,A,B \\right) - \\sum_{y \\in Y}^{}s\\left( y,A,B \\right)\\]\n\n\\[s\\left( w,A,B \\right) = \\frac{1}{\\text{card}\\left( A \\right)}\\sum_{a \\in A}^{}\\cos\\left( w,a \\right) - \\frac{1}{\\text{card}\\left( B \\right)}\\sum_{b \\in B}^{}\\cos\\left( w,b \\right)\\]\n\nWhere:\n\n\n  \n    $s\\left( X,Y,A,B \\right)$: is the association test between all words\nof attributes and target words. And it could have three possible\nvalues:\n\n    \n      \n        If the score is positive, it means the association between X\nand A is big than Y and B. and the higher the score is, the\nmore association there is.\n      \n      \n        If the score is negative, it means the association between Y\nand B is big than X and A. and the lower the score is, the\nmore association there is.\n      \n      \n        If the score is zero, it means there are no association\nbetween X and A and Y and B.\n      \n    \n  \n  \n    $s\\left( w,A,B \\right)$: is the association strength between word w\nand set A and away from set B.\n  \n  \n    $\\text{card}\\left( A \\right)$: It’s the cardinality a set of words A\nwhich is a measure of a set&#39;s size, meaning the number of unique\nelements in that set. For instance, the set $A = { 1,2,4}$ has a\ncardinality of $3$ for the three elements that are in it.\n  \n\n\nNeutralization\n\nBy neutralization, we mean to neutralize the bias for non-gender\nspecific words. So, the words like “receptionist, doctor, literature,\nart, technology, …” will be gender unspecific. We are going to do that\nusing some linear algebra concepts like so:\n\n\n    \n\n\nThe figure above should help visualizing what neutralization does. If\nyou&#39;re using a 50-dimensional word embedding, the 50-dimensional space\ncan be split into two parts: The gender-direction $\\overrightarrow{g}$,\nand the remaining 49 dimensions, which we&#39;ll call\n${\\overrightarrow{g}}_{\\bot}$. Even though\n${\\overrightarrow{g}}_{\\bot}$ is 49 dimensional, given the\nlimitations of what we can draw on a screen, we illustrate it using a\n1-dimensional axis below.\n\nIn linear algebra, we say that the 49-dimensional vector\n${\\overrightarrow{g}}_{\\bot}$ is perpendicular (or “orthogonal”) to\n$\\overrightarrow{g}$, meaning it is at can NOT be affected by\n$\\overrightarrow{g}$ . The neutralization step takes a vector such as\n$e_{\\text{receptionist}}$ and zeros out the component in the direction\nof $\\overrightarrow{g}$ , giving us\n$e_{\\text{receptionist}}^{\\text{debiased}}$.\n\n\\[e_{\\text{debiased}} = e_{w} - e_{\\text{proj}},e_{\\text{proj}} = \\frac{e_{w}\\text{.g}}{\\left\\| g \\right\\|_{2}} \\ast g\\]\n\nWhere $e_{w}$ is the word embedding of a certain word $w$, $g$ is the\ngender direction, $e_{\\text{proj}}$ is the projection of $e_{w}$ onto\nthe direction $g$ and finally $e_{\\text{debiased}}$ is the de-biased\nform of $e_{w}$.\n\nLet’s implement a function which can remove the bias of a given word:\n\ndef project(A, B):\n    return (np.dot(A, B) / np.sum(B**2)) * B\n\ndef neutralize(word, g, word_embedding):\n    e_w = word_embedding[word]\n    e_proj = project(e, g)\n    e_debiased = e_w - e_proj\n    return e_debiased\n\nNow, we can try:\n\n&amp;gt;&amp;gt;&amp;gt; # we can get the gender vector by simply doing so:\n&amp;gt;&amp;gt;&amp;gt; g = embedding[&#39;women&#39;] - embedding[&#39;man&#39;]\n&amp;gt;&amp;gt;&amp;gt; # cosine similarity before neutralizing:\n\n&amp;gt;&amp;gt;&amp;gt; cosine_similarity(embedding[&#39;receptionist&#39;], g))\n0.330779417506\n\n&amp;gt;&amp;gt;&amp;gt; e_debiased = neutralize(&quot;receptionist&quot;, g, embedding)\n&amp;gt;&amp;gt;&amp;gt; # cosine similarity after neutralizing:\n&amp;gt;&amp;gt;&amp;gt; cosine_similarity(e_debiased, g))\n-3.26732746085e-17\n\n\nEqualization\n\nBy equalization, we mean to equalize the values of gender-specific words\nlike “(actress, actor), (father, mother), …”. Equalization is applied\nto pairs of words that you might want to have differ only through the\ngender property. As a concrete example, suppose that &quot;actress&quot; is\ncloser to &quot;babysit&quot; than &quot;actor.&quot; By applying neutralizing to\n&quot;babysit&quot;, we can reduce the gender-stereotype associated with\nbabysitting. But this still does not guarantee that &quot;actor&quot; and\n&quot;actress&quot; are equidistant from &quot;babysit.&quot; The equalization algorithm\ntakes care of this.\n\nThe key idea behind equalization is to make sure that a particular pair\nof words are equi-distant from the 49-dimensional vector\n${\\overrightarrow{g}}_{\\bot}$. In pictures, this is how equalization\nworks:\n\n\n    \n\n\nThe derivation of the linear algebra to do this is a bit more complex,\nbut the key equations are:\n\n\\[\\mu = \\frac{e_{w1} + e_{w2}}{2},\\mu_{B} = \\text{proj}\\left( \\mu,{\\text{bia}s}_{\\text{axis}} \\right),\\mu_{\\bot} = \\mu - \\mu_{B}\\]\n\n\\[e_{w1B} = \\frac{\\sqrt{\\left| 1 - \\left\\| \\mu_{\\bot} \\right\\|_{2}^{2} \\right|} \\ast \\text{proj}\\left( e_{w1},\\text{bias}_{\\text{axis}} \\right) - \\mu_{B}}{\\left| \\text{proj}\\left( e_{w1},\\text{bias}_{\\text{axis}} \\middle| - \\mu_{B} \\right) \\right|}\\]\n\n\\[e_{w2B} = \\frac{\\sqrt{\\left| 1 - \\left\\| \\mu_{\\bot} \\right\\|_{2}^{2} \\right|} \\ast \\text{proj}\\left( e_{w2},\\text{bias}_{\\text{axis}} \\right) - \\mu_{B}}{\\left| \\text{proj}\\left( e_{w2},\\text{bias}_{\\text{axis}} \\middle| - \\mu_{B} \\right) \\right|}\\]\n\n\\[e_{1} = e_{w1B} + \\mu_{\\bot},e_{2} = e_{w2B} + \\mu_{\\bot}\\]\n\nLet’s get to the implantation:\n\ndef equalize(pair, bias_axis, word_to_vec_map):\n    w1, w2 = pair\n    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]\n    mu = (e_w1 + e_w2) / 2.\n    mu_B = project(mu, bias_axis)\n    mu_orth = mu - mu_B\n   \n    e1_orth = mu_orth\n    e2_orth = mu_orth\n    \n    e_w1B = np.sqrt(np.abs(1 - normalize(mu_orth)**2))\\\n                * (project(e_w1, bias_axis) - mu_B) \\\n                / np.linalg.norm(np.abs(project(e_w1, bias_axis) - mu_B))\n\n    e_w2B = np.sqrt(np.abs(1 - normalize(mu_orth)**2)) \\\n                * (project(e_w2, bias_axis) - mu_B) \\\n                / np.linalg.norm(np.abs(project(e_w2, bias_axis) - mu_B))\n    \n    e1 = e_w1B + e1_orth\n    e2 = e_w2B + e2_orth\n    return e1, e2\n\n\nNow, let’s see it in action\n\n&amp;gt;&amp;gt;&amp;gt; # cosine similarities before equalizing:\n&amp;gt;&amp;gt;&amp;gt; cosine_similarity(embedding[&#39;man&#39;], g))\n-0.117110957653\n\n&amp;gt;&amp;gt;&amp;gt; cosine_similarity(embedding[&#39;man&#39;], g))\n0.356666188463\n\n&amp;gt;&amp;gt;&amp;gt; e1, e2 = equalize((&#39;man&#39;, &#39;woman&#39;), g, embedding)\n&amp;gt;&amp;gt;&amp;gt; # cosine similarities after equalizing:\n&amp;gt;&amp;gt;&amp;gt; cosine_similarity(e1, g))\n-0.700436428931\n\n&amp;gt;&amp;gt;&amp;gt; cosine_similarity(e2, g))\n-0.700436428931\n\n"
      },
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "Subword Embedding",
        "url"       : "/word-embedding/subword_embedding",
        "date"      : "19/06/2017",
        "content": "Methods such as word2vec or GloVe ignore the internal structure of words\nand associate each word (or word sense) to a separate vector\nrepresentation. For morphologically rich languages, there may be a\nsignificant number of rare word forms such that either a very large\nvocabulary must be maintained or a significant number of words are\ntreated as out-of-vocabulary (OOV).\n\nAs previously stated, out-of-vocabulary words can significantly impact\nperformance due to the loss of context from rare words. An approach that\ncan help deal with this limitation is the use of subword embeddings\nproposed by this paper: “Enriching Word Vectors with Subword\nInformation”, where vector\nrepresentations $z_{g}$ are associated with character n-grams g and\nwords $w_{i}$ are represented by the sum of the n-gram vectors:\n\n\\[w_{i} = \\sum_{g \\in \\mathbb{G}_{w}}^{}z_{g}\\]\n\nFor instance, the vector for the word “take” consists of the sum of the\nvectors for the n-grams {t, a, k, e, &amp;lt;t, ta, ke, , e&amp;gt;, &amp;lt;ta, tak, ake,\nke&amp;gt;} when $n \\in \\lbrack 1,2,3\\rbrack$ as show in the following figure:\n\n\n    \n\n\nAs n-grams are shared across words, this allows for representation of\neven unseen words since an OOV word will still consist of n-grams that\nwill have representations. Subword embeddings can significantly boost\nNLP tasks such as language modeling and text classification.\n\nTO BE CONTINUED…\n"
      },
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "Contextualized Word Embedding: ELMO &amp; BERT",
        "url"       : "/word-embedding/contextualized_word_embedding",
        "date"      : "22/03/2018",
        "content": "In the past few years, a number of new methods leveraging contextualized\nembeddings have been proposed. These are based on the notion that\nembeddings for words should be based on contexts in which they are used.\nThis context can be the position and presence of surrounding words in\nthe sentence, paragraph, or document.\n\nELMO\n\nELMO stands for “Embeddings from Language Models” and it is based on a\nvery important concept which is contextualized word embeddings.\nContextualized word embeddings means that: instead of using a fixed\nembedding for each word, let’s look at the entire sentence before\nassigning each word in it an embedding.”. So, each word will have a\ndifferent embedding based on the context of the sentence.\n\nSo, for example the word “Paris”, it can be used to describe the city or\nit can be a female name like “Paris Hilton” for example. In all previous\nword embedding techniques, the word “Paris” wil lhave the same word\nembedding for both the city and the name. In ELMO, “Paris” will still\nhave just one embedding vector where it sums all the multiple embeddings\nresulting from the different context the word appeared at.\n\n\n    \n\n\nELMO was published in this paper: “Deep contextualized word\nrepresentations” by “Allen\nInstitute for AI” in 2018. And ELMO creates these contextualized word\nembeddings by using a bi-directional LSTM trained on a massive dataset\nto predict the next word in a sequence of words - a task called Language\nModeling. This is convenient because we have vast amounts of text data\nthat such a model can learn from without needing labels.\n\n\n    \n\n\nELMO comes up with the contextualized embedding from bi-directional LSTM\nthrough the following three steps:\n\n\n  \n    Given a word “w”, we will concatenate the forward and backward word\nembeddings of each word at every layer. So, in the previous\narchitecture, where ELMO has only two layers, we will have three\nsets of vectors:\n\n    \n      \n        Concatenate the forward and backward of the “stick” word embeddings.\n      \n      \n        Concatenate the forward and backward of the first LSTM layer.\n      \n      \n        Concatenate the forward and backward of the second LSTM layer.\n      \n    \n  \n  \n    Multiply each word vector by learnable parameters that represent the\nimportance of that word embedding.\n  \n  \n    Eventually, sum all three weighted vectors together to get just one\nvector.\n  \n\n\nAnd all three steps can be summarized in the following image:\n\n\n    \n\n\nBERT\n\nThis part relies heavily on BERT and how it works. So, if you need a\nrefresher, check the BERT part in the language model document. BERT\nstands for “Bidirectional Encoder Representations from Transformers” and\nit’s a language model architecture that can be fine-tuned for various\ntasks. BERT, also, can be used to create contextualized word embeddings\nlike ELMO as shown in the following figure:\n\n\n    \n\n\nAccording to the paper: “BERT: Pre-training of Deep Bidirectional\nTransf”, the output of each\nencoder layer along each token’s path can be used as a word embedding\nfor that token. And according to the paper, there are some word vectors\nthat work best as contextualized word embedding knowing that it might\ndepend on the task. So, the task the paper used was NER and the\nfollowing summarized the results:\n\n\n    \n\n"
      },
    
      
      
      {
        "collection": "Word Embedding",
        "title"     : "LASER",
        "url"       : "/word-embedding/LASER",
        "date"      : "25/09/2019",
        "content": "LASER stands for “Language-Agnostic Sentence\nRepresentation”. LASER is an encoder-decoder architecture proposed\nby FAIR in 2019 and published in their paper: Massively Multilingual\nSentence Embeddings for Zero-Shot Cross-Lingual Transfer and\nBeyond. The official code for\nthis paper can be found in the Fairseq official GitHub repository:\nfairseq/laser.\n\nLASER was trained on parallel corpora to learn joint multilingual\nsentence representations for 93 languages. The encoder is\nlanguage-agnostic which maps the source sequence into a fixed-length\nvector representation, which is used by the decoder alongside the\nlanguage ID embedding $L_{\\text{id}}$ to create the target sequence.\nThis decoder is then discarded, and the encoder is kept to embed\nsentences in any of the training languages.\n\n\n    \n\n\nIn this paper, they studied a stacked BiLSTM with 1 to 5 layers, each\n512-dimensional resulting 1024-dimensional sentence representations\n(after concatenating both directions). The decoder has always one layer\nof dimension 2048. The input embedding size is set to 320, while the\nlanguage ID embedding has 32 dimensions.\n\nTraining minimizes the cross-entropy loss on the training corpus,\nalternating over all combinations of the languages involved. They used\nAdam with a constant learning rate of 0.001 and dropout set to 0.1, and\ntrain for a fixed number of epochs.\n\nTO BE CONTINUED\n"
      }
    
  
]
