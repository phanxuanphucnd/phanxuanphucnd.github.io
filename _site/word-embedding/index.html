<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>Word Embedding</title>
  <meta name="title" content="Word Embedding">
  <meta name="description" content="I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics: Cross-lingual Langluage Model,  Language Modeling,  Machine Translation,  Misc.,  Multilingual NMT, and  Word Embedding">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="Word Embedding">
  <meta itemprop="description" content="I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics: Cross-lingual Langluage Model,  Language Modeling,  Machine Translation,  Misc.,  Multilingual NMT, and  Word Embedding">
  <meta itemprop="image" content="//images/avatar.jpg">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="Word Embedding">
  <meta property="og:description" content="I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics: Cross-lingual Langluage Model,  Language Modeling,  Machine Translation,  Misc.,  Multilingual NMT, and  Word Embedding">
  <meta property="og:image" content="//images/avatar.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Word Embedding">
  <meta name="twitter:description" content="I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics: Cross-lingual Langluage Model,  Language Modeling,  Machine Translation,  Misc.,  Multilingual NMT, and  Word Embedding">
  
  <meta name="twitter:image" content="//images/avatar.jpg">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/word-embedding/">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <div class="main">
  

  
  <div class="main-post-list">
    
    
    
    
    
    
    <h1 class="page-heading">2019</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/LASER/image1.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/word-embedding/LASER">LASER</a>
            </h2>
            <p class="excerpt">LASER stands for “Language-Agnostic Sentence
Representation”. LASER is an encoder-decoder architecture proposed
by FAIR in 2019 and published in their paper: Massively Multilingual
Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and
Beyond. The official code for
this paper can be found in the Fairseq official GitHub repository:
fairseq/laser.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2019-09-25 00:00" class="post-list__meta--date date"> Published on arXiv on :
                25 Sep 2019</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
    </ol>
    <h1 class="page-heading">2018</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/contextualized_word_embedding/image1.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/word-embedding/contextualized_word_embedding">Contextualized Word Embedding: ELMO & BERT</a>
            </h2>
            <p class="excerpt">In the past few years, a number of new methods leveraging contextualized
embeddings have been proposed. These are based on the notion that
embeddings for words should be based on contexts in which they are used.
This context can be the position and presence of surrounding words in
the sentence, paragraph, or document.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2018-03-22 00:00" class="post-list__meta--date date"> Published on arXiv on :
                22 Mar 2018</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
    </ol>
    <h1 class="page-heading">2017</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/subword_embedding/image1.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/word-embedding/subword_embedding">Subword Embedding</a>
            </h2>
            <p class="excerpt">Methods such as word2vec or GloVe ignore the internal structure of words
and associate each word (or word sense) to a separate vector
representation. For morphologically rich languages, there may be a
significant number of rare word forms such that either a very large
vocabulary must be maintained or a significant number of words are
treated as out-of-vocabulary (OOV).

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2017-06-19 00:00" class="post-list__meta--date date"> Published on arXiv on :
                19 Jun 2017</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
    </ol>
    <h1 class="page-heading">2016</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/de-biasing/image1.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/word-embedding/de-biasing">De-biasing Word Vectors</a>
            </h2>
            <p class="excerpt">In this paper: Man is to Computer Programmer as Woman is to Homemaker?
Debiasing Word Embeddings published in
2016, the researchers examined the gender biases that can be reflected in a
word embedding and explore some algorithms for reducing the bias.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2016-07-21 00:00" class="post-list__meta--date date"> Published on arXiv on :
                21 Jul 2016</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
    </ol>
    <h1 class="page-heading">2015</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/sense2vec/image1.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/word-embedding/sense2vec">sense2vec</a>
            </h2>
            <p class="excerpt">One of the limits of word2vec is polysemy, which means that one word
could have multiple meanings or senses. For example, the word “bank”
could be a verb meaning “do financial work” or a noun meaning “financial
institution”. And this problem is known in NLP by the name of
“word-sense disambiguation”. In 2015, Andrew Trask proposed a model in
his paper: Sense2Vec - A Fast And Accurate Method For Word Sense
Disambiguation In Neural Word
Embeddings. called “sense2vec”.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2015-11-19 00:00" class="post-list__meta--date date"> Published on arXiv on :
                19 Nov 2015</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
    </ol>
    <h1 class="page-heading">2014</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/GloVe/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/word-embedding/GloVe">GloVe</a>
            </h2>
            <p class="excerpt">GloVe (Global Vectors for Word Representation) is a model released in
2014 by Stanford NLP Group researchers Jeffrey Pennington, Richard
Socher, and Chris Manning for learning word embedding and
published in the paper: GloVe: Global Vectors for Word
Representation. The GloVe
authors present some results which suggest that their model is
competitive with Google’s popular word2vec package.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2014-10-25 00:00" class="post-list__meta--date date"> Published on arXiv on :
                25 Oct 2014</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/sentence_embedding/image1.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/word-embedding/sentence_embedding">Sentence Embedding</a>
            </h2>
            <p class="excerpt">Here, we are going to talk about an important issue that tried to use
the Word Embedding to produce a sentence embedding. By sentence
embedding, we mean to provide a vector of $d$ length that has the
meaning of the sentence in a numerical form; the same form as we did
with word embedding.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2014-05-22 00:00" class="post-list__meta--date date"> Published on arXiv on :
                22 May 2014</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
    </ol>
    <h1 class="page-heading">2013</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/word2vec/image1.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/word-embedding/word2vec">Word2Vec</a>
            </h2>
            <p class="excerpt">Word2Vec stands for “word-to-vector” is a model architecture created by
Tomáš Mikolov from Google in 2013 and published in the paper: Efficient
Estimation of Word Representations in Vector
Space. This model aims at
computing continuous vector representations of words from very large
data sets.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2013-09-07 00:00" class="post-list__meta--date date"> Published on arXiv on :
                7 Sep 2013</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
    </ol>
    <h1 class="page-heading">2008</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/t-SNE/image.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/word-embedding/t-SNE">t-SNE</a>
            </h2>
            <p class="excerpt">One of the popular things to do with word embedding, is to take this
N-dimensional data and embed it in a two-dimensional space so that we
can visualize them. The most common algorithm for doing this is the
t-SNE algorithm created by Laurens van der Maaten and Geoffrey
Hinton in 2008 and published in this paper: Visualizing data using
t-SNE.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2008-11-25 00:00" class="post-list__meta--date date"> Published on arXiv on :
                25 Nov 2008</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
         </ol>
      
    <hr class="post-list__divider ">

    <!--  -->
  </div>
  </div>
      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>