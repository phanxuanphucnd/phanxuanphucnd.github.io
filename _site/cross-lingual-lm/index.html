<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>Cross-lingual Language Model</title>
  <meta name="title" content="Cross-lingual Language Model">
  <meta name="description" content="I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics: Cross-lingual Langluage Model,  Language Modeling,  Machine Translation,  Misc.,  Multilingual NMT, and  Word Embedding">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="Cross-lingual Language Model">
  <meta itemprop="description" content="I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics: Cross-lingual Langluage Model,  Language Modeling,  Machine Translation,  Misc.,  Multilingual NMT, and  Word Embedding">
  <meta itemprop="image" content="//images/avatar.jpg">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="Cross-lingual Language Model">
  <meta property="og:description" content="I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics: Cross-lingual Langluage Model,  Language Modeling,  Machine Translation,  Misc.,  Multilingual NMT, and  Word Embedding">
  <meta property="og:image" content="//images/avatar.jpg">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Cross-lingual Language Model">
  <meta name="twitter:description" content="I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics: Cross-lingual Langluage Model,  Language Modeling,  Machine Translation,  Misc.,  Multilingual NMT, and  Word Embedding">
  
  <meta name="twitter:image" content="//images/avatar.jpg">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/cross-lingual-lm/">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <div class="main">
  

  
  <div class="main-post-list">
    
    
    
    
    
    
    <h1 class="page-heading">2021</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/nmT5/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/cross-lingual-lm/nmT5">nmT5: NMT + Multilingual T5</a>
            </h2>
            <p class="excerpt">nmT5 stands for “NMT + Multilingual Text-to-Text Transfer Transformer”
which is an attempt to improve the performance of the
mT5 model by
incorporating parallel data into pre-training. This model was proposed
by the same authors from Google Research as the mT5 paper. In 2021, it
was published in this paper: nmT5 - Is parallel data still relevant for
pre-training massively multilingual language
models?.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2021-08-01 00:00" class="post-list__meta--date date"> Published on arXiv on :
                1 Aug 2021</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/XLM-E/image3.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/cross-lingual-lm/XLM-E">XLM-E: XLM via ELECTRA</a>
            </h2>
            <p class="excerpt">XLM-E stands for “Cross-lingual Language Modeling via Electra” which is
a cross-lingual language model that was pre-trained on two
ELECTRA-style
tasks as we are going to see later. XLM-E was proposed by Microsoft in
2021 and published in their paper: Cross-lingual Language Model
Pre-training via ELECTRA. The
official code for this paper can be found on Microsoft’s GitHub
repository: microsoft/unilm.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2021-06-30 00:00" class="post-list__meta--date date"> Published on arXiv on :
                30 Jun 2021</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/DeltaLM/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/cross-lingual-lm/DeltaLM">∆LM: Delta Language Modeling</a>
            </h2>
            <p class="excerpt">DeltaLM (∆LM) is a pre-trained multilingual encoder-decoder model whose
encoder and decoder are initialized with a pre-trained multilingual
encoder, and trained in a self-supervised way. DeltaLM was proposed by
Microsoft in 2021 and published in their paper: “∆LM: Encoder-Decoder
Pre-training for Language Generation and Translation by Augmenting
Pretrained Multilingual
Encoders”. The official code for
this paper can be found on Microsoft’s GitHub repository:
microsoft/deltalm.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2021-06-25 00:00" class="post-list__meta--date date"> Published on arXiv on :
                25 Jun 2021</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/mT6/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/cross-lingual-lm/mT6">mT6: Multilingual T5 with Translation Pairs</a>
            </h2>
            <p class="excerpt">mT6 stands for “Multilingual Text-to-Text Transfer Transformer with
Translation pairs” which is an attempt to improve the performance of the
mT5 model by
incorporating translation objectives into the pre-training part. This
model was proposed by Microsoft Research in 2021 and published in this
paper: mT6: Multilingual Pretrained Text-to-Text Transformer with
Translation Pairs.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2021-04-18 00:00" class="post-list__meta--date date"> Published on arXiv on :
                18 Apr 2021</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
    </ol>
    <h1 class="page-heading">2020</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/VECO/image1.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/cross-lingual-lm/VECO">VECO: Cross-lingual Pre-training</a>
            </h2>
            <p class="excerpt">VECO stands for “Variable and Flexible Cross-lingual
Pre-training” which is a pre-training approach for cross-lingual
language models that uses “Cross-Attention Masked Language Modeling”
(CA-MLM) objective. VECO was proposed by Alibaba Group in 2020 and
published in their “VECO: Variable and Flexible Cross-lingual
Pre-training for Language Understanding and
Generation” paper. The official
code for this paper can be found on Alibaba’s official GitHub
repository:
alibaba/VECO.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2020-10-30 00:00" class="post-list__meta--date date"> Published on arXiv on :
                30 Oct 2020</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/mT5/image6.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/cross-lingual-lm/mT5">mT5: Multilingual T5</a>
            </h2>
            <p class="excerpt">mT5 stands for “Multilingual Text-to-Text Transfer Transformer” which is
a multilingual variant of
T5 trained on 101
languages. mT5 was proposed by Google Research in 2020 and published in
their paper: mT5: A Massively Multilingual Pre-trained Text-to-Text
Transformer. The official code
can be found on Google Research’s official GitHub repository:
google-research/multilingual-t5.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2020-10-22 00:00" class="post-list__meta--date date"> Published on arXiv on :
                22 Oct 2020</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/AMBER/image1.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/cross-lingual-lm/AMBER">AMBER</a>
            </h2>
            <p class="excerpt">AMBER stands for “Aligned Multilingual Bidirectional
EncodeR” is a cross-lingual language model that adopts the same
architecture as BERT; where the contextual embeddings of words/sentences
with similar meanings across languages are aligned together in the same
space. AMBER was proposed by Google Research in collaboration with
Carnegie Mellon University in 2020 and published in their paper:
Explicit Alignment Objectives for Multilingual Bidirectional
Encoders. The official code for
this paper can be found in this GitHub repository:
amber.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2020-10-15 00:00" class="post-list__meta--date date"> Published on arXiv on :
                15 Oct 2020</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="/images/avatar-light.jpg" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/cross-lingual-lm/InfoXLM">InfoXLM</a>
            </h2>
            <p class="excerpt">InfoXLM stands for “Information-theoretic procedure for
Cross-Lingual Modeling” which is a cross-lingual language model proposed
by Microsoft in 2020 and published in their paper: InfoXLM: An
Information-Theoretic Framework for Cross-Lingual Language Model
Pre-Training. The official code
for this paper can be found in Microsoft’s official UniLM GitHub repository:
unilm/infoxlm.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2020-07-15 00:00" class="post-list__meta--date date"> Published on arXiv on :
                15 Jul 2020</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/LaBSE/image1.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/cross-lingual-lm/LaBSE">LaBSE: Language-agnostic Sentence Embeddings</a>
            </h2>
            <p class="excerpt">LaBSE stands for “Language-agnostic BERT Sentence Embedding” which is a
multilingual model the produces language-agnostic sentence embeddings
for 109 languages. LaBSE model was proposed by Google AI in 2020 and
published in this paper under the same name: Language-agnostic BERT
Sentence Embedding. The official
code for this paper can be found in the following TensorFlow Hub link:
tfhub/LaBSE.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2020-07-03 00:00" class="post-list__meta--date date"> Published on arXiv on :
                3 Jul 2020</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/ALM/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/cross-lingual-lm/ALM">ALM: Alternating Language Modeling</a>
            </h2>
            <p class="excerpt">ALM stands for “Alternating Language Modeling” which is a novel
cross-lingual pre-training method proposed by Microsoft in 2021 and
published in their paper: Alternating Language Modeling for Cross-Lingual
Pre-Training.
The official code for this paper can be found in the following
repository: ALM.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2020-04-03 00:00" class="post-list__meta--date date"> Published on arXiv on :
                3 Apr 2020</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
    </ol>
    <h1 class="page-heading">2019</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/XLM-R/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/cross-lingual-lm/XLM-R">XLM-R</a>
            </h2>
            <p class="excerpt">XLM-R stands for “Cross-lingual Language Modeling-RoBERTa” which was
created by FacebookAI in 2019 and published in this paper:
“Unsupervised Cross-lingual Representation Learning at
Scale”. The goal of paper is to
improve the XLM model’s performance on both cross-lingual and
monolingual understanding tasks. XLM-R follows the XLM model
architecture with a changes that improve the performance at scale. And
these changes are:

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2019-11-05 00:00" class="post-list__meta--date date"> Published on arXiv on :
                5 Nov 2019</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/Unicoder/image3.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/cross-lingual-lm/Unicoder">Unicoder: Universal Encoder</a>
            </h2>
            <p class="excerpt">Unicoder is an abbreviation of “Universal Encoder” which is a language
encoder that is universal across different number of languages. The
Unicoder model was proposed by Microsoft in 2019 and published in this
paper: Unicoder: A Universal Language Encoder by Pre-training with
Multiple Cross-lingual Tasks.
The official code for this paper can be found on Microsoft’s official
GitHub repository:
microsoft/Unicoder.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2019-11-03 00:00" class="post-list__meta--date date"> Published on arXiv on :
                3 Nov 2019</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/XNLG/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/cross-lingual-lm/XNLG">XNLG</a>
            </h2>
            <p class="excerpt">XNLG stands for “Cross-lingual Natural Language Generation” which is an
encoder-decoder cross-lingual model designed for Natural Language
Generation (NLG) tasks such as question generation and abstractive
summarization. This model was created by Microsoft in 2019 and published
in their paper: Cross-Lingual Natural Language Generation via
Pre-Training. The official code
for this paper is found in the following GitHub repository:
xnlg.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2019-09-23 00:00" class="post-list__meta--date date"> Published on arXiv on :
                23 Sep 2019</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/XLM/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/cross-lingual-lm/XLM">XLM</a>
            </h2>
            <p class="excerpt">XLM stands for “Cross-lingual Language Modeling” which is a model
created by FacebookAI in 2019 and published in this paper:
“Cross-lingual Language Model Pretraining”.
XLM is an 12-layer encoder-transformer of with 1024
hidden units, 16 heads, and GELU activation.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2019-01-22 00:00" class="post-list__meta--date date"> Published on arXiv on :
                22 Jan 2019</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
        
      
    
    
    </ol>
    <h1 class="page-heading">2018</h1>
    <ol class="timeline">
      
      
        <li class="timeline-inverted">
          <div class="timeline-album">
            <div class="timeline-image">
              
              <img class="img-me" src="media/mBERT/image0.png" alt="">
            </div>
          </div>
          <div class="timeline-panel">
            <h2 class="post-list__post-title post-title"><a href="/cross-lingual-lm/mBERT">mBERT: Multilingual BERT</a>
            </h2>
            <p class="excerpt">mBERT is a multilingual BERT pre-trained on 104 languages, released by the
authors of the original paper on Google Research’s official GitHub repository: google-research/bert
 on 
November 2018. mBERT follows the same structure of BERT. The only difference is
that mBERT is pre-trained on concatenated Wikipedia data for 104 languages and
it does surprisingly well compared to cross-lingual word embeddings on zero-shot
cross-lingual transfer in XNLI dataset.

&hellip;</p>
            <div class="post-list__meta">
              <time datetime="2018-11-04 00:00" class="post-list__meta--date date"> Published on arXiv on :
                4 Nov 2018</time>
            </div>
            <hr class="post-list__divider">
          </div>
        </li>
         </ol>
      
    <hr class="post-list__divider ">

    <!--  -->
  </div>
  </div>
      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>