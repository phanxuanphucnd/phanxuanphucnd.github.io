<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>AMBER</title>
  <meta name="title" content="AMBER">
  <meta name="description" content="AMBER stands for “Aligned Multilingual Bidirectional
EncodeR” is a cross-lingual language model that adopts the same
architecture as BERT; where the contextual embeddings of words/sentences
with similar meanings across languages are aligned together in the same
space. AMBER was proposed by Google Research in collaboration with
Carnegie Mellon University in 2020 and published in their paper:
Explicit Alignment Objectives for Multilingual Bidirectional
Encoders. The official code for
this paper can be found in this GitHub repository:
amber.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="AMBER">
  <meta itemprop="description" content="AMBER stands for “Aligned Multilingual Bidirectional
EncodeR” is a cross-lingual language model that adopts the same
architecture as BERT; where the contextual embeddings of words/sentences
with similar meanings across languages are aligned together in the same
space. AMBER was proposed by Google Research in collaboration with
Carnegie Mellon University in 2020 and published in their paper:
Explicit Alignment Objectives for Multilingual Bidirectional
Encoders. The official code for
this paper can be found in this GitHub repository:
amber.

">
  <meta itemprop="image" content="/cross-lingual-lm/media/AMBER/image1.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="AMBER">
  <meta property="og:description" content="AMBER stands for “Aligned Multilingual Bidirectional
EncodeR” is a cross-lingual language model that adopts the same
architecture as BERT; where the contextual embeddings of words/sentences
with similar meanings across languages are aligned together in the same
space. AMBER was proposed by Google Research in collaboration with
Carnegie Mellon University in 2020 and published in their paper:
Explicit Alignment Objectives for Multilingual Bidirectional
Encoders. The official code for
this paper can be found in this GitHub repository:
amber.

">
  <meta property="og:image" content="/cross-lingual-lm/media/AMBER/image1.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="AMBER">
  <meta name="twitter:description" content="AMBER stands for “Aligned Multilingual Bidirectional
EncodeR” is a cross-lingual language model that adopts the same
architecture as BERT; where the contextual embeddings of words/sentences
with similar meanings across languages are aligned together in the same
space. AMBER was proposed by Google Research in collaboration with
Carnegie Mellon University in 2020 and published in their paper:
Explicit Alignment Objectives for Multilingual Bidirectional
Encoders. The official code for
this paper can be found in this GitHub repository:
amber.

">
  
  <meta name="twitter:image" content="/cross-lingual-lm/media/AMBER/image1.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/cross-lingual-lm/AMBER">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          3 mins read
        </span>
      </p>
      <time datetime="2020-10-15 00:00" class="post-meta__body date">Published on arXiv on: 15 Oct 2020</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#Google Research">Google Research</a> & <a href="/labs/#Carnegie Mellon University">Carnegie Mellon University</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=AMBER> AMBER</h1>
    <p>AMBER stands for “<strong>A</strong>ligned <strong>M</strong>ultilingual <strong>B</strong>idirectional
<strong>E</strong>ncode<strong>R</strong>” is a cross-lingual language model that adopts the same
architecture as BERT; where the contextual embeddings of words/sentences
with similar meanings across languages are aligned together in the same
space. AMBER was proposed by Google Research in collaboration with
Carnegie Mellon University in 2020 and published in their paper:
<a href="https://arxiv.org/pdf/2010.07972.pdf">Explicit Alignment Objectives for Multilingual Bidirectional
Encoders</a>. The official code for
this paper can be found in this GitHub repository:
<a href="https://github.com/junjiehu/amber">amber</a>.</p>

<h2 id="cross-lingual-alignment">Cross-lingual Alignment</h2>

<p>To produce language-independent representations, AMBER was trained on
monolingual and parallel data using three alignment objectives that
align the multilingual word/sentence representations together. These
three alignment objectives are:</p>

<ul>
  <li><u><strong>MLM or TLM:</strong></u><br />
This objective, proposed in
<a href="https://phanxuanphucnd.github.io/language-modeling/BERT">BERT</a>, takes a
pair of sentences $\left( x,y \right)$, and optimizes the prediction
of randomly masked tokens in the concatenation of the sentence pair
$\left\lbrack x;y \right\rbrack$. When $x$ and $y$ are in the same
language, it’s Masked Language Modeling (MLM). When they are in two
different languages, it’s Translation Language Modeling (TLM). This
can be described as follows where
$\left\lbrack x;y \right\rbrack_{s}$ is the masked tokens of the
concatenation while $\left\lbrack x;y \right\rbrack_{\backslash s}$
is the unmasked tokens:</li>
</ul>

\[\mathcal{L}_{\text{MLM}}\left( x,y \right) = - \mathbb{E}_{s\sim\left\lbrack 1,\left| \left\lbrack x;y \right\rbrack \right| \right\rbrack}\log\text{ P}\left( \left\lbrack x;y \right\rbrack_{s} \middle| \left\lbrack x;y \right\rbrack_{\backslash s} \right)\]

<ul>
  <li><u><strong>Sentence Alignment:</strong></u><br />
This objective encourages cross-lingual alignment of sentence
representations. Given sentence pair $\left( x,y \right)$, we
separately calculate sentence embeddings
$\left( c_{x},\ c_{y} \right)$ where the sentence embeddings is
calculated by averaging the embeddings in the final layer. Then the
model tries to minimize the following loss function where $y’$ is
any sentence in the mini-batch $\mathcal{B}$:</li>
</ul>

<div align="center">
    <img src="media/AMBER/image1.png" width="350" />
</div>

\[\mathcal{L}_{\text{SA}}\left( x,y \right) = - \log\ \frac{e^{c_{x}^{\intercal}c_{y}}}{\sum_{y' \in \mathcal{B}}^{}e^{c_{x}^{\intercal}c_{y'}}}\]

<ul>
  <li><u><strong>Bidirectional Word Alignment:</strong></u><br />
This objective encourages bidirectional alignment of word embeddings
by leveraging the attention mechanism in the
<a href="https://phanxuanphucnd.github.io/machine-translation/Transformer">Transformer</a>
model by minimizing the distance between the trace of the
source-to-target attention $A_{x \rightarrow y}$ and
target-to-source attention $A_{y \rightarrow x}$ matrices. Since the
Transformer has multiple attention heads, we average the trace of
the bidirectional attention matrices generated by all the heads as
shown in the following formula:</li>
</ul>

<div align="center">
    <img src="media/AMBER/image2.png" width="450" />
</div>

\[\mathcal{L}_{\text{WA}}\left( x,y \right) = 1 - \frac{1}{H}\sum_{h = 1}^{H}\frac{\text{tr}\left( \left( A_{x \rightarrow y}^{h} \right)^{\intercal}\left( A_{y \rightarrow x}^{h} \right) \right)}{\min\left( \left| x \right|,\left| y \right| \right)}\]

<p>They combined all three objectives to obtain the following total loss
function:</p>

\[\mathcal{L}\left( x,y \right) = \mathcal{L}_{\text{MLM}}\left( x,y \right) + \mathcal{L}_{\text{SA}}\left( x,y \right) + \mathcal{L}_{\text{WA}}\left( x,y \right)\]

<blockquote>
  <p><strong>Notes:</strong></p>

  <ul>
    <li>AMBER with just MLM objective is the same as
  <a href="https://phanxuanphucnd.github.io/cross-lingual-lm/mBERT">mBERT</a>.</li>
    <li>Sentence Alignment objective is the same as
  <a href="https://arxiv.org/pdf/1902.08564.pdf">Additive Margin Softmax (AMS)</a>
  where the margin is zero $(m=0)$.</li>
  </ul>
</blockquote>

<h2 id="experiments">Experiments</h2>

<p>AMBER was pre-trained using MLM on the Wikipedia data for 1M steps first
using the default hyper-parameters as mBERT found
<a href="https://github.com/google-research/bert">here</a> except that they used a
larger batch of 8,192 sentence pairs. Then, they pre-training it using
the other two objectives for another 1M steps with a batch of 2,048
sentence pairs from Wikipedia corpus and parallel corpus used to train
<a href="https://phanxuanphucnd.github.io/cross-lingual-lm/XLM">XLM</a>. As show in the
following table, shows the size of AMBER compared to other cross-lingual
models:</p>

<div align="center">
    <img src="media/AMBER/image3.png" width="450" />
</div>

<p>After pre-training, they fine-tuned AMBER on English annotations and
applied the model to predict on non-English data on the following tasks:</p>

<ul>
  <li><strong>POS:</strong> Cross-lingual Part-Of-Speech (POS) benchmark which contains
data in 13 languages. The following table shows that AMBER achieves
similar results to XLM-R despite being half its size:</li>
</ul>

<div align="center">
    <img src="media/AMBER/image4.png" width="750" />
</div>

<ul>
  <li><strong>PAWS-X:</strong> is a paraphrase detection dataset in five different
languages. The following table shows that AMBER achieving on-par
results with XLM-R large despite the fact that AMBER is one-fifth of
its size:</li>
</ul>

<div align="center">
    <img src="media/AMBER/image5.png" width="450" />
</div>

<ul>
  <li><strong>XNLI:</strong> is a natural language inference dataset in 15 languages.
The following table shows that AMBER is not as good as XLM-R on this
dataset, but achieving better results than XLM-100.</li>
</ul>

<div align="center">
    <img src="media/AMBER/image6.png" width="1050" />
</div>

<ul>
  <li><strong>Tatoeba:</strong> is a sentence retrieval dataset in 14 different
languages; where they try to find the English translation for a
given a non-English sentence using maximum cosine similarity. The
following table shows AMBER achieving the best results on this
benchmark:</li>
</ul>

<div align="center">
    <img src="media/AMBER/image7.png" width="750" />
</div>

<p>The following table summarizes the average results over all
languages for the past four benchmarks:</p>

<div align="center">
    <img src="media/AMBER/image8.png" width="450" />
</div>

<blockquote>
  <p><strong>Note:</strong><br />
In all of the previous dataset, AMBER (MLM) is achieving better results than
mBERT despite having the same architecture because AMBER uses bigger batch
sizes which is proven to be efficient as explain in
<a href="https://phanxuanphucnd.github.io/language-modeling/RoBERTa">RoBERTa</a> paper.</p>
</blockquote>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/cross-lingual-lm/AMBER';
      this.page.identifier = '/cross-lingual-lm/AMBER';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>