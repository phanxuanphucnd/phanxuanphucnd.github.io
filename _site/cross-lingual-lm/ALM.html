<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>ALM: Alternating Language Modeling</title>
  <meta name="title" content="ALM: Alternating Language Modeling">
  <meta name="description" content="ALM stands for “Alternating Language Modeling” which is a novel
cross-lingual pre-training method proposed by Microsoft in 2021 and
published in their paper: Alternating Language Modeling for Cross-Lingual
Pre-Training.
The official code for this paper can be found in the following
repository: ALM.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="ALM: Alternating Language Modeling">
  <meta itemprop="description" content="ALM stands for “Alternating Language Modeling” which is a novel
cross-lingual pre-training method proposed by Microsoft in 2021 and
published in their paper: Alternating Language Modeling for Cross-Lingual
Pre-Training.
The official code for this paper can be found in the following
repository: ALM.

">
  <meta itemprop="image" content="/cross-lingual-lm/media/ALM/image0.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="ALM: Alternating Language Modeling">
  <meta property="og:description" content="ALM stands for “Alternating Language Modeling” which is a novel
cross-lingual pre-training method proposed by Microsoft in 2021 and
published in their paper: Alternating Language Modeling for Cross-Lingual
Pre-Training.
The official code for this paper can be found in the following
repository: ALM.

">
  <meta property="og:image" content="/cross-lingual-lm/media/ALM/image0.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="ALM: Alternating Language Modeling">
  <meta name="twitter:description" content="ALM stands for “Alternating Language Modeling” which is a novel
cross-lingual pre-training method proposed by Microsoft in 2021 and
published in their paper: Alternating Language Modeling for Cross-Lingual
Pre-Training.
The official code for this paper can be found in the following
repository: ALM.

">
  
  <meta name="twitter:image" content="/cross-lingual-lm/media/ALM/image0.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/cross-lingual-lm/ALM">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          4 mins read
        </span>
      </p>
      <time datetime="2020-04-03 00:00" class="post-meta__body date">Published on arXiv on: 3 Apr 2020</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#Beihang University">Beihang University</a> & <a href="/labs/#Microsoft">Microsoft</a> & <a href="/labs/#Tencent Inc.">Tencent Inc.</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=ALM: Alternating Language Modeling> ALM: Alternating Language Modeling</h1>
    <p>ALM stands for “Alternating Language Modeling” which is a novel
cross-lingual pre-training method proposed by Microsoft in 2021 and
published in their paper: <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6480">Alternating Language Modeling for Cross-Lingual
Pre-Training</a>.
The official code for this paper can be found in the following
repository: <a href="https:/github.com/zddfunseeker/ALM">ALM</a>.</p>

<p>The idea behind ALM is to predict one language conditioned on the
context of the other language which can minor the gap between the
embeddings of languages and that’s very beneficial for the cross-lingual
setting. The following figure shows the difference between ALM and XLM.
In ALM, the input sequence of ALM is mixed with different languages.</p>

<div align="center">
    <img src="media/ALM/image1.png" width="750" />
</div>

<h2 id="code-switching">Code-Switching</h2>

<p>Given a bilingual sentence pair $\left( X,Y \right)$ with the source
sentence $X = \left\{ x_{1},\ x_{2},\ …x_{N} \right\}$ and the target
translation $Y = \left\{ y_{1},\ y_{2},\ …y_{M} \right\}$, where $N$
and $M$ are the lengths of the source and target sentences respectively,
a code-switched sentence $U = \left\{ u_{1},\ u_{2},\ …x_{L} \right\}$
of length $L$ is composed of phrases from $X$ and $Y$.</p>

<p>For each phrase $u_{\left\lbrack i:j \right\rbrack}$, it comes from
either the source sentence
$x_{\left\lbrack a:b \right\rbrack},\ 1 \leq a \leq b \leq N$ or the
target sentence
$y_{\left\lbrack c:d \right\rbrack},\ 1 \leq c \leq d \leq N$ where the
constraint is that these two phrases are the linguistic translation
counterpart in the parallel sentence $\left( X,Y \right)$. The following
figure shows different examples of code-switched data between English
and Chinese:</p>

<div align="center">
    <img src="media/ALM/image2.png" width="450" />
</div>

<p>When most of the code-switched sequence $U$ is derived from $X$, this
sample is called <strong>major-source-language</strong> sample. When most of the
code-switched sequence $U$ is derived from $Y$, this sample is called
<strong>major-target-language</strong>.</p>

<p>Due to the scarcity of natural code-switched sentences, the researchers
had to construct them from bilingual sentence pairs. They did that by
following these steps:</p>

<ul>
  <li>
    <p>First, they performed word alignment with the GIZA toolkit between
the parallel sentences, and extract a bilingual phrase table using
statistical machine translation techniques.</p>
  </li>
  <li>
    <p>Then, for each sentence pair in training corpus, they created the
<strong>major-source-language</strong> samples by substituting some phrases in
source sentence with the corresponding target phrases with highest
probabilities in phrase table. And same for
<strong>major-target-language</strong>, they substituted some phrases in target
sentence with the corresponding source phrases.</p>
  </li>
  <li>
    <p>Each phrase is limited to less than 5 words for both source language
and target language. And the substituted words are less than 30% of
the total words in the sentence.</p>
  </li>
  <li>
    <p>Each bilingual sentence pair is used to create multiple alternating
language sentences by randomly choosing the substituted phrases as
shown in the following figure:</p>
  </li>
</ul>

<div align="center">
    <img src="media/ALM/image3.png" width="450" />
</div>

<h2 id="pre-training">Pre-training</h2>

<p>ALM uses the encoder-part of the Transformer-base architecture with 1024
embedding and hidden units, 8 heads and learned positional embeddings.
During training, they used BPE with 95K sub-word tokens. They used Adam
optimizer with parameters of $\beta_{1} = 0.9$ and $\beta_{2} = 0.98$
accompanied with a dropout rate of $0.1$. They set the
inverse-square-root learning rate schedule with a linear warmup where
the number of warmup step is $4000$ and a learning rate of $0.0005$. The
batch size was set to $8192$ tokens. During decoding, they sued a beam
size of 8.</p>

<p>MLM was done like the following; Given a parallel sentence pair, they
combined two sentences from different languages into a single
code-switched sequence as described above. Then, they mask out a certain
percentage of words in the sequences and feed them into Transformer
model to learn to predict the words being masked out as shown in the
following figure:</p>

<div align="center">
    <img src="media/ALM/image4.png" width="450" />
</div>

<p>Data used in pre-training was monolingual data from Wikipedia and
bilingual data from IWSLT14 German-to-English translation dataset. The
input sentences to the ALM model were $10\%$ from the source sentence
(without code-switching), $10\%$ from the target sentence (without
code-switching), and the rest $80\%$ were code-switched. MLM was applied
by randomly masking $15\%$ of the tokens using:</p>

<ul>
  <li>
    <p>The [MASK] token $80\%$ of the time.</p>
  </li>
  <li>
    <p>A random token $10\%$ of the time.</p>
  </li>
  <li>
    <p>Keep them unchanged $10\%$ of the time.</p>
  </li>
</ul>

<h2 id="machine-translation">Machine Translation</h2>

<p>After pre-training ALM on the MLM objective, they fine-tuned it on the
machine translation task. Using parallel data from WMT14 English-German
dataset and IWSLT14 German-English dataset, they used ALM as the encoder
of machine translation, and construct a Transformer-based decoder
initialized from the ALM. In other words, they fed the source language
into ALM, and generated the target language with decoder.</p>

<p>Fine-tuning was done with the Adam optimizer with a linear warmup as the
pre-training. The learning rates was tuned based on the performance on
the validation set which was $5 \times 10^{- 4}$ for IWSLT14
German-English dataset and $10^{- 3}$ for WMT14 English-German. Early
stopping was used.</p>

<p>The following figure shows the performance on the two datasets; the one
on the left shows the results of WMT14 and the one on the right shows
the results of IWSLT14. In both datasets, we can see that ALM
outperforms all other models:</p>

<div align="center">
    <img src="media/ALM/image5.png" width="750" />
</div>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/cross-lingual-lm/ALM';
      this.page.identifier = '/cross-lingual-lm/ALM';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>