<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>nmT5: NMT + Multilingual T5</title>
  <meta name="title" content="nmT5: NMT + Multilingual T5">
  <meta name="description" content="nmT5 stands for “NMT + Multilingual Text-to-Text Transfer Transformer”
which is an attempt to improve the performance of the
mT5 model by
incorporating parallel data into pre-training. This model was proposed
by the same authors from Google Research as the mT5 paper. In 2021, it
was published in this paper: nmT5 - Is parallel data still relevant for
pre-training massively multilingual language
models?.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="nmT5: NMT + Multilingual T5">
  <meta itemprop="description" content="nmT5 stands for “NMT + Multilingual Text-to-Text Transfer Transformer”
which is an attempt to improve the performance of the
mT5 model by
incorporating parallel data into pre-training. This model was proposed
by the same authors from Google Research as the mT5 paper. In 2021, it
was published in this paper: nmT5 - Is parallel data still relevant for
pre-training massively multilingual language
models?.

">
  <meta itemprop="image" content="/cross-lingual-lm/media/nmT5/image0.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="nmT5: NMT + Multilingual T5">
  <meta property="og:description" content="nmT5 stands for “NMT + Multilingual Text-to-Text Transfer Transformer”
which is an attempt to improve the performance of the
mT5 model by
incorporating parallel data into pre-training. This model was proposed
by the same authors from Google Research as the mT5 paper. In 2021, it
was published in this paper: nmT5 - Is parallel data still relevant for
pre-training massively multilingual language
models?.

">
  <meta property="og:image" content="/cross-lingual-lm/media/nmT5/image0.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="nmT5: NMT + Multilingual T5">
  <meta name="twitter:description" content="nmT5 stands for “NMT + Multilingual Text-to-Text Transfer Transformer”
which is an attempt to improve the performance of the
mT5 model by
incorporating parallel data into pre-training. This model was proposed
by the same authors from Google Research as the mT5 paper. In 2021, it
was published in this paper: nmT5 - Is parallel data still relevant for
pre-training massively multilingual language
models?.

">
  
  <meta name="twitter:image" content="/cross-lingual-lm/media/nmT5/image0.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/cross-lingual-lm/nmT5">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          3 mins read
        </span>
      </p>
      <time datetime="2021-08-01 00:00" class="post-meta__body date">Published on arXiv on: 1 Aug 2021</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#Google Research">Google Research</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=nmT5: NMT + Multilingual T5> nmT5: NMT + Multilingual T5</h1>
    <p>nmT5 stands for “NMT + Multilingual Text-to-Text Transfer Transformer”
which is an attempt to improve the performance of the
<a href="https://phanxuanphucnd.github.io/cross-lingual-lm/mT5">mT5</a> model by
incorporating parallel data into pre-training. This model was proposed
by the same authors from Google Research as the mT5 paper. In 2021, it
was published in this paper: <a href="https://aclanthology.org/2021.acl-short.87.pdf">nmT5 - Is parallel data still relevant for
pre-training massively multilingual language
models?</a>.</p>

<p>A little bit of background: the mT5 model was pre-trained on mC4 dataset
(a multilingual version of the
<a href="https://www.tensorflow.org/datasets/catalog/c4">C4</a> corpus) with a
masked language modeling “span-corruption” objective, where the encoder
is fed a chunk of text with random spans replaced with a mask token, and
the decoder must reconstruct the masked-out tokens. In this paper, they
are trying different objectives to incorporate parallel data into
pre-training:</p>

<ul>
  <li><strong><u>TLM (Translation Language Modeling):</u></strong><br />
This objective was first proposed by the
<a href="https://phanxuanphucnd.github.io/cross-lingual-lm/XLM">XLM</a> model and
was used for encoder only pre-training. In this paper, they extended
it to the encoder-decoder setting.</li>
</ul>

<div align="center">
    <img src="media/nmT5/image1.png" width="750" />
</div>

<ul>
  <li><strong><u>NMT (Neural Machine Translation):</u></strong><br />
The input is the source text and the target is its translation. A
language code is prefixed to the input to inform the model of the
target language.</li>
</ul>

<div align="center">
    <img src="media/nmT5/image2.png" width="750" />
</div>

<ul>
  <li><strong><u>Denoised-NMT:</u></strong><br />
Similar to NMT, but with mask spans in the source sentence. The
model must now learn to implicitly perform language modeling of the
source language while translating into the target language.</li>
</ul>

<div align="center">
    <img src="media/nmT5/image3.png" width="750" />
</div>

<ul>
  <li><strong><u>Denoised-NMT+LM:</u></strong><br />
Similar to Denoised-NMT, but instead of implicit language modeling,
the model must explicitly predict the source text in addition to the
translation. The target is a concatenation of the translation and
source sentence, while the input is the masked source sentence.</li>
</ul>

<div align="center">
    <img src="media/nmT5/image4.png" width="750" />
</div>

<blockquote>
  <p><strong>Note:</strong><br />
nmT5 is the mT5 model with the NMT objective.</p>
</blockquote>

<h2 id="results">Results</h2>

<p>In this paper, they used the mT5-Large model to perform the following
experiments, which is a 24 layer encoder-decoder transformer model.
Instead of training a new model from scratch, they started from the
publicly available mT5-Large checkpoint - which has been trained for
over 1 trillion tokens - and did a second stage pre-training with a mix
of monolingual and parallel data.</p>

<p>For pre-training, they used monolingual data from mC4 and parallel data
from OPUS-100 which contains 55M translations covering 100 languages.
The mC4 corpus consists of unlabeled web text covering 101 languages, of
which 81 overlap with the OPUS-100 languages.</p>

<p>Starting from publicly available mT5-Large checkpoints, they pre-trained
for 100K steps with a mix of monolingual and parallel objectives. The
parallel data is mixed into monolingual data at a $10\%$ ratio, which
amounts to roughly 4 passes over the OPUS-100 corpus. Examples from each
language pair were sampled using the same language sampling distribution
as <a href="https://phanxuanphucnd.github.io/cross-lingual-lm/mT5">mT5</a> with
$\alpha = 0.3$.</p>

<p>Pre-training was done with a batch size of 1M tokens and fine-tuned with
$131,072$ tokens, with a constant learning rate of $0.001$. For
fine-tuning, they fine-tuned for $10,000$ steps for TyDiQA, MTOP, NER
and $25,000$ for WikiLingua, since it is a much larger dataset.
Checkpoint selection is done based on the validation set.</p>

<p>The following table shows the results averaged across all the languages.
Overall, adding parallel data through neural machine translation
objectives improves scores for all 4 tasks, with the NMT objective
performing the best.</p>

<div align="center">
    <img src="media/nmT5/image5.png" width="750" />
</div>

<p>From the past table, we can see that all NMT-based objectives shows
gains over mT5 across all tasks. Among these, NMT averages the best
among all other objectives leading to 7.2 higher scores averaging across
all four tasks:</p>

<div align="center">
    <img src="media/nmT5/image6.png" width="750" />
</div>

<h2 id="model-size">Model Size</h2>

<p>Researchers of mT5 found out that cross-lingual performance of language
models increases monotonically with model size, that’s why the mT5-XXL
had the highest performance across five out of six tasks.</p>

<p>To study the impact of model capacity here, the researchers also
experimented with larger model sizes. Using the mT5-XL size (3.7B
params, 3× larger than mT5-Large), they observed gains for all tasks
with nmT5. However, the magnitude of the gains is largely diminished,
hinting that the need for parallel data reduces as model capacity
increases.</p>

<div align="center">
    <img src="media/nmT5/image7.png" width="750" />
</div>

<p>This finding is particularly promising for low-resource languages, where
it is difficult to obtain high-quality parallel data. At the same time,
nmT5-Large substantially reduces the performance gap between mT5-Large
and mT5-XL, covering 70% of the headroom. Since bigger models are
expensive to train and even more expensive to deploy, this opens up
avenues for effectively using parallel data to improve performance of
smaller language models.</p>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/cross-lingual-lm/nmT5';
      this.page.identifier = '/cross-lingual-lm/nmT5';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>