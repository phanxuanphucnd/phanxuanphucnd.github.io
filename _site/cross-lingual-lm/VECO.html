<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>VECO: Cross-lingual Pre-training</title>
  <meta name="title" content="VECO: Cross-lingual Pre-training">
  <meta name="description" content="VECO stands for “Variable and Flexible Cross-lingual
Pre-training” which is a pre-training approach for cross-lingual
language models that uses “Cross-Attention Masked Language Modeling”
(CA-MLM) objective. VECO was proposed by Alibaba Group in 2020 and
published in their “VECO: Variable and Flexible Cross-lingual
Pre-training for Language Understanding and
Generation” paper. The official
code for this paper can be found on Alibaba’s official GitHub
repository:
alibaba/VECO.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="VECO: Cross-lingual Pre-training">
  <meta itemprop="description" content="VECO stands for “Variable and Flexible Cross-lingual
Pre-training” which is a pre-training approach for cross-lingual
language models that uses “Cross-Attention Masked Language Modeling”
(CA-MLM) objective. VECO was proposed by Alibaba Group in 2020 and
published in their “VECO: Variable and Flexible Cross-lingual
Pre-training for Language Understanding and
Generation” paper. The official
code for this paper can be found on Alibaba’s official GitHub
repository:
alibaba/VECO.

">
  <meta itemprop="image" content="/cross-lingual-lm/media/VECO/image1.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="VECO: Cross-lingual Pre-training">
  <meta property="og:description" content="VECO stands for “Variable and Flexible Cross-lingual
Pre-training” which is a pre-training approach for cross-lingual
language models that uses “Cross-Attention Masked Language Modeling”
(CA-MLM) objective. VECO was proposed by Alibaba Group in 2020 and
published in their “VECO: Variable and Flexible Cross-lingual
Pre-training for Language Understanding and
Generation” paper. The official
code for this paper can be found on Alibaba’s official GitHub
repository:
alibaba/VECO.

">
  <meta property="og:image" content="/cross-lingual-lm/media/VECO/image1.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="VECO: Cross-lingual Pre-training">
  <meta name="twitter:description" content="VECO stands for “Variable and Flexible Cross-lingual
Pre-training” which is a pre-training approach for cross-lingual
language models that uses “Cross-Attention Masked Language Modeling”
(CA-MLM) objective. VECO was proposed by Alibaba Group in 2020 and
published in their “VECO: Variable and Flexible Cross-lingual
Pre-training for Language Understanding and
Generation” paper. The official
code for this paper can be found on Alibaba’s official GitHub
repository:
alibaba/VECO.

">
  
  <meta name="twitter:image" content="/cross-lingual-lm/media/VECO/image1.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/cross-lingual-lm/VECO">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          5 mins read
        </span>
      </p>
      <time datetime="2020-10-30 00:00" class="post-meta__body date">Published on arXiv on: 30 Oct 2020</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#Alibaba Group Inc.">Alibaba Group Inc.</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=VECO: Cross-lingual Pre-training> VECO: Cross-lingual Pre-training</h1>
    <p>VECO stands for “<strong>V</strong>ariable and Fl<strong>e</strong>xible <strong>C</strong>r<strong>o</strong>ss-lingual
Pre-training” which is a pre-training approach for cross-lingual
language models that uses “Cross-Attention Masked Language Modeling”
(CA-MLM) objective. VECO was proposed by Alibaba Group in 2020 and
published in their “<a href="https://arxiv.org/pdf/2010.16046.pdf">VECO: Variable and Flexible Cross-lingual
Pre-training for Language Understanding and
Generation</a>” paper. The official
code for this paper can be found on Alibaba’s official GitHub
repository:
<a href="https://github.com/alibaba/AliceMind/tree/main/VECO">alibaba/VECO</a>.</p>

<h2 id="ca-mlm-pre-training">CA-MLM Pre-training</h2>

<p>The special thing about VECO pre-training is that it plugs a
cross-attention module into the
<a href="https://phanxuanphucnd.github.io/machine-translation/Transformer">Transformer</a>
architecture to explicitly build the cross-relation between languages
when pre-training using the Cross-Attention Masked Language Modeling MLM
(CA-MLM) objective. The following figure shows the difference between
MLM pre-training without plugging cross-attention module (left) and with
plugging the cross-attention module (right):</p>

<div align="center">
    <img src="media/VECO/image1.png" width="750" />
</div>

<blockquote>
  <p><strong>Note:</strong><br />
MLM here is different than MLM objective proposed in the
<a href="https://phanxuanphucnd.github.io/language-modeling/BERT">BERT</a> paper. As we
can see, here MLM is applied on both encoder and decoder.</p>
</blockquote>

<p>Given a pair of input $\left( x,\ y \right)$ and its MLM corrupted
version $\left( \widehat{x},\ \widehat{y} \right)$, the model tries to
minimize the following loss:</p>

\[\mathcal{L}\left( x,\ y \right) = - \log P\left( x \middle| \widehat{x};\ \theta_{s} \right) - \log P\left( x \middle| \widehat{y},\ \widehat{x};\ \theta_{s},\ \theta_{c} \right)\]

\[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  - \log P\left( y \middle| \widehat{y};\ \theta_{s} \right) - \log P\left( y \middle| \widehat{x},\ \widehat{y};\ \theta_{s},\ \theta_{c} \right)\]

<p>Where $\theta_{s}$ and $\theta_{c}$ are the parameters of self-attention
and cross-attention modules respectively.</p>

<blockquote>
  <p><strong>Note:</strong><br />
In case of monolingual data, CA-MLM uses two adjacent sentences as the
$\left( x,\ y \right)$ pair.</p>
</blockquote>

<h3 id="cross-attention-module">Cross-Attention Module</h3>

<p>As said before, the VECO pre-training extends the
<a href="https://phanxuanphucnd.github.io/machine-translation/Transformer">Transformer</a>
architecture and plugs a cross-attention module in each layer. Now, each
layer has three modules: self-attention module, a plug-and-play
cross-attention module, and a feed-forward linear module. Both
self-attention and cross-attention modules are based on the multi-head
<a href="https://phanxuanphucnd.github.io/machine-translation/Attention">attention
mechanism</a>.</p>

<p>An attention function can be described as mapping a query $Q$ and a set
of key-value $K,V$ pairs to an output. For the self-attention module,
all the queries, keys and values are the same representations from the
previous layer. Specifically, for the $l^{\text{th}}$ layer, the output
of a self-attention head $A_{l}^{s}$ is computed via:</p>

\[Q = H^{l - 1}W_{l}^{Q},\ \ \ K = H^{l - 1}W_{l}^{K},\ \ \ V = H^{l - 1}W_{l}^{V}\]

\[A_{l}^{s} = \text{softmax}\left( \frac{QK^{T}}{\sqrt{d_{k}}} \right)V\]

<p>Where $H^{l - 1}$ is the output of the previous layer, and
$W_{i}^{Q},W_{i}^{K},W_{i}^{V}$ are learned parameter matrices of the
self-attention.</p>

<p>For the cross-attention module, the queries come from the previous
layer, and the keys and values come from the last layer’s
representations of paired input. Specifically, for the $l^{\text{th}}$
layer, the output of a cross-attention head $A_{l}^{c}$ is computed via:</p>

\[Q = S^{l - 1}U_{l}^{Q},\ \ \ K = H^{L}U_{l}^{K},\ \ \ V = H^{L}U_{l}^{V}\]

\[A_{l}^{c} = \text{softmax}\left( \frac{QK^{T}}{\sqrt{d_{k}}} \right)V\]

<p>Where $S^{l - 1}$ is the output of the previous layer,
$U_{i}^{Q},U_{i}^{K},\ U_{i}^{V}$ are learned parameter matrices of the
cross-attention, and finally $H^{L}$ is the output of the last layer.</p>

<blockquote>
  <p><strong>Note:</strong><br />
There are two types of contextualized vector representation:</p>

  <ul>
    <li>$H$: Which depends on the masked sequence:</li>
  </ul>

\[P\left( x \middle| \widehat{x} \right) = \text{softmax}\left( f\left( H_{x}^{L} \right) \right),\ \ \ \ \ P\left( y \middle| \widehat{y} \right) = \text{softmax}\left( f\left( H_{y}^{L} \right) \right)\]

  <ul>
    <li>$S$: Which depends on the masked paired sequence:</li>
  </ul>

\[P\left( x \middle| \widehat{x},\ \widehat{y} \right) = \text{softmax}\left( f\left( S_{x}^{L} \right) \right),\ \ \ \ \ P\left( y \middle| \widehat{x},\ \widehat{y} \right) = \text{softmax}\left( f\left( S_{y}^{L} \right) \right)\]
</blockquote>

<p>Furthermore, this cross-attention module can be plugged-in or
plugged-out on-demand which makes it very suitable for both
cross-lingual language understanding tasks and generation tasks as we
are going to see next.</p>

<p>Regrading cross-lingual understanding tasks on the XTREME benchmark,
VECO ranks first at the
<a href="https://sites.research.google/xtreme">leaderboard</a> at the submission
deadline. Regrading cross-lingual generation tasks such as machine
translation, VECO outperforms existing state-of-the-art models by 1∼2
BLEU score.</p>

<h2 id="fine-tuning">Fine-tuning</h2>

<p>As illustrated in the following figure, VECO is very flexible when
fine-tuning on various downstream tasks due to the plug-and-play
cross-attention module. It can be used for initializing the encoder-only
Transformer for natural language understanding (NLU) tasks and
encoder-decoder Transformer for natural language generation (NLG) tasks.</p>

<div align="center">
    <img src="media/VECO/image2.png" width="750" />
</div>

<p>During NLU fine-tuning, there are two approaches that we can choose
from:</p>

<ul>
  <li>
    <p><strong>Plug-out fine-tuning:</strong> is to unplug the cross-attention module
from the pre-trained model which makes the model very similar to
<a href="https://phanxuanphucnd.gtihub.io/multilingual-nmt/mBERT">mBERT</a>.</p>
  </li>
  <li>
    <p><strong>Plug-in fine-tuning:</strong> is to plug the cross-attention module into
the fine-tuned model. This can be used if the other language y is
available in the downstream task. In this approach, the two
representations $\left\lbrack H_{x}^{L};S_{x}^{L} \right\rbrack$ are
concatenated to predict the label of $x$ and
$\left\lbrack H_{y}^{L};S_{y}^{L} \right\rbrack$ are concatenated to
predict the label of $y$.</p>
  </li>
</ul>

<h2 id="experiments--results">Experiments &amp; Results</h2>

<p>They collected monolingual and bilingual corpus covering 50 languages.
For monolingual training datasets, they extracted 1.36TB data in 50
languages from the
<a href="https://github.com/facebookresearch/cc_net">CommonCrawl</a> Corpus, which
contains 6.5G sentences and 0.4G documents. The following table has
statistics for all 50 languages monolingual data where the values are in
million:</p>

<div align="center">
    <img src="media/VECO/image3.png" width="750" />
</div>

<p>They up/down-sample the monolingual text like
<a href="https://phanxuanphucnd.github.io/cross-lingual-lm/XLM">XLM</a> from each
language with a smoothing parameter $\alpha = 0.5$ where $D_{l}$ is the
number of sentences in language $l$.</p>

\[p_{l} = \left( \frac{D_{l}}{\sum_{k}^{}D_{k}} \right)^{\alpha}\]

<p>For bilingual data, they extracted 6.4G parallel sentences across 50
languages from the <a href="http://opus.nlpl.eu/">OPUS</a> website.</p>

<p>In this paper, they used pre-trained two VECO models following the
<a href="https://phanxuanphucnd.github.io/cross-lingual-lm/XLM-R">XLM-R</a> model. The
large one is a 24-layer model with 1024 embedding/hidden size and 4096
feed-forward size, while the small one is 6 layers with 768
embedding/hidden size and 3072 feed-forward size. Also, they used the
same 250K vocabulary that was used by
<a href="https://phanxuanphucnd.github.io/cross-lingual-lm/XLM-R">XLM-R</a> . The full
list of the model can be seen in the following table:</p>

<div align="center">
    <img src="media/VECO/image4.png" width="450" />
</div>

<p>The following table shows a comparison between VECO and other baselines:</p>

<div align="center">
    <img src="media/VECO/image5.png" width="750" />
</div>

<p>When fine-tuning on natural language understanding tasks, VECO
outperforms previous cross-lingual models on all datasets of the XTREME
benchmark:</p>

<div align="center">
    <img src="media/VECO/image6.png" width="750" />
</div>

<p>When fine-tuning on natural language generation tasks (machine
translation), VECO outperforms the randomly initialized same-sized
Transformer baseline by 2.3 BLEU points. Moreover, it even beats the
(randomly initialized) state-of-the-art Deep-Transformer, which is three
times deep as VECO.</p>

<div align="center">
    <img src="media/VECO/image7.png" width="450" />
</div>

<p>And to investigate where the improvement in VECO comes from, they
trained <a href="https://phanxuanphucnd.github.io/cross-lingual-lm/XLM">XLM</a>,
<a href="https://phanxuanphucnd.github.io/multilingual_nmt/XLM">mBART</a> and VECO model
from scratch using the same datasets and parameter settings where all of
them were pre-trained on MLM and TLM tasks and fine-tuned on XNLI
downstream task and machine translation De-En pair from the IWSLT14
dataset. The results are shown in the following table:</p>

<div align="center">
    <img src="media/VECO/image8.png" width="450" />
</div>

<p>The previous table shows that:</p>

<ul>
  <li>
    <p>When pre-training on monolingual data only, VECO outperforms XLM by
0.8 points on the XNLI dataset and 0.3 BLEU scores on the IWSLT
dataset which suggests that <u><strong>CA-MLM can still benefit from
adjacent sentences in monolingual corpus</strong></u>. Remember that
CA-MLM uses adjacent sentences as the $\left( x,y \right)$ pair.</p>
  </li>
  <li>
    <p>When pre-training on both monolingual and bilingual data, VECO
achieved a larger improvement compared to XLM, with 3.2 and 2.1
points improvement on the two datasets, respectively. <u><strong>It
reveals that CA-MLM objective of VECO can better utilize the bilingual
corpus, compared to only optimized by TLM and MLM of XLM.</strong></u></p>
  </li>
</ul>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/cross-lingual-lm/VECO';
      this.page.identifier = '/cross-lingual-lm/VECO';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>