<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>mT6: Multilingual T5 with Translation Pairs</title>
  <meta name="title" content="mT6: Multilingual T5 with Translation Pairs">
  <meta name="description" content="mT6 stands for “Multilingual Text-to-Text Transfer Transformer with
Translation pairs” which is an attempt to improve the performance of the
mT5 model by
incorporating translation objectives into the pre-training part. This
model was proposed by Microsoft Research in 2021 and published in this
paper: mT6: Multilingual Pretrained Text-to-Text Transformer with
Translation Pairs.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="mT6: Multilingual T5 with Translation Pairs">
  <meta itemprop="description" content="mT6 stands for “Multilingual Text-to-Text Transfer Transformer with
Translation pairs” which is an attempt to improve the performance of the
mT5 model by
incorporating translation objectives into the pre-training part. This
model was proposed by Microsoft Research in 2021 and published in this
paper: mT6: Multilingual Pretrained Text-to-Text Transformer with
Translation Pairs.

">
  <meta itemprop="image" content="/cross-lingual-lm/media/mT6/image0.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="mT6: Multilingual T5 with Translation Pairs">
  <meta property="og:description" content="mT6 stands for “Multilingual Text-to-Text Transfer Transformer with
Translation pairs” which is an attempt to improve the performance of the
mT5 model by
incorporating translation objectives into the pre-training part. This
model was proposed by Microsoft Research in 2021 and published in this
paper: mT6: Multilingual Pretrained Text-to-Text Transformer with
Translation Pairs.

">
  <meta property="og:image" content="/cross-lingual-lm/media/mT6/image0.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="mT6: Multilingual T5 with Translation Pairs">
  <meta name="twitter:description" content="mT6 stands for “Multilingual Text-to-Text Transfer Transformer with
Translation pairs” which is an attempt to improve the performance of the
mT5 model by
incorporating translation objectives into the pre-training part. This
model was proposed by Microsoft Research in 2021 and published in this
paper: mT6: Multilingual Pretrained Text-to-Text Transformer with
Translation Pairs.

">
  
  <meta name="twitter:image" content="/cross-lingual-lm/media/mT6/image0.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/cross-lingual-lm/mT6">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          4 mins read
        </span>
      </p>
      <time datetime="2021-04-18 00:00" class="post-meta__body date">Published on arXiv on: 18 Apr 2021</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#Beijing Institute of Technology">Beijing Institute of Technology</a> & <a href="/labs/#Microsoft Research">Microsoft Research</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=mT6: Multilingual T5 with Translation Pairs> mT6: Multilingual T5 with Translation Pairs</h1>
    <p>mT6 stands for “Multilingual Text-to-Text Transfer Transformer with
Translation pairs” which is an attempt to improve the performance of the
<a href="https://phanxuanphucnd.github.io/cross-lingual-lm/mT5">mT5</a> model by
incorporating translation objectives into the pre-training part. This
model was proposed by Microsoft Research in 2021 and published in this
paper: <a href="https://arxiv.org/pdf/2104.08692.pdf">mT6: Multilingual Pretrained Text-to-Text Transformer with
Translation Pairs</a>.</p>

<p>A little bit of background: the mT5 model was pre-trained on mC4 dataset
(a multilingual version of the
<a href="https://www.tensorflow.org/datasets/catalog/c4">C4</a> corpus) with a
masked language modeling “span-corruption” objective, where the encoder
is fed a chunk of text with random spans replaced with a mask token, and
the decoder must reconstruct the masked-out tokens. MT6 differs from MT5
in terms of both pre-training tasks and the training objective. We are
going to talk about that next.</p>

<h2 id="pre-training-tasks">Pre-training Tasks</h2>

<p>One of the pre-training tasks used for T5 model and consequently mT5 was
Span Corruption which first randomly masks several spans of the input
sentence then the output is the concatenation of the original tokens of
the masked spans, each of which starts with a unique mask token to
indicate the span to be decoded as shown in the following figure:</p>

<div align="center">
    <img src="media/mT6/image1.png" width="450" />
</div>

<p>In the paper, they presented three more text-to-text pre-training tasks
for improving mT5 with translation data. These pre-training tasks are:</p>

<ul>
  <li><strong><u>Machine Translation (MT):</u></strong><br />
This is a typical text-to-text task with the goal of translating a
sentence from the source language into a target language.</li>
</ul>

<div align="center">
    <img src="media/mT6/image2.png" width="450" />
</div>

<ul>
  <li><strong><u>Translation Pair Span Corruption (TPSC):</u></strong><br />
Inspired by the
<a href="https://phanxuanphucnd.github.io/language-modeling/MASS">MASS</a> objective,
this task aims to predict the masked spans from a translation pair
instead of a monolingual sentence.</li>
</ul>

<div align="center">
    <img src="media/mT6/image3.png" width="450" />
</div>

<ul>
  <li><strong><u>Translation Span Corruption (TSC):</u></strong><br />
A potential issue of TPSC is that the spans in the target sequence
can be organized in unnatural word order. As shown in Figure 2, the
output sequence of TPSC is organized as <em>”[M1] for your [M2]
last week [M3] invitation [M4]“</em>. It can be found that the
French word “invitation” is after the English word “week”, which
could harm the language model of the decoder. This motivated them to
propose the translation span corruption (TSC) task where they only
masked and predict the spans in one language.</li>
</ul>

<div align="center">
    <img src="media/mT6/image4.png" width="450" />
</div>

<p>Regarding the data, they used natural sentences from CCNet in 94 languages for
monolingual text-to-text tasks and parallel corpora of 14 English-centric
language pairs, collected from MultiUN, IIT Bombay, OPUS, and WikiMatrix.</p>

<h2 id="pnat-objective">PNAT Objective</h2>

<p>Recall that the backbone architecture of mT5 is the simple
encoder-decoder Transformer which is trained to predict the target text
conditioned on the input source text in auto-regressive manner. Let $x$
and $y$ denote the input sequence and the output sequence respectively,
the loss function of mT5 will be:</p>

\[\mathcal{L} = - \sum_{i = 1}^{\left| y \right|}{\log\left( p\left( y_{i} \middle| x,\ y_{&lt; i} \right) \right)}\]

<p>To encourage mT6 to utilize more information from the encoding side
while preserving the ability of auto-regressive decoding, they proposed
a new training objective for text-to-text training, called partially
non-auto-regressive decoding (PNAT). Let’s see how PNAT works.</p>

<p>Given an input sequence containing $m$ spans, they divided it into
groups $n_{g}$ groups and trained the model to decode each group
separately where the prediction is only conditioned on the tokens from
the same group as shown in the following figure:</p>

<div align="center">
    <img src="media/mT6/image5.png" width="450" />
</div>

<p>For the $j^{\text{th}}$ group, $l_{j}$ and $r_{j}$ are the start
position and the end position respectively. The PNAT objective is
defined as:</p>

\[\mathcal{L}^{\text{PNAT}} = - \sum_{j = 1}^{n_{g}}{\sum_{i = l_{j}}^{r_{j}}{\log\left( p\left( y_{i} \middle| x,\ y_{l_{j}}\text{\ ...\ }y_{i - 1} \right) \right)}}\]

<p>The mT6 model is jointly pre-trained on both monolingual and parallel
corpora using the original Span Corruption objective along with one of
the three different pre-training tasks proposed in this paper. So, the
overall pre-training objective is:</p>

\[\mathcal{L}_{mT6} = \mathcal{L}_{\text{SC}}^{\text{PNAT}} + \mathcal{L}_{X}^{\text{PNAT}},\ \ \ \ x \in \left\{ MT,\ TPSC,\ TSC \right\}\]

<h2 id="results">Results</h2>

<p>In all of the experiments, they considered the mT5-small model, with
$d_{\text{model}} = 512$, $d_{\text{ff}} = 1024$, $6$ attention heads,
and 8 layers for both the encoder and the decoder. They used the
vocabulary provided by XLM-R, and extended it with 100 unique mask
tokens for the span corruption tasks. They pre-trained mT6 for $0.5M$
steps with batches of $256$ length-$512$ input sequences. The model was
optimized by the Adam optimizer with a linear learning rate scheduler.
All hyper-parameters needed for pre-training and fine-tuning this model
are described below:</p>

<div align="center">
    <img src="media/mT6/image6.png" width="750" />
</div>

<p>And the following are the results on fine-tuning benchmarks:</p>

<ul>
  <li><strong>XTREME:</strong> All results are averaged over five runs.</li>
</ul>

<div align="center">
    <img src="media/mT6/image7.png" width="750" />
</div>
<ul>
  <li><strong>Gigaword multilingual Abstractive summarization:</strong> RG is short for
ROUGE. mT5 &amp; mT6 results are averaged over three runs:</li>
</ul>

<div align="center">
    <img src="media/mT6/image8.png" width="750" />
</div>

<ul>
  <li><strong>Wikilingua cross-lingual summarization:</strong> All results are ROUGE-2
scores and averaged over three runs:</li>
</ul>

<div align="center">
    <img src="media/mT6/image9.png" width="450" />
</div>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/cross-lingual-lm/mT6';
      this.page.identifier = '/cross-lingual-lm/mT6';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>