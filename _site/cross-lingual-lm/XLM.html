<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>XLM</title>
  <meta name="title" content="XLM">
  <meta name="description" content="XLM stands for “Cross-lingual Language Modeling” which is a model
created by FacebookAI in 2019 and published in this paper:
“Cross-lingual Language Model Pretraining”.
XLM is an 12-layer encoder-transformer of with 1024
hidden units, 16 heads, and GELU activation.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="XLM">
  <meta itemprop="description" content="XLM stands for “Cross-lingual Language Modeling” which is a model
created by FacebookAI in 2019 and published in this paper:
“Cross-lingual Language Model Pretraining”.
XLM is an 12-layer encoder-transformer of with 1024
hidden units, 16 heads, and GELU activation.

">
  <meta itemprop="image" content="/cross-lingual-lm/media/XLM/image0.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="XLM">
  <meta property="og:description" content="XLM stands for “Cross-lingual Language Modeling” which is a model
created by FacebookAI in 2019 and published in this paper:
“Cross-lingual Language Model Pretraining”.
XLM is an 12-layer encoder-transformer of with 1024
hidden units, 16 heads, and GELU activation.

">
  <meta property="og:image" content="/cross-lingual-lm/media/XLM/image0.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="XLM">
  <meta name="twitter:description" content="XLM stands for “Cross-lingual Language Modeling” which is a model
created by FacebookAI in 2019 and published in this paper:
“Cross-lingual Language Model Pretraining”.
XLM is an 12-layer encoder-transformer of with 1024
hidden units, 16 heads, and GELU activation.

">
  
  <meta name="twitter:image" content="/cross-lingual-lm/media/XLM/image0.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/cross-lingual-lm/XLM">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          4 mins read
        </span>
      </p>
      <time datetime="2019-01-22 00:00" class="post-meta__body date">Published on arXiv on: 22 Jan 2019</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#FAIR">FAIR</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=XLM> XLM</h1>
    <p>XLM stands for “Cross-lingual Language Modeling” which is a model
created by FacebookAI in 2019 and published in this paper:
“<a href="https://arxiv.org/pdf/1901.07291.pdf">Cross-lingual Language Model Pretraining</a>”.
XLM is an 12-layer <strong><u>encoder-transformer</u></strong> of with 1024
hidden units, 16 heads, and GELU activation.</p>

<p>XLM was trained on multiple languages with new pre-training objectives
that pushed the XNLI benchmark by an absolute gain of 4.9% accuracy and
obtained 34.3 BLEU on WMT’16 German-English which is 9 BELU more than
the state-of-the-art back then. The official code for this paper can be
found in the FairSeq official repository:
<a href="https://github.com/pytorch/fairseq/tree/master/examples/cross_lingual_language_model">fairseq/cross_lingual_language_model</a>.</p>

<p>In this paper, the authors presented three language modeling objectives
for the language modeling. Two of them only require monolingual data
(unsupervised), while the third one requires parallel sentences
(supervised). And these objectives are:</p>

<ul>
  <li>
    <p>CLM (Causal Language Modeling).</p>
  </li>
  <li>
    <p>MLM (Masked Language Modeling).</p>
  </li>
  <li>
    <p>TLM (Translation Language Modeling).</p>
  </li>
</ul>

<p>Before getting into pre-training, let’s first check the vocabulary used
for our XLM.</p>

<h2 id="shared-vocabulary">Shared Vocabulary</h2>

<p>In the paper, they processed all languages with the same shared
vocabulary created through Byte Pair Encoding (BPE). They used 80k BPE
splits and a vocabulary of 95k. This greatly improves the alignment of
embedding spaces across languages that share either the same alphabet or
other tokens such as digits or proper nouns.</p>

<p>They learned the BPE splits on the concatenation of sentences sampled
randomly from the monolingual corpora. Sentences are sampled according
to a multinomial distribution with probabilities
$\left[ q_i \right]_{i = 1..N}$, where $N$ is the number of
languages and $n_{i}$ is the number of sentences in the $i^{th}$
monolingual corpora:</p>

\[q_{i} = \frac{p_{i}^{\alpha}}{\sum_{j = 1}^{N}p_{j}^{\alpha}},\ \ \ \ \ \ \ \ \ \ \ \ \ p_{i} = \frac{n_{i}}{\sum_{k = 1}^{N}n_{k}}\]

<p>Sampling with this distribution increases the number of tokens
associated to low-resource languages and alleviates the bias towards
high-resource languages. In particular, this prevents words of
low-resource languages from being split at the character level.</p>

<h2 id="pre-training-objectives">Pre-training Objectives</h2>

<p>In this part, we are going to discuss the three language modeling
objectives used for the language modeling pre-training. The first two of
them only require monolingual data (unsupervised), while the third one
requires parallel sentences (supervised).</p>

<h3 id="clm">CLM</h3>

<p>CLM stands for “Causal Language Modeling” which is one of the three
objectives used for learning the cross-lingual language model. This
objective is used to train the model predict the probability of a word
given the previous words in a sentence
$P\left( w_{t} \middle| w_{1},\ …\ w_{t - 1},\ \theta \right)$.
OpenAI’s GPT and GPT-2 are trained on this objective. You can refer to
my articles on GPT and GPT-2 if you’re interested in the details of this
objective.</p>

<h3 id="mlm">MLM</h3>

<p>MLM stands for “Masked Language Modeling” which is the second objective
out of the three objectives used for learning the cross-lingual language
model. MLM is the same objective used for BERT where we randomly sample
15% of the BPE tokens from the text streams, replace them by a [MASK]
token 80% of the time, by a random token 10% of the time, and we keep
them unchanged 10% of the time.</p>

<div align="center">
    <img src="media/XLM/image1.png" width="750" />
</div>

<p>In face, there are two differences between this approach and the one
used with BERT. And they are:</p>

<ul>
  <li>
    <p>Here, they used text streams of an arbitrary number of sentences
(truncated at 256 tokens) instead of pairs of sentences.</p>
  </li>
  <li>
    <p>To counter the imbalance between frequent tokens (e.g. punctuations
or stop words) and rare tokens, they sub-sampled the frequent
outputs according to a multinomial distribution, whose weights are
proportional to the square root of their invert frequencies.</p>
  </li>
</ul>

<h3 id="tlm">TLM</h3>

<p>TLM stands for “Translation Language Modeling” which is the third and
last objective used for learning the cross-lingual language model. Both
the CLM and MLM objectives are unsupervised and only require monolingual
data. However, these objectives cannot be used to leverage parallel data
when it is available. So, this objective is used to leverage the
existence of parallel data to improve cross-lingual pre-training.</p>

<p>TLM is an extension of MLM, where instead of considering monolingual
text streams, we concatenate parallel sentences and perform MLM over
this concatenation. For example, let’s assume that we have a parallel
entry of two languages English and French as shown below:</p>

<div align="center">
    <img src="media/XLM/image2.png" width="750" />
</div>

<p>To predict a word masked in an English sentence, the model can either
attend to surrounding English words or to the French translation,
encouraging the model to align the English and French representations.
In particular, the model can leverage the French context if the English
one is not sufficient to infer the masked English words. To facilitate
the alignment, we also reset the positions of target sentences.</p>

<h2 id="fine-tuning">Fine-tuning</h2>

<p>In the paper, the XLM model was fine-tuned on various tasks -all from
the XNLI dataset- which shows that XLM can be used for a wide variety of
cross-lingual tasks:</p>

<ul>
  <li><strong>Zero-shot Cross-lingual Classification:</strong><br />
They fine-tuned XLMs on a cross-lingual classification by adding a
linear classifier on top of the first hidden state of the pretrained
Transformer, and fine-tuned all parameters on the English NLI
training dataset and evaluated it on the other 15 XNLI languages:</li>
</ul>

<div align="center">
    <img src="media/XLM/image3.png" width="750" />
</div>

<ul>
  <li><strong>Unsupervised Machine Translation:</strong><br />
They pretrained the entire encoder and decoder with a
cross-lingual language model and explored various initialization
schemes as shown below:</li>
</ul>

<div align="center">
    <img src="media/XLM/image4.png" width="450" />
</div>

<ul>
  <li>
    <p><strong>Low-resource Language Modeling:</strong><br />
Here’s where “languages with the same script or similar words
provide better mapping” comes into the picture. For example, around
80% of Nepali tokens (low-resource language) is in common with Hindi
(high-resource-language):</p>
  </li>
  <li>
    <p><strong>Cross-lingual Word Embedding:</strong><br />
Since we have a shared vocabulary, the lookup table (or embedding
matrix) of the XLM model gives us the cross-lingual word embeddings.</p>
  </li>
</ul>

<div align="center">
    <img src="media/XLM/image5.png" width="450" />
</div>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/cross-lingual-lm/XLM';
      this.page.identifier = '/cross-lingual-lm/XLM';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>