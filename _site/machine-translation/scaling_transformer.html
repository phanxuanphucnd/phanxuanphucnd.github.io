<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>Scaling Transformer</title>
  <meta name="title" content="Scaling Transformer">
  <meta name="description" content="Scaling model sizes, datasets and the total computation budget has been
identified as a reliable approach to improve generalization performance
on several machine learning tasks. Here, we are going to discuss a paper
called “Scaling Laws for Neural Machine
Translation” published by Google
Research in 2021 where the researchers study the effect of scaling the
transformer depths on the performance.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="Scaling Transformer">
  <meta itemprop="description" content="Scaling model sizes, datasets and the total computation budget has been
identified as a reliable approach to improve generalization performance
on several machine learning tasks. Here, we are going to discuss a paper
called “Scaling Laws for Neural Machine
Translation” published by Google
Research in 2021 where the researchers study the effect of scaling the
transformer depths on the performance.

">
  <meta itemprop="image" content="/machine-translation/media/scaling_transformer/image4.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="Scaling Transformer">
  <meta property="og:description" content="Scaling model sizes, datasets and the total computation budget has been
identified as a reliable approach to improve generalization performance
on several machine learning tasks. Here, we are going to discuss a paper
called “Scaling Laws for Neural Machine
Translation” published by Google
Research in 2021 where the researchers study the effect of scaling the
transformer depths on the performance.

">
  <meta property="og:image" content="/machine-translation/media/scaling_transformer/image4.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Scaling Transformer">
  <meta name="twitter:description" content="Scaling model sizes, datasets and the total computation budget has been
identified as a reliable approach to improve generalization performance
on several machine learning tasks. Here, we are going to discuss a paper
called “Scaling Laws for Neural Machine
Translation” published by Google
Research in 2021 where the researchers study the effect of scaling the
transformer depths on the performance.

">
  
  <meta name="twitter:image" content="/machine-translation/media/scaling_transformer/image4.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/machine-translation/scaling_transformer">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          5 mins read
        </span>
      </p>
      <time datetime="2021-09-16 00:00" class="post-meta__body date">Published on arXiv on: 16 Sep 2021</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#Google Research">Google Research</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=Scaling Transformer> Scaling Transformer</h1>
    <p>Scaling model sizes, datasets and the total computation budget has been
identified as a reliable approach to improve generalization performance
on several machine learning tasks. Here, we are going to discuss a paper
called “<a href="https://arxiv.org/pdf/2109.07740.pdf">Scaling Laws for Neural Machine
Translation</a>” published by Google
Research in 2021 where the researchers study the effect of scaling the
transformer depths on the performance.</p>

<h2 id="experiments">Experiments</h2>

<p>All experiments in this paper were performed using a series of
<a href="https://phanxuanphucnd.github.io/machine-translation/Transformer">Transformer</a>
networks where the model dimension to $1024$, feed-forward layer
dimension to $8192$, number of attention heads to $16$, attention hidden
dimension to $1024$, and varying layers as shown in the following figure
knowing that the baseline is a 12-layer encoder 12-layer decoder
Transformer model:</p>

<div align="center">
    <img src="media/scaling_transformer/image1.png" width="1050" />
</div>

<p>All models were trained with a cross-entropy loss and Adafactor
optimizer. All models were trained for $500k$ training steps with a
fixed batch-size of $500k$ tokens. All models were regularized using a
dropout rate of $0.1$, label smoothing of $0.1$, and a gradient clipping
of $10$ to improve the training stability.</p>

<h3 id="training-data">Training Data</h3>

<p>All results -in this paper- were reported on two language pairs,
(English→German) and (German→English) using an in-house web-crawled
dataset with around 2.2 billion sentence pairs for both translation
directions. A sentence-piece vocabulary of size $32k$ was used for
training all models. This dataset provides a large enough training set
to ensure the dataset size is not a bottleneck in the model performance.</p>

<h3 id="evaluation-data">Evaluation Data</h3>

<p>To evaluate these models, they used a variety of test sets covering
different domains such as <u><strong>Web-Domain</strong></u>,
<u><strong>News-Domain</strong></u>, <u><strong>Wikipedia</strong></u>, and
<u><strong>Patents</strong></u>. The news-domain test sets come from the
WMT2019 evaluation campaign (newstest2019) while the other test sets are
internal test sets.</p>

<p>Throughout the paper, they refer to a certain data as either
“source-original” or “target-original”, here is the difference:</p>

<ul>
  <li>
    <p><strong>Source-original:</strong> means that the <u><strong>source</strong></u>
sentences have been crawled from the web while the reference translations
were generated later by professional translators.</p>
  </li>
  <li>
    <p><strong>Target-original:</strong> means that the <u><strong>target</strong></u>
sentences have been crawled from the web while the reference
translations were generated later by professional translators.</p>
  </li>
</ul>

<h2 id="scaling-effect-on-loss">Scaling Effect on Loss</h2>

<p>In the paper, the researchers were able to find a formula that could
capture the effect of scaling the encoder/decoder layers on the
performance of a certain test set. Given the English→German web-domain
test set, they trained multiple models with different encoder/decoder
layers and measured the perplexity of each model on the test set. The
following graph shows the performance of the encoder-scaling and
decoder-scaling on source-original (left) and target-original (right):</p>

<div align="center">
    <img src="media/scaling_transformer/image2.png" width="750" />
</div>

<p>As we can see, the dashed line of each case fits the points perfectly
(variance $R^{2} = 99.4$). These dashed lines were creating using the
power law of the form:</p>

\[\widehat{L}\left( N \right) = \alpha N^{- p} + L_{\infty}\]

<p>Where $N$ is the total number of parameters outside of embedding /
softmax layers and $\left( \alpha,\ p,\ L_{\infty} \right)$ are the
fitted parameters of the power law <u><strong>which change based on the
data</strong></u>.</p>

<p>Now, given a Transformer model of $\overline{N}_e$ encoder layers
and $\overline{N}_d$ decoder layers trained on a certain dataset,
the following formula predicts the new performance on the same dataset
when the encoder layers become $N_e$ and the decoder layers become
$N_d$:</p>

\[\widehat{L}\left( N_{e},\ N_{d} \right) = \alpha\left( \frac{\overline{N}_e}{N_{e}} \right)^{p_{e}}\left( \frac{\overline{N}_d}{N_{d}} \right)^{p_{d}} + L_{\infty}\]

<p>Where the parameters
$\left( \alpha,\ p_{e},\ p_{d},\ L_{\infty} \right)$ are achieved using
the following algorithm that uses <code class="language-plaintext highlighter-rouge">scipy.optimize.least_squares()</code>
function for curve fitting while using <code class="language-plaintext highlighter-rouge">'soft_l1'</code> loss option which is
a popular option to have some robustness to outliers:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="s">"""
    Fitting a bi-variate scaling law.
    p: A 1-D array of dim 4, corresponding to alpha, p_e, p_d, l_inf.
    x: A matrix of dimension n </span><span class="se">\t</span><span class="s">imes 2. First column encoder params,
    second col decoder params.
    y: A 1-D array of log-perplexity of dim n.
    """</span>
    <span class="n">x_e</span> <span class="o">=</span> <span class="n">NE_bar</span> <span class="o">/</span> <span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_d</span> <span class="o">=</span> <span class="n">ND_bar</span> <span class="o">/</span> <span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">power</span><span class="p">(</span><span class="n">x_e</span><span class="p">,</span> <span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">power</span><span class="p">(</span><span class="n">x_d</span><span class="p">,</span> <span class="n">p</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="n">p</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span>


<span class="k">def</span> <span class="nf">fit_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f_scale</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">to_numpy</span><span class="p">().</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">to_numpy</span><span class="p">().</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">X</span><span class="p">).</span><span class="nb">any</span><span class="p">()</span> <span class="ow">or</span> <span class="n">np</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="nb">any</span><span class="p">():</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">'Data contains NaNs'</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">'Error in shapes'</span><span class="p">)</span>
    <span class="n">p0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">4</span> <span class="p">,))</span>
    <span class="n">p0</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="c1"># alpha
</span>    <span class="n">p0</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.4</span> <span class="c1"># p_e
</span>    <span class="n">p0</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.6</span> <span class="c1"># p_d
</span>    <span class="n">p0</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="c1"># l_inf
</span>    <span class="n">fit</span> <span class="o">=</span> <span class="n">least_squares</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'soft_l1'</span><span class="p">,</span> <span class="n">f_scale</span><span class="o">=</span><span class="n">f_scale</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">max_nfev</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">return</span> <span class="n">fit</span>
</code></pre></div></div>

<blockquote>
  <p><strong>Note:</strong><br />
The parameter $\alpha$ corresponds to the maximum loss reduction
compared to the baseline model, while the parameter $L_{\infty}$
corresponds to the irreducible loss of the data.</p>
</blockquote>

<p>The above formula captures the scaling behavior of the Transformer NMT
models over a certain dataset. The following figure shows different
values of the encoder exponent and the decoder exponent over multiple
test sets:</p>

<div align="center">
    <img src="media/scaling_transformer/image3.png" width="750" />
</div>

<p>As we can see, the decoder exponents $p_{d}$ were observed to be larger
than the encoder exponents $p_{e}$. As a result, <u><strong>when improving the
test loss is concerned, it is much more effective to scale the decoder
rather than the encoder</strong></u>. This is contrary to the usual practice
where many practitioners train NMT models with deep encoders and shallow
decoders.</p>

<p>Now to a very important question, “what is the optimal Transformer size
given a certain dataset?” The paper proposed the following formula which
predicts the optimal size for both the encoder and the decoder:</p>

\[N_{e}^{*} = \frac{p_{e}}{p_{e} + p_{d}}B,\ \ \ \ \ \ \ \ N_{d}^{*} = \frac{p_{d}}{p_{e} + p_{d}}B\]

<p>Where $B$ is the total number of parameters you can afford in your
organization. In addition, when optimally scaling the model, the scaling
law reduces to:</p>

\[\alpha^{*} \equiv \alpha\left( \frac{\overline{N}_e\left( p_{e} + p_{d} \right)}{p_{e}} \right)^{p_{e}}\left( \frac{\overline{N}_d\left( p_{e} + p_{d} \right)}{p_{e}} \right)^{p_{d}}\]

<blockquote>
  <p><u><strong>Important Note:</strong></u><br />
In the paper, they found out that symmetrically scaling the encoder and
decoder layers, which yields $\frac{N_{d}}{N} \approx 0.55$, is barely
distinguishable from the optimal scaling scheme.</p>
</blockquote>

<h2 id="scaling-effect-on-quality">Scaling Effect on Quality</h2>

<p>They examined the effects of scaling on the output quality as measured
by BLEU score. The following figure presents the co-evolution of BLEU
score and cross-entropy loss throughout the training for all of our
models.</p>

<div align="center">
    <img src="media/scaling_transformer/image4.png" width="750" />
</div>

<p>Depending on the construction of the test sets, two different empirical
behaviors emerge:</p>

<ul>
  <li>On <u><strong>target-original test sets</strong></u>, larger models are able to
improve (lower) the test loss which was accompanied with consistent
improvements (increases) in BLEU score. In fact, the following
simple power law can capture the relation:</li>
</ul>

\[BLEU = c_{B}L^{- p_{B}}\]

<ul>
  <li>
    <p>On <u><strong>source-original test sets</strong></u>, larger models
consistently achieve better (lower) test losses, however, beyond a certain
threshold, BLEU scores begin to deteriorate.</p>
  </li>
  <li>
    <p>Also, a careful look at the left-subplots brings up another
interesting trend. At similar values of the test loss,
<u><strong>encoder-scaled models result in better generation quality
compared to decoder-scaled models</strong></u>. This findings agrees with
previous work that relied on encoder-scaling when optimizing for
BLEU.</p>
  </li>
</ul>


  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/machine-translation/scaling_transformer';
      this.page.identifier = '/machine-translation/scaling_transformer';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>