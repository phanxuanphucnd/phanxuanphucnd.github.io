<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>Transformers</title>
  <meta name="title" content="Transformers">
  <meta name="description" content="Transformer architecture is a novel architecture for encoder-decoder
paradigm created in an attempt to combine all good things from
Seq2Seq
architecture and
ConvS2S with
attention mechanisms. Transformer was proposed by a team from Google
Research and Google Brain in 2017 and published in a paper under the
name: “Attention is all you
need”. The official code for this
paper can be found on the Tensor2Tensor official GitHub repository:
tensor2tensor.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="Transformers">
  <meta itemprop="description" content="Transformer architecture is a novel architecture for encoder-decoder
paradigm created in an attempt to combine all good things from
Seq2Seq
architecture and
ConvS2S with
attention mechanisms. Transformer was proposed by a team from Google
Research and Google Brain in 2017 and published in a paper under the
name: “Attention is all you
need”. The official code for this
paper can be found on the Tensor2Tensor official GitHub repository:
tensor2tensor.

">
  <meta itemprop="image" content="/machine-translation/media/Transformer/image1.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="Transformers">
  <meta property="og:description" content="Transformer architecture is a novel architecture for encoder-decoder
paradigm created in an attempt to combine all good things from
Seq2Seq
architecture and
ConvS2S with
attention mechanisms. Transformer was proposed by a team from Google
Research and Google Brain in 2017 and published in a paper under the
name: “Attention is all you
need”. The official code for this
paper can be found on the Tensor2Tensor official GitHub repository:
tensor2tensor.

">
  <meta property="og:image" content="/machine-translation/media/Transformer/image1.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Transformers">
  <meta name="twitter:description" content="Transformer architecture is a novel architecture for encoder-decoder
paradigm created in an attempt to combine all good things from
Seq2Seq
architecture and
ConvS2S with
attention mechanisms. Transformer was proposed by a team from Google
Research and Google Brain in 2017 and published in a paper under the
name: “Attention is all you
need”. The official code for this
paper can be found on the Tensor2Tensor official GitHub repository:
tensor2tensor.

">
  
  <meta name="twitter:image" content="/machine-translation/media/Transformer/image1.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/machine-translation/Transformer">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          11 mins read
        </span>
      </p>
      <time datetime="2017-06-12 00:00" class="post-meta__body date">Published on arXiv on: 12 Jun 2017</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#Google Brain">Google Brain</a> & <a href="/labs/#Google Research">Google Research</a> & <a href="/labs/#University of Toronto">University of Toronto</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=Transformers> Transformers</h1>
    <p>Transformer architecture is a novel architecture for encoder-decoder
paradigm created in an attempt to combine all good things from
<a href="https://phanxuanphucnd.github.io/machine-translation/Seq2Seq">Seq2Seq</a>
architecture and
<a href="https://phanxuanphucnd.github.io/machine-translation/ConvS2S">ConvS2S</a> with
attention mechanisms. Transformer was proposed by a team from Google
Research and Google Brain in 2017 and published in a paper under the
name: “<a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is all you
need</a>”. The official code for this
paper can be found on the Tensor2Tensor official GitHub repository:
<a href="t https:/github.com/tensorflow/tensor2tensor">tensor2tensor</a>.</p>

<p>Transformer architecture deals with the input text data in an
encoder-decoder manner the same as Seq2Seq and tries to parallelize the
input data the same as ConvS2S. In this paper, the Transformer
architecture consists of six layers of encoder and six layers of decoder
as shown in the following figure:</p>

<div align="center">
    <img src="media/Transformer/image1.png" width="750" />
</div>

<h2 id="architecture">Architecture</h2>

<p>Most competitive machine translation models have an encoder-decoder
structure where the encoder maps an input sequence of symbol
representations $X = \left( x_{1},\ …x_{n} \right)$ to a sequence of
continuous representations $Z = \left( z_{1},\ …z_{n} \right)$. Given
$Z$, the decoder then generates an output sequence
$Y = \left( y_{1},\ …y_{m} \right)$ of symbols in an autoregressive
manner (one token at a time).</p>

<p>The most critical and influential part of the Transformer is the
attention mechanism which takes a quadratic time and space over the
input sequence which makes training Transformer takes longer time that
Seq2Seq and ConvS2S models. In this transformer architecture, there are
three different attention mechanisms used:</p>

<ul>
  <li>
    <p>Attention between the input tokens (self-attention).</p>
  </li>
  <li>
    <p>Attention between the output tokens (self-attention).</p>
  </li>
  <li>
    <p>Attention between the input and the output tokens</p>
  </li>
</ul>

<blockquote>
  <p><strong>Note:</strong><br />
The attention between the input (or output) tokens is called
<strong>self-attention</strong> because the attention is between the same parameters.</p>
</blockquote>

<h2 id="padding">Padding</h2>

<p>To be able to parallelize sentences with different lengths in
transformer, we need to define a value that represents the maximum
length (MAX_LENGTH) found in our training data. And all sentences whose
length is less than MAX_LENGTH should be padded using a PAD vector.</p>

<p>So, in the following image we have a mini-batch of three sentences where
the longest one is seven-tokens long. And the MAX_LENGTH is nine. In
practice, PAD is the $0^{th}$ index of the embedding matrix which means it
will be learnable vector. It’s learnable for convenience not because we
need it to be. Also, the PAD vector should be ignored when computing the
loss.</p>

<div align="center">
    <img src="media/Transformer/image2.png" width="550" />
</div>

<blockquote>
  <p><strong>Note:</strong>
In <a href="https://github.com/pytorch/fairseq">fairseq</a> framework, padding is done
randomly at either the beginning of the sentence or at the end. Also, the pad
token <code class="language-plaintext highlighter-rouge">&lt;p&gt;</code> has an index of <code class="language-plaintext highlighter-rouge">1</code> while index <code class="language-plaintext highlighter-rouge">0</code> is reserved for the beginning
of the sentence token <code class="language-plaintext highlighter-rouge">&lt;s&gt;</code>.</p>
</blockquote>

<h2 id="encoder">Encoder</h2>

<div align="center">
    <img src="media/Transformer/image3.png" width="350" />
</div>

<p>We are going to focus on the encoder part of the transformer architecture
which consists of different modules:</p>

<ul>
  <li>
    <p><strong>Embedding:</strong> where we map words into vectors representing their
meaning such that similar words will have similar vectors.
The embedding matrix have a size of $\mathbb{R}^{n \times d_m}$ where $n$
is the input length and $d$ is the embedding dimension.</p>
  </li>
  <li>
    <p><strong>Positional Encoding:</strong> Word meaning differs based on its position
in the sentence. A positional vector is a vector of the same size as
the embedding vector that gives context based on word-position in a
sentence. This can be done by applying following equation:</p>
  </li>
</ul>

\[\text{PE}_{\left( \text{pos},\ 2i \right)} = \sin\left( \frac{\text{pos}}{10000^{\frac{2i}{d_m}}} \right)
\ \ \ \ \ 
\text{PE}_{\left( \text{pos},\ 2i + i \right)} = \cos\left( \frac{\text{pos}}{10000^{\frac{2i}{d_m}}} \right)\]

<p>    Where $pos$ is the word position/index (starting from
zero). $i$ is the $i^{th}$ value of the word embedding and $d_m$ is the size of
the word embedding. So, if $i$ is even, then we are going to apply the first
equation; and if $i$ is odd, then we are going to apply the second
equation. After getting the <strong>positional vectors</strong>, we add them to the
original embedding vector to get context vector:</p>

<div align="center">
    <img src="media/Transformer/image4.png" width="750" />
</div>

<p>I know these functions don’t make sense and the original paper says the
following:</p>

<blockquote>
  <p>“We tried to encode position into word embedding using sinusoidal
functions and using learned positional embeddings, and we found that the two
versions produced nearly identical results.”</p>
</blockquote>

<p>But in case you wanted to dig deeper in this part, check this YouTube
<a href="https://www.youtube.com/watch?v=dichIcUZfOw">video</a>. It’s a good start. Also,
look into this <a href="https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3">article</a>.</p>

<ul>
  <li><strong>Single-Head Self-Attention:</strong><br />
Self-attention allows the encoder to associate each input word to
other words in the input. To achieve self-attention, we feed the
embedded input $X \in \mathbb{R}^{n \times d_{m}}$ into three
different linear fully-connected layers
$W^{Q},W^{K} \in \mathbb{R}^{d_{m} \times d_{k}},\ W^{V} \in \mathbb{R}^{d_{m} \times d_{v}}$
producing three different matrices respectively; which are <strong>query</strong>
$Q \in \mathbb{R}^{n \times d_{k}}$, <strong>key</strong>
$K \in \mathbb{R}^{n \times d_{k}}$, and <strong>value</strong>
$V \in \mathbb{R}^{n \times d_{v}}$.</li>
</ul>

\[Q = XW^{Q},\ \ \ \ K = XW^{K},\ \ \ \ V = XW^{V}\]

<div align="center">
    <img src="media/Transformer/image5.png" width="350" />
</div>

<p>   Now, the attention mechanism will attend the resulting three matrices
via the following equation:</p>

\[\text{Attention}\left( Q,\ K,\ V \right) = softmax\left( \frac{QK^{T}}{\sqrt{d_{k}}} \right)V\]

<p>   So, we are going to perform a dot product of Q and K to get a score
matrix that scores the relation between each word in the input and
the other words in the input as well.</p>

<div align="center">
    <img src="media/Transformer/image6.png" width="550" />
</div>

<p>   Then, these scores are getting scaled down by dividing over the
square root of the dimension of query and key (which is $d$) to
allow more stable gradients as the dot product could lead to
exploding values:</p>

<div align="center">
    <img src="media/Transformer/image7.png" width="250" />
</div>

<p>   Then, we are going to perform a Softmax over these down-scaled
scores to get the probability distribution which is called the
<u><strong>attention weights</strong></u>:</p>

<div align="center">
    <img src="media/Transformer/image8.png" width="350" />
</div>

<p>   Finally, we are going to perform a dot product between the attention
weights and the values V to get an output vector. The higher the attention
weight is, the higher it contributes to the output vector.</p>

<div align="center">
    <img src="media/Transformer/image9.png" width="350" />
</div>

<blockquote>
  <p><strong>Note:</strong>
The name of these three vectors comes from retrieval systems. So,
when you type a <u><strong>query</strong></u> on Google to search for, this query
will be mapped to a set of results <u><strong>keys</strong></u> to score each
result. And the highest results will be the <u><strong>values</strong></u> you
were looking for.</p>
</blockquote>

<ul>
  <li><strong>Multi-Head Self-Attention:</strong><br />
A multi-head self-attention is just performing the single-head
self-attention $h$ times and <u><strong>concatenating</strong></u> the
output matrices together before applying a linear layer
$W^{O} \in \mathbb{R}^{h d_v \times d_m}$ as shown in the following formula:</li>
</ul>

\[\text{MultiHead}\left( Q,\ K,\ V \right) = Concat\left( \text{head}_{1},...\text{head}_{h} \right)\ W^{O}\]

\[\text{head}_{i} = \text{Attention}\left( Q_i, K_i, V_i \right)\]

<p>     In theory, this will make each head learn something
different about the input. After concatenation, we apply a linear
fully-connected layer to match dimensions for the residual connection.
The following image shows the multi-head attention of just two heads $h=2$:</p>

<div align="center">
    <img src="media/Transformer/image10.png" width="550" />
</div>

<ul>
  <li><strong>Residual Connection &amp; Normalization</strong>:
After the multi-head self-attention, the positional input embedding
is added to the output vectors. This is known as a “residual
connection” which is mainly used to prevent gradient from vanishing.
After that, a layer normalization is applied:</li>
</ul>

<div align="center">
    <img src="media/Transformer/image11.png" width="350" />
    <!-- <img src="media/Transformer/image12.png" width=150> -->
</div>

\[x_{l + 1} = \text{LayerNorm}\left( x_{l} + F_{l}(x_{l}) \right)\]

<p>   Then, we apply a batch or layer normalization. The difference is
pretty subtle where the batch normalization normalizes over all data in the
batch and the layer normalization normalizes over all weights in the layer.</p>

<ul>
  <li><strong>Feed-forward</strong>:
Now, the normalization output gets fed to the feed-forward network
for further processing. The feed-forward network is just a couple of
linear layers with a $\text{ReLU}$ activation function in between.
The dimension of the feed-forward network is defined by the
$d_{\text{ff}}$ parameter where $W_1 \in \mathbb{R}^{d_{m} \times d_{ff}},
W_2 \in \mathbb{R}^{d_{ff} \times d_{m}}$ are the learnable weights and 
$b_1 \in \mathbb{R}^{d_{ff}}, b_2 \in \mathbb{R}^{d_{m}}$ are the
learnable biases.</li>
</ul>

\[\text{FFN}\left( x \right) = \text{ReLU}\left( xW_{1} + b_{1} \right)W_{2} + b_{2}\]

<div align="center">
    <img src="media/Transformer/image13.png" width="250" />
</div>

<h2 id="decoder">Decoder</h2>

<div align="center">
    <img src="media/Transformer/image14.png" width="450" />
</div>

<p>In this part, we are going to focus on the
decoder part of the transformer architecture. As we can see, it’s the
same components as the encoder except for two things:</p>

<ul>
  <li>
    <p><strong>Shifted-right inputs</strong>:
Input (sentence in another language) is shifted right by one word
while training because we want to make sure the encoder was able to
get this word before updating its value.</p>
  </li>
  <li>
    <p><strong>Masked Multi-Head self-Attention</strong>:
The masked multi-head is a little bit different than the
multi-head self-attention one. As the masked one will only be able
to access the previous words not the following one. In order to
solve that, we are going to create look-ahead mask matrix that masks
any further values with -inf.</p>
  </li>
</ul>

<div align="center">
    <img src="media/Transformer/image15.png" width="350" />
</div>

<p>   We are using -inf as a small numbers that will be zero
when applying the Softmax.</p>

<ul>
  <li><strong>Multi-head Attention #2:</strong>
The query and key of this block will come from the encoder output
and the values will be the output of the masked multi-head block.</li>
</ul>

<h2 id="training">Training</h2>

<p>Before training, sentences were encoded using byte-pair encoding with a
shared source-target vocabulary of about $37,000$ tokens for WMT
English-German and 32,000 for English-French.</p>

<p>For training, each training batch contained approximately $25,000$
tokens. They used Adam optimizer with
$\beta_{1} = 0.9,\ \beta_{2} = 0.98$ and $\epsilon = 10^{- 9}$. Learning
rate was varied over the course of training, according to the following
formula:</p>

\[lr = d_{\text{model}}^{- 0.5}.\min\left( {step\_ num}^{- 0.5},\ step\_ num*{warmup\_ steps}^{- 1.5} \right)\]

<p>This corresponds to increasing the learning rate linearly for the first
$\text{warmup_steps}$ training steps, and decreasing it thereafter
proportionally to the inverse square root of the $\text{step_num}$. They used
$\text{warmup_steps} = 4000$.</p>

<p>For regularization, they used dropout to the output of each sub-layer,
before it is added to the sub-layer input and normalized. In addition,
they applied dropout to the sums of the embeddings and the positional
encodings in both the encoder and decoder stacks. Also, label smoothing
$\epsilon_{l_{s}} = 0.1$ was used.</p>

<p>There were two variants of the Transformer configurations found in the
paper, Transformer-base and Transformer-big configurations which can be
summarized in the following table:</p>

<div align="center" class="inline-table">
<table>
    <thead>
        <tr>
            <th></th>
            <th>$$N$$</th>
            <th>$$d_{m}$$</th>
            <th>$$d_{\text{ff}}$$</th>
            <th>$$h$$</th>
            <th>$$d_{k}$$</th>
            <th>$$d_{v}$$</th>
            <th>$$P_{\text{dropout}}$$</th>
            <th>$$\epsilon_{l_{s}}$$</th>
            <th># parameters</th>
            <th>train steps</th>
        </tr>
    </thead>
    <tr>
        <td><strong>Base</strong></td>
        <td>6</td>
        <td>512</td>
        <td>2048</td>
        <td>8</td>
        <td>64</td>
        <td>64</td>
        <td>0.1</td>
        <td>0.1</td>
        <td>65 M</td>
        <td>100k</td>
    </tr>
    <tr>
        <td><strong>Large</strong></td>
        <td>6</td>
        <td>1024</td>
        <td>4096</td>
        <td>16</td>
        <td>64</td>
        <td>64</td>
        <td>0.3</td>
        <td>0.1</td>
        <td>213 M</td>
        <td>300k</td>
    </tr>
</table>
</div>

<p>For decoding, they used beam search with a $\text{beam size} = 4$ and length
penalty $\alpha = 0.6$.</p>

<p>The following table shows that the big transformer model achieves
state-of-the-art performance on the WMT 2014 English-German translation
task and English-French translation:</p>

<div align="center">
    <img src="media/Transformer/image20.png" width="750" />
</div>

<p>The base models score was obtained by averaging the last 5 checkpoints,
which were written at 10-minute intervals. For the big models, the last
20 checkpoints were averaged.</p>

<blockquote>
  <p><strong>Note:</strong><br />
For the English-French dataset, the big transformer used
$P_{\text{dropout}} = 0.1$, instead of $P_{\text{dropout}} = 0.3$.</p>
</blockquote>

<h2 id="layer-normalization">Layer Normalization</h2>

<p>Normalization is an important part of the Transformer architecture as it
improves the performance and avoids the overfitting. Here, we are going
to discuss the different layer normalization techniques that can be used
based as suggested by this paper: <a href="https://arxiv.org/pdf/1910.05895.pdf">Transformers without Tears: Improving
the Normalization of
Self-Attention</a> The official code
for this paper can be found in this GitHub repository:
<a href="https://github.com/tnq177/transformers_without_tears">transformers_without_tears</a>.</p>

<p>This paper compares between two different orders of layer normalization
in the Transformer architecture:</p>

<ul>
  <li><u><strong>Post-Norm:</strong></u><br />
Post-normalization is the default type of normalization used in the
standard Transformer architecture. It’s called that because it
occurs after the residual addition:</li>
</ul>

\[x_{l + 1} = \text{LayerNorm}\left( x_{l} + F_{l}(x_{l}) \right)\]

<ul>
  <li><u><strong>Pre-Norm:</strong></u><br />
Pre-normalization is applied immediately before the sublayer.
Pre-Norm enables warmup-free training providing greater stability
and doesn’t get affected by the weight initialization unlike
Post-Norm:</li>
</ul>

\[x_{l + 1} = x_{l} + F_{l}\left( \text{LayerNorm}(x_{l}) \right)\]

<blockquote>
  <p><u><strong>Very Important Note:</strong></u><br />
In the paper, they found out that post-normalization works best
with high-resource languages while pre-normalization works best with
low-resource languages.</p>
</blockquote>

<p>Also, in the paper, they proposed an alternative to the layer
normalization:</p>

<ul>
  <li><u><strong>Scale-Norm:</strong></u>
Scale-Norm is an alternative for layer normalization. As we can see
from the following equation, Scale-Norm replaced the two learnable
parameters $\gamma,\ \beta$ in layer normalization with one global
learned scalar $g$:</li>
</ul>

\[\text{ScaleNorm}\left( x;g \right) = g\frac{x}{\left\| x \right\|}\]

<ul>
  <li><u><strong>Scale-Norm + Fix-Norm:</strong></u>
Fix-Norm is applied to the word embeddings. It looks exactly like
Scale-Norm with only one global learnable scalar g, so we can apply
both of them jointly like so:</li>
</ul>

\[ScaleNorm + FixNorm\left( x,w;g \right) = g\frac{\text{w.x}}{\left\| w \right\|.\left\| x \right\|}\]

<p>And the following are the results published in the paper on various
machine translation directions:</p>

<div align="center">
    <img src="media/Transformer/image16.png" width="750" />
</div>

<h2 id="layerdrop">LayerDrop</h2>

<p>LayerDrop is a novel regularization method for Transformers used to
prevent them from overfitting. This method was proposed by Facebook AI
in 2019 and published in this paper: <a href="https://arxiv.org/pdf/1909.11556.pdf">Reducing Transformer Depth On
Demand With Structured Dropout</a>.
The official code for this paper can be found in the official Fairseq
GitHub repository:
<a href="https://github.com/pytorch/fairseq/tree/main/examples/layerdrop">fairseq/layerdrop</a>.</p>

<p><a href="https://arxiv.org/pdf/1603.09382.pdf">Deep Networks with Stochastic
Depth</a> paper has shown that
dropping layers during training can regularize and reduce the training
time of very deep convolutional networks. And this is the core idea of
LayerDrop where entire layers are randomly dropped at training time
which regularizes very deep Transformers and stabilizes their training,
leading to better performance.</p>

<p>The following figure shows a comparison of a 9-layer transformer trained
with LayerDrop (right) and 3 transformers of different sizes (left). As
we can see, the different pruned version of the transformer on the right
obtained better results than the same sized transformers trained from
scratch:</p>

<div align="center">
    <img src="media/Transformer/image17.png" width="750" />
</div>

<p>Which shows that LayerDrop also acts like a distillation technique that
can lead to small and efficient transformers of any depth which can be
extracted automatically at test time from a single large pre-trained
model, without the need for finetuning.</p>

<p>LayerDrop does not explicitly provide a way to select which group of
layers will be dropped. So, the publishers considered several different
pruning strategies:</p>

<ul>
  <li>
    <p><u><strong>Every Other:</strong></u>
A straightforward strategy is to simply drop every other layer.
Pruning with a drop rate $p$ means dropping the layers at a depth
$d$ such that
$d \equiv 0\left( \text{mod}\left\lfloor \frac{1}{p} \right\rfloor \right)$.
This strategy is intuitive and leads to balanced networks.</p>
  </li>
  <li>
    <p><u><strong>Search on Valid:</strong></u>
Another possibility is to compute various combinations of layers to
form shallower networks using the validation set, then select the
best performing for test. This is straightforward but
computationally intensive and can lead to overfitting on validation.</p>
  </li>
  <li>
    <p><u><strong>Data Driven Pruning:</strong></u>
Another approach is to learn the drop rate of each layer. Given a
target drop rate $p$, we learn an individual drop rate $p_{d}$ for
the layer at depth $d$ such that the average rate over layers is
equal to $p$.</p>

    <p>The “<strong>Every Other</strong>” works the best with the following drop rate
$p$ where $N$ is the number of layers, $r$ is the target pruned
size:</p>
  </li>
</ul>

\[p = 1 - \frac{r}{N}\]

<blockquote>
  <p><strong>Note:</strong><br />
In the paper, they used a LayerDrop rate of $p = 0.2$ for all their
experiments. However, they recommend using $p = 0.5$ to obtain very
small models at inference time.</p>
</blockquote>

<h2 id="drophead">DropHead</h2>

<p>DropHead is another novel regularization method for Transformers used to
prevent overfitting. This method was proposed by Microsoft Research Asia
in 2020 and published in this paper: <a href="https://arxiv.org/pdf/2004.13342.pdf">Scheduled DropHead: A
Regularization Method for Transformer
Models</a>. There is unofficial
implementation for this paper, it can be found in this GitHub
repository:
<a href="https://github.com/Kirill-Kravtsov/drophead-pytorch">drophead-pytorch</a>.</p>

<p>In the core, DropHead drops entire attention heads during training to
prevent the multi-head attention model from being dominated by a small
portion of attention heads which can help reduce the risk of overfitting
and allow the models to better benefit from the multi-head attention.
The following figure shows the difference between dropout (left) and
DropHead (right):</p>

<div align="center">
    <img src="media/Transformer/image18.png" width="750" />
</div>

<p>In the paper, they proposed a specific dropout rate scheduler for the
DropHead mechanism, which looks like a V-shaped curve (green curve
below): It applies a relatively high dropout rate of $p_{\text{start}}$
and linearly decrease it to $0$ during the early stage of training,
which is empirically chosen to be the same training steps for learning
rate warmup. Afterwards, it linearly increases the dropout rate to
$p_{\text{end}}$. To avoid introducing additional hyper-parameters, they
decided to set $p_{\text{start}} = p_{\text{end}}$.</p>

<div align="center">
    <img src="media/Transformer/image19.png" width="750" />
</div>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/machine-translation/Transformer';
      this.page.identifier = '/machine-translation/Transformer';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>