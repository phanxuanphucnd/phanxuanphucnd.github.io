<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>REDER</title>
  <meta name="title" content="REDER">
  <meta name="description" content="REDER stands for “Reversible Duplex Transformer” which is a
Transformer
model where its both ends can simultaneously input and output a distinct
language thus enabling reversible machine translation by simply flipping
the input and output ends. REDER was proposed by ByteDance AI lab in
2022 and published in their paper: Duplex Sequence-to-Sequence Learning
for Reversible Machine
Translation. The official code
for this paper can be found in the following GitHub repository:
REDER.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="REDER">
  <meta itemprop="description" content="REDER stands for “Reversible Duplex Transformer” which is a
Transformer
model where its both ends can simultaneously input and output a distinct
language thus enabling reversible machine translation by simply flipping
the input and output ends. REDER was proposed by ByteDance AI lab in
2022 and published in their paper: Duplex Sequence-to-Sequence Learning
for Reversible Machine
Translation. The official code
for this paper can be found in the following GitHub repository:
REDER.

">
  <meta itemprop="image" content="/machine-translation/media/REDER/image0.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="REDER">
  <meta property="og:description" content="REDER stands for “Reversible Duplex Transformer” which is a
Transformer
model where its both ends can simultaneously input and output a distinct
language thus enabling reversible machine translation by simply flipping
the input and output ends. REDER was proposed by ByteDance AI lab in
2022 and published in their paper: Duplex Sequence-to-Sequence Learning
for Reversible Machine
Translation. The official code
for this paper can be found in the following GitHub repository:
REDER.

">
  <meta property="og:image" content="/machine-translation/media/REDER/image0.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="REDER">
  <meta name="twitter:description" content="REDER stands for “Reversible Duplex Transformer” which is a
Transformer
model where its both ends can simultaneously input and output a distinct
language thus enabling reversible machine translation by simply flipping
the input and output ends. REDER was proposed by ByteDance AI lab in
2022 and published in their paper: Duplex Sequence-to-Sequence Learning
for Reversible Machine
Translation. The official code
for this paper can be found in the following GitHub repository:
REDER.

">
  
  <meta name="twitter:image" content="/machine-translation/media/REDER/image0.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/machine-translation/REDER">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          6 mins read
        </span>
      </p>
      <time datetime="2021-05-07 00:00" class="post-meta__body date">Published on arXiv on: 7 May 2021</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#ByteDance AI Lab">ByteDance AI Lab</a> & <a href="/labs/#Nanjing University">Nanjing University</a> & <a href="/labs/#UC Santa Barbara">UC Santa Barbara</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=REDER> REDER</h1>
    <p>REDER stands for “<strong>Re</strong>versible <strong>D</strong>uplex Transform<strong>er</strong>” which is a
<a href="https://phanxuanphucnd.github.io/machine-translation/Transformer">Transformer</a>
model where its both ends can simultaneously input and output a distinct
language thus enabling reversible machine translation by simply flipping
the input and output ends. REDER was proposed by ByteDance AI lab in
2022 and published in their paper: <a href="https://arxiv.org/pdf/2105.03458.pdf">Duplex Sequence-to-Sequence Learning
for Reversible Machine
Translation</a>. The official code
for this paper can be found in the following GitHub repository:
<a href="https://github.com/zhengzx-nlp/REDER">REDER</a>.</p>

<div align="center">
    <img src="media/REDER/image1.png" width="750" />
</div>

<p>In this paper, they proposed an alternative approach for utilizing
bidirectional supervisions called <u><strong>duplex sequence-to-sequence</strong></u>
learning which is better than the typical sequence-to-sequence learning
where the encoder learns to represent the source language and the
decoder learns to generate the target language, and multi-task learning
where the encoder learns to represent multiple language simultaneously
while the decoder learns to generate multiple language simultaneously.</p>

<div align="center">
    <img src="media/REDER/image2.png" width="750" />
</div>

<blockquote>
  <p><strong>Note:</strong><br />
The name “Duplex” comes from telecommunications and computer networking
where the “simplex” communication means the communication channel is
unidirectional while the “duplex” communication is bidirectional.</p>
</blockquote>

<p>We should note that building duplex seq2seq networks is non-trivial; and
that’s because of the following reasons:</p>

<ul>
  <li>
    <p><u><strong>Reversibility:</strong></u><br />
Typical encoder-decoder networks such as Transformers are
irreversible, i.e., one cannot just obtain its inverse function by
flipping the same encoder-decoder network.</p>
  </li>
  <li>
    <p><u><strong>Homogeneity:</strong></u><br />
The decoder works autoregressively, while the encoder works in a
non-autoregressive manner.</p>
  </li>
</ul>

<p>That’s why REDER is designed without explicit encoder and decoder division
introducing two solutions (<strong>Reversible Duplex Transformer</strong> layers and
the <strong>Symmetric Network</strong> architecture) to address the reversibility and
homogeneity issues respectively as we are going to see next.</p>

<h2 id="reversible-duplex-transformer-layer">Reversible Duplex Transformer Layer</h2>

<p>The following figure shows the overall architecture of REDER where we
can see that it has two ends: the source end (left) and the target end
(right). As illustrated, REDER is composed of a series of identical
Reversible Duplex Transformer layers. Since the network has two ends, it
defines a forward mapping function
$f_{\theta}^{\rightarrow}:X \rightarrow Y$ and a reverse mapping
function $f_{\theta}^{\leftarrow}:Y \rightarrow X$ that satisfy the
following reversibility:
$f_{\theta}^{\rightarrow} = \left( f_{\theta}^{\leftarrow} \right)^{- 1}$
and
$f_{\theta}^{\leftarrow} = \left( f_{\theta}^{\rightarrow} \right)^{- 1}$.</p>

<div align="center">
    <img src="media/REDER/image3.png" width="750" />
</div>

<p>The Reversible Duplex Transformer layer is adopted from the Reversible
Networks (RevNet) where each layer is composed of two sub-layers: a
multi-head self-attention (SAN) and a feed-forward network (FFN) with a
special reversible design to ensure a duplex behavior.</p>

<p>Given a sentence pair $\left( x,y \right)$, the inner representations of
each layer in divided into two directions:</p>

<ul>
  <li>The forward direction $\overrightarrow{H}_l$ which is the
concatenation of two halves
$\left\lbrack \overrightarrow{H}_l^{\left( 1 \right)},\ \overrightarrow{H}_l^{\left( 2 \right)} \right\rbrack$:</li>
</ul>

\[{\overrightarrow{H}}_{l} = F_{l}\left( \left\lbrack {\overrightarrow{H}}_{l - 1}^{\left( 1 \right)},\ {\overrightarrow{H}}_{l - 1}^{\left( 2 \right)} \right\rbrack \right) = \left\lbrack {\overrightarrow{H}}_{l}^{\left( 1 \right)},\ {\overrightarrow{H}}_{l}^{\left( 2 \right)} \right\rbrack\]

\[{\overrightarrow{H}}_{l}^{\left( 1 \right)} = {\overrightarrow{H}}_{l - 1}^{\left( 1 \right)} + \text{SAN}\left( {\overrightarrow{H}}_{l - 1}^{\left( 2 \right)} \right)\]

\[{\overrightarrow{H}}_{l}^{\left( 2 \right)} = {\overrightarrow{H}}_{l - 1}^{\left( 2 \right)} + \text{FFN}\left( {\overrightarrow{H}}_{l}^{\left( 1 \right)} \right)\]

<ul>
  <li>The backward direction $\overleftarrow{H}_l$ which is the
concatenation of two halves
$\left\lbrack \overleftarrow{H}_l^{\left( 1 \right)},\ \overrightarrow{H}_l^{\left( 2 \right)} \right\rbrack$:</li>
</ul>

\[{\overleftarrow{H}}_{l - 1} = F_{l}^{- 1}\left( \left\lbrack {\overleftarrow{H}}_{l}^{\left( 1 \right)},\ {\overleftarrow{H}}_{l}^{\left( 2 \right)} \right\rbrack \right) = \left\lbrack {\overleftarrow{H}}_{l - 1}^{\left( 1 \right)},\ {\overleftarrow{H}}_{l - 1}^{\left( 2 \right)} \right\rbrack\]

\[{\overleftarrow{H}}_{l - 1}^{\left( 2 \right)} = {\overleftarrow{H}}_{l}^{\left( 2 \right)} + FFN\left( {\overleftarrow{H}}_{l}^{\left( 1 \right)} \right)\]

\[{\overleftarrow{H}}_{l - 1}^{\left( 1 \right)} = {\overleftarrow{H}}_{l}^{\left( 1 \right)} + \text{SAN}\left( {\overleftarrow{H}}_{l - 1}^{\left( 2 \right)} \right)\]

<blockquote>
  <p><strong>Note:</strong><br />
The attention mechanism used in this paper is a relative self-attention
proposed in the “<a href="https://www.aclweb.org/anthology/N18-2074">Self-attention with relative position
representations</a>” paper,
instead of the original one proposed in the Transformer paper.</p>
</blockquote>

<h2 id="symmetric-network">Symmetric Network</h2>

<p>As discussed earlier, using reversible duplex transformer layers solves
the reversibility problem. Now, let’s see how to solve the Homogeneity
problem. To achieve homogeneous computations, the model must satisfy the
following cycle consistency:</p>

\[\forall x \in X:f_{\theta}^{\leftarrow}\left( f_{\theta}^{\rightarrow}\left( x \right) \right) = x,\ \ \ \ \ \ \ \ \forall y \in Y:f_{\theta}^{\rightarrow}\left( f_{\theta}^{\leftarrow}\left( y \right) \right) = y\]

<p>One solution is to make the network symmetric which can be done as
follows: given an network of $L$ layers, the layers starting from $1$
till $\frac{L}{2}$ will be in reverse form, whereas the layers starting
from $\frac{L}{2} + 1$ to $L$ be in regular form:</p>

\[f_{\theta}^{\rightarrow}\left( x \right) \triangleq F_{1}^{- 1} \circ \text{...} \circ F_{\frac{L}{2}}^{- 1} \circ F_{\frac{L}{2} + 1} \circ \text{...} \circ F_{L}\left( x \right)\]

\[f_{\theta}^{\leftarrow}\left( y \right) \triangleq F_{L} \circ \text{...} \circ F_{\frac{L}{2} + 1} \circ F_{\frac{L}{2}}^{- 1} \circ \text{...} \circ F_{1}^{- 1}\left( y \right)\]

<p>And this property means that the REDER model works in a fully
non-autoregressive fashion in both reading and generating sequences.
Specifically, given an input sequence
$x = \left( x_{1},\ …x_{n} \right)$, the i-th element of REDER’s input
$x_{i}$ is the concatenation of two copies of the embedding of $x_{i}$
as shown below:</p>

\[{\overrightarrow{H}}_{0,i} = \left\lbrack {\overrightarrow{H}}_{0,i}^{\left( 1 \right)},\ {\overrightarrow{H}}_{0,i}^{\left( 2 \right)} \right\rbrack = \left\lbrack e\left( x_{i} \right),\ e\left( x_{i} \right) \right\rbrack\]

<p>Once the forward computation is done, the concatenation of the output of
the model <span>
$\left\lbrack {\overrightarrow{H}}_{L,i}^{\left( 1 \right)},\ {\overrightarrow{H}}_{L,i}^{\left( 2 \right)} \right\rbrack$</span>
serves as the representations of target translation after a softmax
operation is performed to measure the similarity between itself and the
concatenated embedding of ground-truth reference
$\left\lbrack e\left( y_{i} \right),\ e\left( y_{i} \right) \right\rbrack$
to obtain the prediction probability:</p>

\[p\left( y_{i} \middle| x;\ \theta \right) = \text{softmax}\left( \frac{1}{2}{\left\lbrack e\left( y_{i} \right),\ e\left( y_{i} \right) \right\rbrack\ }^{T}.\left\lbrack {\overrightarrow{H}}_{L,i}^{\left( 1 \right)},\ {\overrightarrow{H}}_{L,i}^{\left( 2 \right)} \right\rbrack \right)\]

<h2 id="objective-function">Objective Function</h2>

<p>Given a parallel dataset<span>
$\mathcal{D}_{x,y} = \left( x^{\left( n \right)},\ y^{\left( n \right)} \right)_{n = 1}^{N}$
</span>,the final objective of REDER is to minimize the following objective
function:</p>

\[\mathcal{L}\left( \theta;\mathcal{D}_{x,y} \right) = \sum_{n = 1}^{N}\left( - \log\left( p_{\text{ctc}}\left( y^{\left( n \right)} \middle| x^{\left( n \right)};\theta \right) \right) - log\left( p_{\text{ctc}}\left( x^{\left( n \right)} \middle| y^{\left( n \right)};\theta \right) \right) + \lambda_{\text{fba}}\mathcal{L}_{\text{fba}}\left( x^{\left( n \right)} \middle| y^{\left( n \right)};\theta \right) + \lambda_{\text{fba}}\mathcal{L}_{\text{fba}}\left( y^{\left( n \right)} \middle| x^{\left( n \right)};\theta \right) + \lambda_{\text{cc}}\mathcal{L}_{\text{cc}}\left( x^{\left( n \right)};\theta \right) + \lambda_{\text{cc}}\mathcal{L}_{\text{cc}}\left( y^{\left( n \right)};\theta \right) \right)\]

<p>Where:</p>

<ul>
  <li>
    <p>$p_{\text{ctc}}\left( y^{\left( n \right)} \middle| x^{\left( n \right)};\theta \right)$
is the CTC loss of the forward direction, while
$p_{\text{ctc}}\left( x^{\left( n \right)} \middle| y^{\left( n \right)};\theta \right)$
is the CTC loss of the backward direction.</p>
  </li>
  <li>
    <p>$\mathcal{L}_{\text{fba}}\left( x^{\left( n \right)} \middle| y^{\left( n \right)};\theta \right)$
is the forward-backward agreement of the forward direction, while
$\mathcal{L}_{\text{fba}}\left( y^{\left( n \right)} \middle| x^{\left( n \right)};\theta \right)$
is the forward-backward agreement of the backward direction. This
loss function is calculated via the following formula knowing that
$sg()$ is the stop-gradient operation:</p>
  </li>
</ul>

\[\mathcal{L}_{\text{fba}}\left( x \middl| y;\theta \right) = \frac{1}{L}\sum_{l = 1}^{L}{1 - \cos\left( {\overrightarrow{H}}_{l},\ \text{sg}\left( {\overleftarrow{H}}_{l} \right) \right)}\]

<ul>
  <li>$\mathcal{L}_{\text{cc}}\left( x^{\left( n \right)};\theta \right)$
is the cycle consistency loss of the forward direction, while
$\mathcal{L}_{\text{cc}}\left( y^{\left( n \right)};\theta \right)$
is the cycle consistency of the backward direction. This loss
function is calculated via the following formula:</li>
</ul>

\[\mathcal{L}_{\text{cc}}\left( x;\theta \right) = distance\left( x,\ f_{\theta}^{\leftarrow}\left( f_{\theta}^{\rightarrow}\left( x \right) \right) \right)\]

<ul>
  <li>$\lambda_{\text{fba}}$ and $\lambda_{\text{cc}}$ are the
coefficients of the auxiliary losses.</li>
</ul>

<blockquote>
  <p><strong>Note:</strong><br />
Training REDER is done in a a two-stage training setting, where we first train
REDER without using any auxiliary losses until a predefined number of updates,
and then activate the additional losses and continue training the model until
convergence.</p>
</blockquote>

<h2 id="experiments">Experiments</h2>

<p>In the paper, they evaluated REDER on two standard translation
benchmarks: WMT14 English↔German (4.5M training pairs), and WMT16
English↔Romanian (610K training pairs). REDER was implemented using
<a href="https://github.com/pytorch/fairseq">fairseq</a> framework and it consists
of $12$ stacked layers. The number of head is $8$, the model dimension
is $512$, and the FFN is $2048$. For regularization, they used a dropout
rate of $0.1$ and L2-regularization of $0.01$ and label smoothing with
$\epsilon = 0.1$. Also, they both $\lambda_{\text{fba}}$ and
$\lambda_{\text{cc}}$ to $0.1$ for all experiments. All models were
trained for 300K updates with a batch size of approximately 64K tokens.
For scoring, they average the best 5 checkpoints to obtain the final
model.</p>

<p>The following table shows the a comparison between REDER and other
existing models knowing that all NAT models were trained with Knowledge
Distillation (KD), also “MTL” stands for “multitask learning” and “BT”
stands for “back-translation”. (row 4 ∼ row 9) employ greedy decoding
while (row 10 ∼ row 14) employ beam search decoding with beam size of
20.</p>

<div align="center">
    <img src="media/REDER/image4.png" width="750" />
</div>

<p>From the previous table we can see:</p>

<ul>
  <li>
    <p>REDER achieves competitive results compared with strong NAT
baselines.</p>
  </li>
  <li>
    <p>Duplex learning has more potential than multitask learning and
back-translation.</p>
  </li>
  <li>
    <p>REDER performs on par with autoregressive Transformer</p>
  </li>
</ul>

<p>Like other NAT approaches, they found out that REDER heavily relies
on knowledge distillation. The following table shows that the
accuracy of REDER without KD significantly drops:</p>

<div align="center">
    <img src="media/REDER/image5.png" width="750" />
</div>

<p>To examine whether REDER can generalize to distant languages, they
conducted experiments on WMT20 English↔Japanese dataset. The
following table shows that REDER can achieve very close results
compared with Auto-regressive Transformer (AT) in such a large-scale
scenario with distant languages:</p>

<div align="center">
    <img src="media/REDER/image6.png" width="350" />
</div>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/machine-translation/REDER';
      this.page.identifier = '/machine-translation/REDER';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>