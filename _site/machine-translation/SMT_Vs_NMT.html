<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>SMT Vs NMT</title>
  <meta name="title" content="SMT Vs NMT">
  <meta name="description" content="Although the NMT had made remarkable achievements on particular
translation experiments, researchers were wondering if the good
performance persists on other tasks and can NMT indeed replace SMT.
Accordingly, Junczys-Dowmunt et
al. who
performed experiments on the “United Nations Parallel Corpus” which
involves 15 language pairs and 30 translation directions, and NMT was
either on par with or surpassed SMT across all 30 translation directions
in the experiment measured through BLEU scores which proves how
promising NMT is.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="SMT Vs NMT">
  <meta itemprop="description" content="Although the NMT had made remarkable achievements on particular
translation experiments, researchers were wondering if the good
performance persists on other tasks and can NMT indeed replace SMT.
Accordingly, Junczys-Dowmunt et
al. who
performed experiments on the “United Nations Parallel Corpus” which
involves 15 language pairs and 30 translation directions, and NMT was
either on par with or surpassed SMT across all 30 translation directions
in the experiment measured through BLEU scores which proves how
promising NMT is.

">
  <meta itemprop="image" content="/machine-translation/media/SMT_Vs_NMT/image4.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="SMT Vs NMT">
  <meta property="og:description" content="Although the NMT had made remarkable achievements on particular
translation experiments, researchers were wondering if the good
performance persists on other tasks and can NMT indeed replace SMT.
Accordingly, Junczys-Dowmunt et
al. who
performed experiments on the “United Nations Parallel Corpus” which
involves 15 language pairs and 30 translation directions, and NMT was
either on par with or surpassed SMT across all 30 translation directions
in the experiment measured through BLEU scores which proves how
promising NMT is.

">
  <meta property="og:image" content="/machine-translation/media/SMT_Vs_NMT/image4.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="SMT Vs NMT">
  <meta name="twitter:description" content="Although the NMT had made remarkable achievements on particular
translation experiments, researchers were wondering if the good
performance persists on other tasks and can NMT indeed replace SMT.
Accordingly, Junczys-Dowmunt et
al. who
performed experiments on the “United Nations Parallel Corpus” which
involves 15 language pairs and 30 translation directions, and NMT was
either on par with or surpassed SMT across all 30 translation directions
in the experiment measured through BLEU scores which proves how
promising NMT is.

">
  
  <meta name="twitter:image" content="/machine-translation/media/SMT_Vs_NMT/image4.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/machine-translation/SMT_Vs_NMT">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          7 mins read
        </span>
      </p>
      <time datetime="2017-06-12 00:00" class="post-meta__body date">Published on arXiv on: 12 Jun 2017</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#Johns Hopkins University">Johns Hopkins University</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=SMT Vs NMT> SMT Vs NMT</h1>
    <p>Although the NMT had made remarkable achievements on particular
translation experiments, researchers were wondering if the good
performance persists on other tasks and can NMT indeed replace SMT.
Accordingly, <a href="http://workshop2016.iwslt.org/downloads/IWSLT_2016_paper_4.pdf">Junczys-Dowmunt et
al.</a> who
performed experiments on the “United Nations Parallel Corpus” which
involves 15 language pairs and 30 translation directions, and NMT was
either on par with or surpassed SMT across all 30 translation directions
in the experiment measured through BLEU scores which proves how
promising NMT is.</p>

<p>When comparing NMT with SMT, we can see that NMT has some pros like:</p>

<ul>
  <li>
    <p>NMT does not need prior domain knowledge with translation, which
enables zero-shot translation.</p>
  </li>
  <li>
    <p>NMT produces more fluent results.</p>
  </li>
  <li>
    <p>NMT produces more phrase-similarities between different languages.</p>
  </li>
  <li>
    <p>NMT is language-agnostic. You can use it basically for any language.</p>
  </li>
</ul>

<p>On the other side, there are still problems and challenges of NMT need
to be tackled:</p>

<ul>
  <li>
    <p>The training and decoding process is quite slow.</p>
  </li>
  <li>
    <p>The style of translation can be inconsistent for the same word.</p>
  </li>
  <li>
    <p>There exists an “out-of-vocabulary” problem on the translation
results.</p>
  </li>
  <li>
    <p>The “black-box” neural network mechanism leads to poor
interpretability; thus the parameters for training are mostly picked
based on experience.</p>
  </li>
  <li>
    <p>Biases is very clear in NMT:</p>
  </li>
</ul>

<div align="center">
    <img src="media/SMT_Vs_NMT/image1.png" width="750" />
</div>

<p>Because of the characteristics of NMT and its superiority over SMT, NMT
also starts to be adopted by the industry recently:</p>

<ul>
  <li>
    <p>In September 2016, the Google Translate team published a blog
showing that they had started using NMT to replace Phrase-Based
Machine Translation for Chinese-English translations on their
product. The NMT they deployed is named Google Neural Machine
Translation (GNMT), and a paper: “<a href="https://arxiv.org/pdf/1609.08144.pdf">Google’s Neural Machine
Translation System: Bridging the Gap between Human and Machine
Translation</a>” was published at
the same time to explain that model in details.</p>
  </li>
  <li>
    <p>In 2017, Facebook AI Research (FAIR) announced their way of
implementing NMT with CNN, which can achieve a similar performance
as the RNN-based NMT while running nine times faster.</p>
  </li>
  <li>
    <p>In June 2017, Google released a solely attention-based NMT model
which used neither CNN nor RNN and purely based on the “attention”
mechanism under the name “<a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All you
need</a>”.</p>
  </li>
  <li>
    <p>In July 2017, Amazon released their NMT implementation with MXNet.</p>
  </li>
  <li>
    <p>Microsoft talked about their usage of NMT in 2016, although not
revealed any further technical details yet.</p>
  </li>
</ul>

<p>In June 2017, two researchers from John Hopkins University published a
paper under the name: <a href="https://arxiv.org/pdf/1706.03872.pdf">Six Challenges for Neural Machine
Translation</a> which discusses the
most urging problems in Neural Machine Translation systems in comparison
with Statistical ones. And the found out that all problems can be
summarized in six challenges as mentioned below; knowing that all NMT
models used in this paper were trained using <a href="https://github.com/rsennrich/nematus">Nematus
toolkit</a> and all SMT models were
trained using <a href="http://www.stat.org/moses/">Moses toolkit</a>:</p>

<h2 id="training-data-size">Training Data Size</h2>

<p>A well-known property of SMT systems is that increasing amounts of
training data lead to better results. To measure the effect of the
training data size on NMT systems in the paper, they built
English-Spanish systems on <a href="http://statmt.org/wmt13/translation-task.html">WMT13
data</a> of about 385.7
million English words paired with Spanish. To measure the effect of
different training dataset size, they split the data in the following
ratios: $\frac{1}{1024},\ \frac{1}{512},\ …\frac{1}{2},\ 1$ and
measured the models performance at each data size as shown in the
following figure:</p>

<div align="center">
    <img src="media/SMT_Vs_NMT/image2.png" width="450" />
</div>

<p>As shown, NMT exhibits a much steeper learning curve starting with very
bad results and ending with outperforming SMT, and even beating the SMT
system with a big language model. The contrast between the NMT and SMT
learning curves is quite striking which shows that <u><strong>NMT systems are
superior when having enough data; while SMT systems are superior with
small amount of training data</strong></u>.</p>

<h2 id="domain-mismatch">Domain Mismatch</h2>

<p>Domain in machine translation is defined by information from a specific
source which could be different from other domains in <u><strong>topic</strong></u>,
<u><strong>style</strong></u>, <u><strong>level of formality</strong></u>,
<u><strong>complexity</strong></u> …etc. Same words in different domains have
different translations and expressed in different styles. To measure how well
Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) hold
up with domain mismatch, they trained five different systems using five
different corpora from different domains as shown below:</p>

<div align="center">
    <img src="media/SMT_Vs_NMT/image3.png" width="750" />
</div>

<p>They used German-English pairs with test sets sub-sampled from the data.
A common byte-pair encoding (BPE) is used for all training runs. The
following figure shows a comparison between NMT and SMT systems BLEU
scores when trained on the corpora at the row and tested on the corpora
at the column:</p>

<div align="center">
    <img src="media/SMT_Vs_NMT/image4.png" width="750" />
</div>

<p>The previous figure shows that <u><strong>in-domain NMT and SMT systems are
similar (NMT is better for IT and Subtitles, SMT is better for Law,
Medical, and Koran), the out-of-domain performance for the NMT systems
is worse in almost all cases.</strong></u></p>

<blockquote>
  <p><strong>Note:</strong><br />
A currently popular approach for domain adaptation is to train a general
domain system, followed by training on in-domain data for a few epochs.</p>
</blockquote>

<h2 id="rare-words">Rare Words</h2>

<p>The next problem that we will discuss is the effect of rare words on NMT
and SMT systems. To measure that effect, they used pre-trained NMT and
SMT systems that have the same case-sensitive BLEU score of 34.5 on the
WMT 2016 English-German news test set (for the NMT model, this reflects
the BLEU score resulting from translation with a beam size of 1).</p>

<p>Then, they followed the approach described in this paper: <a href="https://aclanthology.org/2012.amta-papers.9.pdf">Interpolated
backoff for factored translation models
</a> for examining the effect
of source word frequency on translation accuracy. This approach can be
summarized in the following steps:</p>

<ul>
  <li>
    <p>First, they align the source sentence and the machine translation
output from each system using <a href="https://github.com/clab/fast_align">fast-align
toolkit</a> with “gdfa”
(grow-diag-final-and).</p>
  </li>
  <li>
    <p>Now, each source word is either unaligned (“dropped”) or aligned.
For each target word that was aligned, they checked if that word
appears in the reference translation or not.</p>

    <ul>
      <li>
        <p>If the target word appears the same number of times in the  hypothesis
as in the reference, they awarded the system a score of one.</p>
      </li>
      <li>
        <p>If the target word appears more times in the hypothesis than in  the
reference, they awarded the system a fractional credit the equals to
$\frac{\text{reference_count}}{\text{hypothesis_count}}$.</p>
      </li>
      <li>
        <p>If the target word does not appear in the reference, they didn’t  award
the system.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Then, all scores over the full set of target words is averaged to compute
the precision for that source word.</p>
  </li>
  <li>
    <p>Finally, words are binned by frequency and average translation
precisions can be computed.</p>
  </li>
</ul>

<p>The following figure shows the words-frequency bins on the x-axis
and the average precision on the y-axis. The values above the
horizontal axis represent precisions, while the lower portion
represents what proportion of the words were dropped/deleted.</p>

<div align="center">
    <img src="media/SMT_Vs_NMT/image5.png" width="750" />
</div>

<p>From the previous figure, the first item of note is that the NMT system
has an overall higher proportion of deleted words. Of the 64379 words
examined, the NMT system deleted 3769 while the SMT system deleted 2274.
The next interesting observation is what happens with unknown words
(left-most side of the figure). The SMT system translates these
correctly 53.2% of the time, while the NMT system translates them
correctly 60.1% of the time.</p>

<p>To sum up, <u><strong>Surprisingly, NMT systems (at least those using byte-pair
encoding) is better than SMT systems on translation very infrequent
words. However, both NMT and SMT systems do continue to have difficulty
translating some infrequent words, particularly those belonging to
highly-inflected categories.</strong></u></p>

<h2 id="long-sentences">Long Sentences</h2>

<p>A well-known flaw of early encoder-decoder NMT models was the inability
to properly translate long sentences. However, this was fixed by using
the <a href="https://phanxuanphucnd.github.io/machine-translation/Attention">attention
mechanism</a>. To
measure the effect of long sentences on NMT (with attention mechanism)
and SMT systems, they used the large English-Spanish dataset (from
before) and broke it up into buckets based on sentence length (1-9
subword tokens, 10-19 subword tokens, ... etc.) and computed
corpus-level BLEU scores for each bucket.</p>

<p>The following figure shows that <u><strong>NMT is better than SMT in general,
but SMT system outperforms NMT on sentences of length 60 and
higher</strong></u>:</p>

<div align="center">
    <img src="media/SMT_Vs_NMT/image6.png" width="750" />
</div>

<p>The quality for the two systems is relatively close, except for the very
long sentences (80 and more tokens). The quality of the NMT system is
dramatically lower for these since it produces too short translations.</p>

<h2 id="word-alignments">Word Alignments</h2>

<p>Some might think that the attention mechanism in NMT systems plays the
role of the word alignment between the source and target sentences. To
examine this, the researchers compared the soft attention matrix in NMT
with word alignments obtained by <a href="https://github.com/clab/fast_align">fast-align
toolkit</a>. The following figure shows
two the difference on two sentences (left when translating from
English→German; right when doing the opposite):</p>

<div align="center">
    <img src="media/SMT_Vs_NMT/image7.png" width="750" />
</div>

<p>As seen from the left example, most words match up pretty well and both
are a bit fuzzy around the words “have-been” / “sind”. In the right
example, all the alignment points appear to be off by one position
despite the fact that the NMT translation quality was pretty good. Over
all sentences, they measured how well the attention mechanism of the NMT
system matches the alignments of fast-align using two metrics:</p>

<ul>
  <li>
    <p><strong>Match score:</strong> that checks for each output if the aligned input
word according to fast-align is indeed the input word that received
the highest attention probability.</p>
  </li>
  <li>
    <p><strong>Probability mass score:</strong> that sums
up the probability mass given to each alignment point obtained from
fast-align.</p>
  </li>
</ul>

<p>The following table shows alignment scores for the two systems. The
results suggest that there is a big overlap between attention and word
alignments keeping in mind that German–English was definitely an
outlier:</p>

<div align="center">
    <img src="media/SMT_Vs_NMT/image8.png" width="450" />
</div>

<p><u><strong>These results shows that attention might look similar to word
alignments. However, it suggests that it has a broader role</strong></u>. For
instance, when translating a verb, attention may also be paid to its
subject and object since these may disambiguate it.</p>

<h2 id="beam-search">Beam Search</h2>

<p>In SMT and NMT, beam search is still the most used technique for
decoding. Usually the translation quality in both system increases when
a bigger beam size is chosen. But when comes to NMT, does this pattern
go on forever?!</p>

<p>Actually, experiments run on the NMT system using 8 language pairs
suggest otherwise. As shown below, worse translations are found beyond a
certain beam size in almost all cases. The optimal beam size varies from
4 (Czech–English) to around 30 (English–Romanian).</p>

<div align="center">
    <img src="media/SMT_Vs_NMT/image9.png" width="750" />
</div>

<p>Normalizing sentence level model scores by length of the output
alleviates the problem somewhat and also leads to better optimal quality
in most cases (5 of the 8 language pairs investigated). Based on these
experiments, <u><strong>optimal beam sizes are in the range of 30--50 in almost
all cases, but quality still drops with larger beams.</strong></u> The main
cause of deteriorating quality are shorter translations under wider
beams.</p>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/machine-translation/SMT_Vs_NMT';
      this.page.identifier = '/machine-translation/SMT_Vs_NMT';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>