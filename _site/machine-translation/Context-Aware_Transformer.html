<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>Context-Aware Transformer</title>
  <meta name="title" content="Context-Aware Transformer">
  <meta name="description" content="Context-Aware Transformer is a modified version of the
Transformer
model which is designed to allow the flow of information from the
context to the decoder to provide better and more coherent results.
Context-Aware Transformer was proposed by Yandex, University of
Edinburgh, and University of Amsterdam in 2018 and published in their
paper: Context-Aware Neural Machine Translation Learns Anaphora
Resolution.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="Context-Aware Transformer">
  <meta itemprop="description" content="Context-Aware Transformer is a modified version of the
Transformer
model which is designed to allow the flow of information from the
context to the decoder to provide better and more coherent results.
Context-Aware Transformer was proposed by Yandex, University of
Edinburgh, and University of Amsterdam in 2018 and published in their
paper: Context-Aware Neural Machine Translation Learns Anaphora
Resolution.

">
  <meta itemprop="image" content="/machine-translation/media/Context-Aware_Transformer/image2.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="Context-Aware Transformer">
  <meta property="og:description" content="Context-Aware Transformer is a modified version of the
Transformer
model which is designed to allow the flow of information from the
context to the decoder to provide better and more coherent results.
Context-Aware Transformer was proposed by Yandex, University of
Edinburgh, and University of Amsterdam in 2018 and published in their
paper: Context-Aware Neural Machine Translation Learns Anaphora
Resolution.

">
  <meta property="og:image" content="/machine-translation/media/Context-Aware_Transformer/image2.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Context-Aware Transformer">
  <meta name="twitter:description" content="Context-Aware Transformer is a modified version of the
Transformer
model which is designed to allow the flow of information from the
context to the decoder to provide better and more coherent results.
Context-Aware Transformer was proposed by Yandex, University of
Edinburgh, and University of Amsterdam in 2018 and published in their
paper: Context-Aware Neural Machine Translation Learns Anaphora
Resolution.

">
  
  <meta name="twitter:image" content="/machine-translation/media/Context-Aware_Transformer/image2.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/machine-translation/Context-Aware_Transformer">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          3 mins read
        </span>
      </p>
      <time datetime="2018-07-15 00:00" class="post-meta__body date">Published on arXiv on: 15 Jul 2018</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#Yandex Research">Yandex Research</a> & <a href="/labs/#University of Edinburgh">University of Edinburgh</a> & <a href="/labs/#University of Amsterdam">University of Amsterdam</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=Context-Aware Transformer> Context-Aware Transformer</h1>
    <p>Context-Aware Transformer is a modified version of the
<a href="https://phanxuanphucnd.github.io/machine-translation/Transformer">Transformer</a>
model which is designed to allow the flow of information from the
context to the decoder to provide better and more coherent results.
Context-Aware Transformer was proposed by Yandex, University of
Edinburgh, and University of Amsterdam in 2018 and published in their
paper: <a href="https://aclanthology.org/P18-1117.pdf">Context-Aware Neural Machine Translation Learns Anaphora
Resolution</a>.</p>

<blockquote>
  <p><strong>Note:</strong><br />
The paper’s name holds the term “anaphora” which is defined as the
“repetition of a word or phrase at the beginning of successive clauses”.
We can see Anaphora clear in the words of <a href="https://en.m.wikipedia.org/wiki/Langston_Hughes">Langston
Hughes</a> in his poem:
<a href="https://en.m.wikipedia.org/wiki/Let_America_be_America_Again">Let America be America
again</a>
where he said: “<u><strong>I am the</strong></u> farmer, bondsman to the soil. <u><strong>I
am the</strong></u> worker sold to the machine. <u><strong>I am the</strong></u> Negro,
servant to you all…”.</p>
</blockquote>

<p>Standard <a href="https://phanxuanphucnd.github.io/machine-translation/Transformer">Transformer</a>
encoders process sentences in isolation and ignore context information
that could prevent mistakes and improve translation coherence. On the
other hand, Context-Aware Transformer encoders process sentences along
with their contexts independently, and then a single attention layer, in
a combination with a gating function, is used to produce a context-aware
representation of the source sentence.</p>

<p>The following figure shows a simple comparison between standard
Transformer model and context-aware Transformer. As you can see from the
figure, context-aware encoders take two inputs: the sentence and its
context while the decoder remains unchanged.</p>

<div align="center">
    <img src="media/Context-Aware_Transformer/image1.png" width="750" />
</div>

<h2 id="context-aware-encoder">Context-Aware Encoder</h2>

<p>As shown in the following figure, the
context-aware encoder consists of three different blocks:</p>

<div align="center">
    <img src="media/Context-Aware_Transformer/image2.png" width="750" />
</div>

<ul>
  <li>
    <p><u><strong>Source Encoder:</strong></u><br />
The source encoder is composed of a stack of $N$ layers. The first
$N - 1$ layers are identical and represent the original layers of
the standard Transformer’s encoder while the last layer incorporates
contextual information.</p>
  </li>
  <li>
    <p><u><strong>Context Encoder:</strong></u><br />
The context encoder is composed of a stack of $N$ layers. The first
$N - 1$ layers are identical and represent the original layers of
the standard Transformer’s encoder while the last layer incorporates
contextual information.</p>
  </li>
  <li>
    <p><u><strong>Gated Contextual Attention:</strong></u><br />
The output of the source encoder’s attention
$c_{i}^{\left( s_ attn \right)}$ and the output of the context
encoder’s attention $c_{i}^{\left( c_ attn \right)}$ are combined
via a gated sum with a sigmoid function $\sigma$ for non-linearity:</p>
  </li>
</ul>

\[g_{i} = \sigma\left( W_{g}\left\lbrack c_{i}^{\left( \text{s\_attn} \right)};\ c_{i}^{\left( \text{c\_attn} \right)} \right\rbrack + b_{g} \right)\]

\[c_{i} = g_{i} \odot c_{i}^{\left( \text{s\_attn} \right)} + \left( 1 - g_{i} \right) \odot c_{i}^{\left( \text{c\_attn} \right)}\]

<blockquote>
  <p><strong>Note:</strong><br />
The parameters of the first $N - 1$ layers are shared between both
encoders.</p>
</blockquote>

<h2 id="experiments">Experiments</h2>

<p>In the paper, they used
<a href="http:/opus.nlpl.eu/OpenSubtitles2018.php">OpenSubtitles2018</a> corpus for
English and Russian. The data after cleaning was around 2 million
instances for training; and for validation and testing, they randomly
select two subsets of 10000 instances from movies not encountered in
training. The whole data used in this paper is available in the
following <a href="t http:/data.statmt.org/acl18_contextnmt_data">link</a>.</p>

<p>Sentences were encoded using byte-pair encoding with source and target
vocabularies of about 32k tokens. They used the same parameters and
optimizer as in the original Transformer paper.</p>

<p>The following table shows the BLEU score of multiple Transformer models;
the first two are standard Transformer models while the other three are
context-aware. From the table, we can see that the best model is the one
using a context encoder for the previous sentence.</p>

<div align="center">
    <img src="media/Context-Aware_Transformer/image3.png" width="750" />
</div>

<p>In order to verify these results, they evaluated the context encoder
(previous sentence) on the same test set with shuffled context
sentences. And they found out that its performance dropped
significantly. This confirms that the model does rely on context
information to achieve the improvement in translation.</p>

<p>To check the top words depending on the
context, they analyzed the distribution of attention to context for
individual source words in the test set to see for which words the model
depends most on contextual history. Then, they averaged the attention
score and the word position. Then, sorted them according to the
attention score. The following table contains the top-10 words:</p>

<div align="center">
    <img src="media/Context-Aware_Transformer/image4.png" width="350" />
</div>

<p>An interesting finding is that contextual attention is high for the
translation of “it”, “yours”, “ones”, “you” and “I”, which are indeed
very ambiguous out-of-context when translating into Russian. Also, the
“yes”, “yeah”, and “well” are ambiguous since they are used in
sentence-initial position.</p>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/machine-translation/Context-Aware_Transformer';
      this.page.identifier = '/machine-translation/Context-Aware_Transformer';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>