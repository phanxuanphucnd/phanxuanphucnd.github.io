<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>Evaluation Metrics</title>
  <meta name="title" content="Evaluation Metrics">
  <meta name="description" content="Machine Translation models translate a given sentence by searching for
the sentence that best suits a given criterion. However, all approaches
have to be evaluated to quantify the quality and accuracy of the
produced translations. Naturally, the best method would be to have human
experts rate each produced translation (Candidate) in order to evaluate
the whole MT system based on the reference translations.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="Evaluation Metrics">
  <meta itemprop="description" content="Machine Translation models translate a given sentence by searching for
the sentence that best suits a given criterion. However, all approaches
have to be evaluated to quantify the quality and accuracy of the
produced translations. Naturally, the best method would be to have human
experts rate each produced translation (Candidate) in order to evaluate
the whole MT system based on the reference translations.

">
  <meta itemprop="image" content="/machine-translation/media/Evaluation/image10.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="Evaluation Metrics">
  <meta property="og:description" content="Machine Translation models translate a given sentence by searching for
the sentence that best suits a given criterion. However, all approaches
have to be evaluated to quantify the quality and accuracy of the
produced translations. Naturally, the best method would be to have human
experts rate each produced translation (Candidate) in order to evaluate
the whole MT system based on the reference translations.

">
  <meta property="og:image" content="/machine-translation/media/Evaluation/image10.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Evaluation Metrics">
  <meta name="twitter:description" content="Machine Translation models translate a given sentence by searching for
the sentence that best suits a given criterion. However, all approaches
have to be evaluated to quantify the quality and accuracy of the
produced translations. Naturally, the best method would be to have human
experts rate each produced translation (Candidate) in order to evaluate
the whole MT system based on the reference translations.

">
  
  <meta name="twitter:image" content="/machine-translation/media/Evaluation/image10.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/machine-translation/Evaluation">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          12 mins read
        </span>
      </p>
      <time datetime="2020-01-01 00:00" class="post-meta__body date">Published on arXiv on: 1 Jan 2020</time>
      
    </div>
  </header>

  <section class="post">
    <h1 id=Evaluation Metrics> Evaluation Metrics</h1>
    <p>Machine Translation models translate a given sentence by searching for
the sentence that best suits a given criterion. However, all approaches
have to be evaluated to quantify the quality and accuracy of the
produced translations. Naturally, the best method would be to have human
experts rate each produced translation (Candidate) in order to evaluate
the whole MT system based on the reference translations.</p>

<p>This is a quite costly process and is not viable for development of MT systems.
For this reason a number of metrics exist that automate the process and use
different scoring methods to automatically evaluate the produced translation
based on a reference sentence. The evaluation metrics objective is to be as
close as possible to the human translation and currently the most common
techniques for calculating the correlation between human and automatic
evaluations are the <strong>Spearman’s rank correlation coefficient</strong> and the
<strong>Pearson product-moment correlation coefficient</strong>. These evaluation metrics
can be divided into three categories:</p>

<ul>
  <li>
    <p>Count-based Metrics.</p>
  </li>
  <li>
    <p>Edit-distance-based Metrics.</p>
  </li>
  <li>
    <p>Pre-trained Metrics.</p>
  </li>
</ul>

<h2 id="count-based-metrics">Count-based Metrics</h2>

<p>Count-based metrics compute the n-grams of both reference and candidate
and then compare them with each other using a scoring function.</p>

<h3 id="bleu">BLEU</h3>

<p>BLEU, stands for “<strong>B</strong>ilingual <strong>E</strong>valuation <strong>U</strong>nderstudy”, is an
evaluation metric for machine translation proposed by Aachen University
in 2002 and published in this paper: “<a href="https://aclanthology.org/P02-1038.pdf">Discriminative Training and
Maximum Entropy Models for Statistical Machine
Translation</a>”. BELU is considered
the most commonly used evaluation metric for machine translation so far.
In the following part, we will get a sense of how it works:</p>

<div align="center">
    <img src="media/Evaluation/image1.png" width="450" />
</div>

<p>Taking the former example where the French sentence has been translated
by two different linguists. Let’s say that our Machine Translation Model
has produced a bad translation for the French sentence; which is:</p>

<div align="center">
    <img src="media/Evaluation/image2.png" width="450" />
</div>

<p>Let’s see how we can calculate the BLEU score for this translation.
First, we will split the sentence into words and see if each word
appears in the provided references like so. But, BLEU score doesn’t care
about only the words. It’s cares about word-pairs as well. So, let’s see
the bigram word-pairs of the previous translation too:</p>

<div align="center">
    <img src="media/Evaluation/image3.png" width="750" />
</div>

<p>So, the unigram score will be
$\frac{2 + 1 + 1 + 1}{3 + 2 + 1 + 1} = \frac{5}{7}$ and the bigram score
will be
$\frac{1 + 0 + 1 + 1 + 1}{2 + 1 + 1 + 1 + 1} = \frac{4}{6} = \frac{2}{3}$.
In other words, the BLEU score of a n-gram model will be:</p>

\[P_{n} = \left( \sum_{ngram \in \hat{y}}^{}{\text{Coun}t_{\text{maxref}}\left( \text{ngram} \right)} \right) \ast \left( \sum_{ngram \in \hat{y}}^{}{\text{Coun}t_{\text{output}}\left( \text{ngram} \right)} \right)^{- 1}\]

<p>It’s common to get the values of $P_{1}$ till $P_{4}$ then combine these
scores in the following formula:</p>

\[\text{BLEU} = \text{BP} \ast \exp\left( \frac{1}{4} \ast \sum_{n = 1}^{4}P_{n} \right)\]

<p>$\text{BP}$ factor stands for “Brevity Penalty”. It turns out that if we
output very short translations, it's easier to get high BLEU score
because most of the words will appear in the references. But we don't
want translations that are very short. So, the $\text{BP}$, or the
brevity penalty, is an adjustment factor that penalizes translation
systems that output translations that are too short. So, the formula for
the brevity penalty is the following. It's equal to $1$ if our machine
translation system actually outputs things that are longer than the
human generated reference outputs. And otherwise is some formula like
that that overall penalizes shorter translations:</p>

\[\text{BP} = \left\{ \begin{matrix}
\ \ \ \ \ \ \ \ \ \ 1\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ L_{\text{out}} &gt; L_{\text{ref}} \\
\exp\left( 1 - \frac{L_{\text{ou}t}}{L_{\text{ref}}} \right)\ \ \ \ \ \ \ \ \ \ \ \text{otherwise} \\
\end{matrix} \right.\]

<p>Such that $L_{\text{out}}$ is the length of the generated sentence by
our Machine Translation Model, $L_{\text{ref}}$ is the length of the
reference sentence.</p>

<h3 id="meteor">METEOR</h3>

<p>Meteor, stands for “<strong>M</strong>etric for <strong>E</strong>valuation of <strong>T</strong>ranslation
with <strong>E</strong>xplicit <strong>Or</strong>dering”, is an evaluation metric for machine
translation proposed by Carnegie Mellon University in 2005 and published
in this paper: “<a href="https://aclanthology.org/W05-0909.pdf">METEOR: An Automatic Metric for MT Evaluation with
Improved Correlation with Human
Judgments</a>”.</p>

<p>This metric was designed to fix some of the problems found in the BLEU
metric, and also produce good correlation with human judgement.
According to the paper, METEOR shows correlation of up to 0.964 with
human judgement at the corpus level, compared to BLEU's achievement of
0.817 on the same data set.</p>

<p>To see how this metric works, let’s take an example and evaluate the
following hypothesis translation using the reference one:</p>

<div align="center">
    <img src="media/Evaluation/image4.png" width="750" />
</div>

<p>We can calculate the METEOR metric using the following steps:</p>

<ul>
  <li>The first step is to create an alignment between two sentences
(hypothesis, reference) resulting something like this:</li>
</ul>

<div align="center">
    <img src="media/Evaluation/image5.png" width="750" />
</div>

<ul>
  <li>Then, calculate the precision $P$ and the recall $R$:</li>
</ul>

\[P = \frac{\#\ unigrams\ in\ hypothesis\ found\ in\ reference}{\#\ unigrams\ in\ hypothesis} = \frac{6}{7}\]

\[R = \frac{\#\ unigrams\ in\ hypothesis\ found\ in\ reference}{\#\ unigrams\ in\ reference} = \frac{6}{6} = 1\]

<ul>
  <li>Then, Precision and recall are combined using the harmonic mean with
recall weighted 9 times more than precision:</li>
</ul>

\[F_{\text{mean}} = \frac{10P.R}{R + 9P} = \frac{10.\frac{6}{7}.1}{1 + 9.\frac{6}{7}} = 0.9836\]

<ul>
  <li>Calculate the hypothesis chunks; a chunk is defined as a set of
unigrams that are adjacent in the hypothesis and in the reference.
So, in this example, we have 2 chunks that consists of 6 unigrams:</li>
</ul>

\[c = 2,\ \ u_{m} = 6\]

<div align="center">
    <img src="media/Evaluation/image6.png" width="750" />
</div>

<ul>
  <li>Now, we can calculate the penalty; The penalty has the effect of
reducing the F~mean~ by up to 50% if there are no bigram or longer matches:</li>
</ul>

\[penalty = 0.5\left( \frac{c}{u_{m}} \right)^{3} = 0.5\left( \frac{2}{6} \right)^{3} = 0.0185\]

<ul>
  <li>Finally, the METEOR value is:</li>
</ul>

\[M = F_{\text{mean}}\left( 1 - penalty \right) = 0.9836\left( 1 - 0.0185 \right) = 0.9654\]

<blockquote>
  <p><strong>Notes:</strong></p>

  <ul>
    <li>If there are two alignments with the same number of mappings, the
  alignment is chosen with the fewest crosses/intersections. From
  the two alignments shown below, the alignment on the left would be
  selected at this point:</li>
  </ul>

  <div align="center">
    <img src="media/Evaluation/image7.png" width="750" />
</div>

  <ul>
    <li>
      <p>The highest value for METEOR metric is 1 unlike BLEU.</p>
    </li>
    <li>
      <p>To calculate a score over a whole corpus, the aggregate values for
  <strong>P</strong>, <strong>R</strong> and <strong>penalty</strong> are taken and then combined using the
  same formula.</p>
    </li>
    <li>
      <p>METEOR also works for comparing a candidate translation against more
  than one reference translations. In this case the algorithm
  compares the candidate against each of the references and selects
  the highest score.</p>
    </li>
  </ul>
</blockquote>

<h3 id="beer">BEER</h3>

<p>BEER, stands for “<strong>BE</strong>tter <strong>E</strong>valuation as <strong>R</strong>anking”, is an
evaluation metric for machine translation proposed by the university of
Amesterday in 2014 and published in this paper: “<a href="https://aclanthology.org/W14-3354.pdf">BEER: BEtter
Evaluation as Ranking</a>” and the
official GitHub repository can be found here: 
<a href="https://github.com/stanojevic/beer">beer</a>.</p>

<p>TO BE CONTINUED…</p>

<h3 id="chrf">ChrF</h3>

<p>ChrF, stands for “character F-score” is another count-based metric for
evaluating machine translation models. ChrF was proposed by Maja
Popovic´at the Humboldt University of Berlin in 2015 and published in
this paper: “<a href="https://aclanthology.org/W15-3049.pdf">CHRF: character n-gram F-score for automatic MT
evaluation</a>”.</p>

<blockquote>
  <p><strong>Note:</strong><br />
You can use <code class="language-plaintext highlighter-rouge">nltk.chrf_score()</code> function for this metric.</p>
</blockquote>

<p>This metric depends on the character-level n-gram since it correlates very well
with human judgments. The general formula for the CHRF score is:</p>

\[\text{chr}F_{\beta} = \left( 1 + \beta^{2} \right).\frac{chrP + chrR}{\beta^{2}.chrP + chrR}\]

<p>Where $\text{chrP}$ and $\text{chrR}$ are the n-gram precision and
recall respectively averaged over all character n-grams. And and $\beta$
is a parameter which assigns $\beta$ times more importance to recall
than to precision. If $\beta = 1$, they have the same importance.</p>

<p>To understand this metric better, let’s take an example and evaluate the
following hypothesis translation using the reference one:</p>

<div align="center">
    <img src="media/Evaluation/image8.png" width="450" />
</div>

<p>We can calculate the chrF metric using the following steps:</p>

<ul>
  <li>Calculate the unigram character for both reference and hypothesis:</li>
</ul>

<div align="center" class="inline-table">
<table>
    <thead>
        <tr>
            <th></th>
            <th>a</th>
            <th>c</th>
            <th>e</th>
            <th>h</th>
            <th>i</th>
            <th>k</th>
            <th>l</th>
            <th>o</th>
            <th>s</th>
            <th>t</th>
            <th>v</th>
            <th>y</th>
        </tr>
    </thead>
    <tr>
        <td><strong>Reference</strong></td>
        <td>1</td>
        <td>1</td>
        <td>2</td>
        <td>1</td>
        <td>1</td>
        <td>0</td>
        <td>1</td>
        <td>1</td>
        <td>0</td>
        <td>2</td>
        <td>1</td>
        <td>0</td>
    </tr>
    <tr>
    <td><strong>Hypothesis</strong></td>
        <td>0</td>
        <td>0</td>
        <td>2</td>
        <td>1</td>
        <td>3</td>
        <td>2</td>
        <td>1</td>
        <td>0</td>
        <td>0</td>
        <td>3</td>
        <td>0</td>
        <td>1</td>
    </tr>
</table>
</div>

<ul>
  <li>
    <p>Calculate the following metrics:</p>

    <ul>
      <li>$tp$ (True Positive): count of characters found in both
hypothesis and reference. So, in this example, we have (e,2),
(h,1), (i,1), (l,1), (t,2):</li>
    </ul>
  </li>
</ul>

\[tp = 7\]

<ul>
  <li>$\text{tpfp}$ (True Positive + False Positive): count of characters
found in hypothesis.</li>
</ul>

\[tpfp = 11\]

<ul>
  <li>$\text{tpfn}$ (True Positive + False Negative): count of characters
found in reference.</li>
</ul>

\[tpfn = 13\]

<ul>
  <li>Then, we can calculate the precision $\text{chr}P$ and the recall
$\text{chrR}$:</li>
</ul>

\[\text{chr}P = \frac{\text{tp}}{\text{tpfp}} = \frac{7}{11},\ \ \ \ \ chrR = \frac{\text{tp}}{\text{tpfn}} = \frac{7}{13}\]

<ul>
  <li>Now, we can calculate the character f-score:</li>
</ul>

\[\text{chr}F_{\beta} = \left( 1 + \beta^{2} \right).\frac{chrP + chrR}{\beta^{2}.chrP + chrR}\]

<ul>
  <li>All of that for unigram character model. In the paper, they did that
when n=1 till n=6. So, we will do the same when n=2,3,4,5,6 and
then average the f-score.</li>
</ul>

<h2 id="edit-distance-based-metrics">Edit-Distance-Based Metrics</h2>

<p>Edit distance based metrics utilize the edit distance to express the
difference between the candidate and the reference. Edit distance is a
way to quantify how two words are far apart. More formally, the minimum
edit distance between two strings is defined as the minimum number of
editing operations (operations like insertion, deletion, substitution,
and shifts of adjacent letters) needed to transform one string into
another. The most common edit distance is the <strong>Levenshtein distance</strong>.</p>

<h3 id="mms">MMS</h3>

<p>MMS, stands for “<strong>M</strong>aximum <strong>M</strong>atching <strong>S</strong>tring”, is an evaluation
metric for machine translation proposed by New York University in 2003
and published in this paper: “<a href="https://aclanthology.org/2003.mtsummit-papers.51.pdf">Evaluation of Machine Translation and its
Evaluation</a>”.</p>

<p>TO BE CONTINUED…</p>

<h3 id="ter">TER</h3>

<p>TER, stands for “<strong>T</strong>ranslation <strong>E</strong>dit <strong>R</strong>ate”, is an evaluation
metric for machine translation proposed by the university of Maryland in
2006 and published in this paper: “<a href="http://www.cs.umd.edu/~snover/pub/amta06/ter_amta.pdf">A Study of Translation Edit Rate
with Targeted Human
Annotation</a>” and
an unofficial repository implementing TER can be found in the
<a href="https://github.com/mjpost/sacrebleu">sacreBLEU</a> python package.</p>

<p>TER measures the amount of editing that a human would have to perform to
change a candidate translation so it exactly matches a reference
translation normalized by the average length of the references.</p>

\[TER = \frac{\#\ of\ edits}{average\ \#\ of\ reference\ words}\]

<p>Let’s take an example to see how TER works; consider the following
translation:</p>

<div align="center">
    <img src="media/Evaluation/image9.png" width="750" />
</div>

<p>Here, the hypothesis (HYP) is fluent and means the same thing (except
for missing “American”) as the reference (REF). However, TER does not
consider this an exact match as:</p>

<ul>
  <li>
    <p>The phrase “this week” is “shifted” (this counts as <u><strong>one
</strong></u> shift). Shifts have a cost of one no matter how far this
phrase moves.</p>
  </li>
  <li>
    <p>The phrase “Saudi Arabia” in the reference appears as “the Saudis”
in the hypothesis (this counts as <u><strong>two</strong></u> separate
substitutions).</p>
  </li>
  <li>
    <p>The word “American” appears only in the reference (this counts as
<u><strong>one</strong></u> insertion).</p>

    <p>Then, the TER score is:</p>
  </li>
</ul>

\[TER = \frac{4}{13} = 30.77\%\]

<blockquote>
  <p><strong>Important Notes:</strong></p>

  <ul>
    <li>
      <p>Since we are concerned with the minimum number of edits needed to
  modify the hypothesis, we only measure the number of edits to the
  closest reference.</p>
    </li>
    <li>
      <p>TER assumes that all edits (insertion, deletion, ...etc.) have the
  same cost.</p>
    </li>
    <li>
      <p>Punctuations are treated as normal words.</p>
    </li>
    <li>
      <p>Mis-capitalization is counted as an edit.</p>
    </li>
    <li>
      <p>In the paper, T(1) denoted that TER was used on just one reference
  sentence while T(4) was used on four different reference sentences.</p>
    </li>
  </ul>
</blockquote>

<h3 id="character">CharacTER</h3>

<p>CharacTER is a Translation Edit Rate (TER) on Character evaluation
metric for machine translation proposed by Aachen University in 2016 and
published in this paper: “<a href="https://aclanthology.org/W16-2342.pdf">CharacTER: Translation Edit Rate on Character
Level</a>” and the official code for
this metric can be found on this GitHub repository:
<a href="https://github.com/rwth-i6/CharacTER">CharacTER</a>.</p>

<p>CharacTer is defined as the minimum number of character edits required
to adjust a hypothesis until it completely matches the reference,
normalized by the length of the hypothesis sentence:</p>

\[CharacTER = \frac{shift\ cost\  + \ edit\ distance}{\#\ characters\ in\ the\ hypothesis\ sentence}\]

<p>CharacTer calculates shift edit on word level; two words are considered
to be matched if <u>they are exactly the same</u>, or if <u>the edit
distance between them is below a threshold value</u>.</p>

<h3 id="eed">EED</h3>

<p>EED, stands for “<strong>E</strong>xtended <strong>E</strong>dit <strong>D</strong>istance”, is an evaluation
metric for machine translation proposed by Aachen University in 2019 and
published in this paper: “<a href="https://aclanthology.org/W19-5359.pdf">EED: Extended Edit Distance Measure for
Machine Translation</a>”. The
official code for this metric can be found in the following GitHub
repository: <a href="https://github.com/rwth-i6/ExtendedEditDistance">ExtendedEditDistance</a>.</p>

<p>This paper proposes an extension of the Levenshtein edit distance, which
achieves better human correlation whilst remaining fast, flexible and
easy to understand. This extension is can be described as a “jump”, a
jumps provides the opportunity to continue the edit distance computation
from a different point. In the following figure, a jump is represented
as a dashed line:</p>

<div align="center">
    <img src="media/Evaluation/image10.png" width="450" />
</div>

<p>EED utilizes the idea of jumps as an extension of the edit distance. EED
operates at character level and is defined as follows:</p>

\[EED = min\left( \frac{\left( e + \alpha \text{.j} \right) + \rho \text{.v}}{\left| r \right| + \rho \text{.v}},\ 1 \right),\ \ \ \text{EED} \in \left\lbrack 0,\ 1 \right\rbrack\]

<p>Where:</p>

<ul>
  <li>
    <p>$e$ is the sum of the edit operation with uniform cost of 1 for
insertions and substitutions and 0.2 for deletions.</p>
  </li>
  <li>
    <p>$j$ denotes the number of jumps performed with the corresponding
control parameter $\alpha = 2.0$.</p>
  </li>
  <li>
    <p>$v$ defines the number of characters that have been visited multiple
times or not at all and scales over $\rho = 0.3$.</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$\left</td>
          <td>r \right</td>
          <td>$ is the length of the reference sentence.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h2 id="pre-trained-metrics">Pre-trained Metrics</h2>

<p>These kind of metrics use pre-trained neural models to evaluate the
quality of MT output texts given the source sentence, the human
reference, or both. One thing good about these metrics is that they are
not strictly dependent on the human translation, so they can better
evaluate synonyms or paraphrases.</p>

<p>On the other hand, their performance is influenced by the data on which
they have been trained. Also, the pre-trained models introduce a
black-box problem where it is difficult to diagnose potential unexpected
behavior of the metric, such as various biases learned from training
data.</p>

<h3 id="yisi">YiSi</h3>

<p>Check the paper: <a href="https://aclanthology.org/W19-5358.pdf">YiSi - A unified semantic MT quality evaluation and
estimation metric for languages with different levels of available
resources</a>. The official code for
this paper can be found on this GitHub repository:
<a href="https://github.com/chikiulo/yisi">yisi</a>.</p>

<p>TO BE CONTINUED…</p>

<h3 id="bertscore">BERTScore</h3>

<p>BERTScore is an automatic evaluation metric for text generation proposed
by Cornell University in 2020 and published in their paper: <a href="https://arxiv.org/pdf/1904.09675.pdf">Bertscore:
Evaluating Text Generation With
Bert</a>. The official repository for
this paper can be found here
<a href="https://github.com/Tiiiger/bert_score">bert_score</a>.</p>

<p>BERTScore computes a similarity score for each token in the candidate
sentence with each token in the reference sentence. However, instead of
exact matches, we compute token similarity using contextual embeddings.
Which makes it the perfect candidate for evaluating machine translation
models. The scoring algorithm is relatively straightforward as shown in
the following figure:</p>

<div align="center">
    <img src="media/Evaluation/image11.png" width="750" />
</div>

<p>Given a reference sentence and a candidate sentence $x$ and a candidate
sentence $\widehat{x}$, the scoring algorithm goes like so:</p>

<ul>
  <li>
    <p>Tokenize the sentences using the tokenizer provided by each model;
so the reference sentence becomes
$x = \langle x_{1},\ …x_{k} \rangle$ and the candidate sentence becomes
$\widehat{x} = \langle \widehat{x}_1,\text{ …}\widehat{x}_l \rangle$</p>
  </li>
  <li>
    <p>Given these tokenized sentences, BERT generates representation the
same size as the tokenized sentences; the translation hypothesis
embedding
<span>$\mathbf{x}=\langle \mathbf{x}_{\mathbf{1}}\mathbf{,\ …}\mathbf{x}_{\mathbf{k}} \rangle$</span>
and the reference sentence embedding
<span>$\widehat{\mathbf{x}}=\langle \widehat{\mathbf{x}}_{\mathbf{1}},\text{ …}\widehat{\mathbf{x}}_{\mathbf{l}} \rangle$</span>.</p>
  </li>
  <li>
    <p>Compute a matrix of pair-wise cosine similarities of all words from
the hypothesis and from the reference sentence. The cosine
similarity of a reference token $x_{i}$ and a candidate token
${\widehat{x}}_{j}$ is:</p>
  </li>
</ul>

\[cosine = \frac{\mathbf{x}_{\mathbf{i}}^{\mathbf{T}}.{\widehat{\mathbf{x}}}_{\mathbf{j}}}{\left\| \mathbf{x}_{\mathbf{i}} \right\|.\left\| {\widehat{\mathbf{x}}}_{\mathbf{j}} \right\|}\]

<ul>
  <li>Use greedy approach, we get the maximum similarity where each token
in the reference sentence is matched to the most similar token in
the candidate sentence to use them to compute the precision &amp;
recall.</li>
</ul>

\[R_{\text{BERT}} = \frac{1}{\left| x \right|}\sum_{x_{i} \in x}^{}{\max_{\widehat{x}_j \in \widehat{x}}{\mathbf{x}_{\mathbf{i}}^{\mathbf{T}}.{\widehat{\mathbf{x}}}_{\mathbf{j}}}}\]

\[P_{\text{BERT}} = \frac{1}{\left| \widehat{x} \right|}\sum_{\widehat{x}_j \in \widehat{x}}^{}{\max_{x_{i} \in x}{\mathbf{x}_{\mathbf{i}}^{\mathbf{T}}.{\widehat{\mathbf{x}}}_{\mathbf{j}}}}\]

<ul>
  <li>We combine precision and recall to compute the F1 measure: the
harmonic average of precision and recall.</li>
</ul>

\[F_{\text{BERT}} = \frac{2P_{\text{BERT}}.R_{\text{BERT}}}{P_{\text{BERT}} + R_{\text{BERT}}}\]

<ul>
  <li>Since previous work demonstrated that rare words can be more
indicative for sentence similarity than common words, we incorporate
inverse document frequency (idf) weighting. Given $M$ reference
sentences $\left[ x^{\left( i \right)} \right]_{i = 1}^{M}$, the
idf score of a word-piece token $w$ is:</li>
</ul>

\[\text{idf}\left( w \right) = - \log\left( \frac{1}{M}\sum_{i = 1}^{M}{\mathbb{I}\left\lbrack w \in x^{\left( i \right)} \right\rbrack} \right)\]

<p>   Where $\mathbb{I}\left\lbrack . \right\rbrack$ is an indicator function.</p>

<ul>
  <li>Now, the precision &amp; recall becomes:</li>
</ul>

\[R_{\text{BERT}} = \frac{1}{\sum_{x_{i} \in x}^{}{\text{idf}\left( x_{i} \right)}}\sum_{x_{i} \in x}^{}{\text{idf}\left( x_{i} \right)\max_{\widehat{x}_j \in \widehat{x}}{\mathbf{x}_{\mathbf{i}}^{\mathbf{T}}.{\widehat{\mathbf{x}}}_{\mathbf{j}}}}\]

\[P_{\text{BERT}} = \frac{1}{\sum_{x_{i} \in x}^{}{\text{idf}\left( x_{i} \right)}}\sum_{\widehat{x}_j \in \widehat{x}}^{}{\max_{x_{i} \in x}{\text{idf}\left( x_{i} \right).\mathbf{x}_{\mathbf{i}}^{\mathbf{T}}.{\widehat{\mathbf{x}}}_{\mathbf{j}}}}\]

<ul>
  <li>Finally, BERTScore have the same numerical range of cosine
similarity $\left\lbrack - 1,1 \right\rbrack$. However, in practice
scores are in a more limited range $\left\lbrack 0,1 \right\rbrack$.
So, we rescale BERTScore with respect to its empirical lower bound b
as a baseline like so:</li>
</ul>

\[{\widehat{R}}_{\text{BERT}} = \frac{R_{\text{BERT}} - b}{1 - b}\]

\[{\widehat{P}}_{\text{BERT}} = \frac{P_{\text{BERT}} - b}{1 - b}\]

\[{\widehat{F}}_{\text{BERT}} = \frac{F_{\text{BERT}} - b}{1 - b}\]

<ul>
  <li>We compute b using Common Crawl monolingual datasets. For each
language and contextual embedding model, we create 1M
candidate-reference pairs by grouping two random sentences. Because
of the random pairing and the corpus diversity, each pair has very
low lexical and semantic overlapping. We compute $b$ by averaging
BERTScore computed on these sentence pairs.</li>
</ul>

<h3 id="bleurt">BLEURT</h3>

<p>BLEURT is an automatic evaluation metric for text generation proposed by
Google Research in 2020 and published in their paper: <a href="https://arxiv.org/pdf/2004.04696.pdf">BLEURT: Learning
Robust Metrics for Text
Generation</a>. The official
code for this paper can be found on Google Research’s official GitHub
repository: <a href="https://github.com/google-research/bleurt">bleur</a>.</p>

<p>TO BE CONTINUED…</p>

<h3 id="comet">COMET</h3>

<p>BLEURT is an automatic evaluation metric for machine translation
proposed by Unbabel AI in 2020 and published in their paper: <a href="https://arxiv.org/pdf/2009.09025.pdf">COMET: A
Neural Framework for MT
Evaluation</a>. The official
repository for this paper can be found on this GitHub repository: 
<a href="https://github.com/Unbabel/COMET">COMET</a>.</p>

<p>TO BE CONTINUED…</p>

<h3 id="prism">Prism</h3>

<p>Prism is an automatic evaluation metric for machine translation proposed
by John Hopkins Unversity in 2020 and published in their paper:
<a href="https://arxiv.org/pdf/2004.14564.pdf">Automatic Machine Translation Evaluation in Many Languages via
Zero-Shot Paraphrasing</a>. The
official code for this paper can be found on this GitHub repository:
<a href="https://github.com/thompsonb/prism">prism</a>.</p>

<p>TO BE CONTINUED…</p>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/machine-translation/Evaluation';
      this.page.identifier = '/machine-translation/Evaluation';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>