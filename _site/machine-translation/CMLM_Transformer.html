<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>CMLM Transformer</title>
  <meta name="title" content="CMLM Transformer">
  <meta name="description" content="CMLM stands for “Conditional Masked Language Modeling” Transformer which
is an encoder-decoder Transformer
architecture trained with a masked
language modeling (MLM) training objective and uses “masked-predict”
algorithm for decoding. This model was proposed by FAIR in 2019 and
published in their paper: Mask-Predict: Parallel Decoding of
Conditional Masked Language
Models. The official code for
this paper can be found on Facebook Research’s official GitHub
repository:
facebookresearch/Mask-Predict.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="CMLM Transformer">
  <meta itemprop="description" content="CMLM stands for “Conditional Masked Language Modeling” Transformer which
is an encoder-decoder Transformer
architecture trained with a masked
language modeling (MLM) training objective and uses “masked-predict”
algorithm for decoding. This model was proposed by FAIR in 2019 and
published in their paper: Mask-Predict: Parallel Decoding of
Conditional Masked Language
Models. The official code for
this paper can be found on Facebook Research’s official GitHub
repository:
facebookresearch/Mask-Predict.

">
  <meta itemprop="image" content="/machine-translation/media/CMLM_Transformer/image3.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="CMLM Transformer">
  <meta property="og:description" content="CMLM stands for “Conditional Masked Language Modeling” Transformer which
is an encoder-decoder Transformer
architecture trained with a masked
language modeling (MLM) training objective and uses “masked-predict”
algorithm for decoding. This model was proposed by FAIR in 2019 and
published in their paper: Mask-Predict: Parallel Decoding of
Conditional Masked Language
Models. The official code for
this paper can be found on Facebook Research’s official GitHub
repository:
facebookresearch/Mask-Predict.

">
  <meta property="og:image" content="/machine-translation/media/CMLM_Transformer/image3.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="CMLM Transformer">
  <meta name="twitter:description" content="CMLM stands for “Conditional Masked Language Modeling” Transformer which
is an encoder-decoder Transformer
architecture trained with a masked
language modeling (MLM) training objective and uses “masked-predict”
algorithm for decoding. This model was proposed by FAIR in 2019 and
published in their paper: Mask-Predict: Parallel Decoding of
Conditional Masked Language
Models. The official code for
this paper can be found on Facebook Research’s official GitHub
repository:
facebookresearch/Mask-Predict.

">
  
  <meta name="twitter:image" content="/machine-translation/media/CMLM_Transformer/image3.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/machine-translation/CMLM_Transformer">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          7 mins read
        </span>
      </p>
      <time datetime="2019-09-04 00:00" class="post-meta__body date">Published on arXiv on: 4 Sep 2019</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#FAIR">FAIR</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=CMLM Transformer> CMLM Transformer</h1>
    <p>CMLM stands for “Conditional Masked Language Modeling” Transformer which
is an encoder-decoder <a href="https://phanxuanphucnd.github.io/machine-translation/Transformer">Transformer</a>
architecture trained with a masked
language modeling (MLM) training objective and uses “masked-predict”
algorithm for decoding. This model was proposed by FAIR in 2019 and
published in their paper: <a href="https://arxiv.org/pdf/2009.14794.pdf">Mask-Predict: Parallel Decoding of
Conditional Masked Language
Models</a>. The official code for
this paper can be found on Facebook Research’s official GitHub
repository:
<a href="https://github.com/facebookresearch/Mask-Predict">facebookresearch/Mask-Predict</a>.</p>

<p>Unlike most machine translation systems which generate translations
autoregressively from left to right, the CMLM Transformer uses a
non-autoregressive decoding algorithm called “masked-predict” that
iteratively decodes in linear time. CMLM Transformer is the same as the
<a href="https://phanxuanphucnd.github.io/machine-translation/Transformer">standard Transformer</a>
with one change in the decoder; the masking of the self-attention in the
decoder was removed to make the decoder attend to both left and right
contexts when generating the output translation.</p>

<blockquote>
  <p><strong>Note:</strong>
The difference between MLM and CMLM is that MLM predicts masked tokens given
the remaining sentence while CMLM predicts masked tokens given the source
sentence + the remaining of target sentence.</p>
</blockquote>

<h2 id="cmlm-training">CMLM Training</h2>

<p>A conditional masked language modeling (CMLM) predicts a set of target
tokens $Y_{\text{mask}}$ given a source text $X$ and part of the target
text $Y_{\text{obs}}$. It makes the strong assumption that the tokens
$Y_{\text{mask}}$ are conditionally independent of each other (given $X$
and $Y_{\text{obs}}$). During training, CMLM is done as follows:</p>

<ul>
  <li>
    <p>A number of tokens that should be masked is chosen randomly from a
uniform distribution between one and the sequence’s length.</p>
  </li>
  <li>
    <p>Then, that number of tokens get masked $Y_{\text{mask}}$. Following
<a href="https://phanxuanphucnd.github.io/language-modeling/BERT">BERT</a>, masking
is done by replacing the token with a special
$\left\lbrack \text{MASK} \right\rbrack$ token.</p>
  </li>
  <li>
    <p>CMLM is optimized using cross-entropy loss over every token in
$Y_{\text{mask}}$. This can be done in parallel, since the model
assumes that the tokens in $Y_{\text{mask}}$ are conditionally
independent of each other.</p>
  </li>
</ul>

<p>In traditional left-to-right machine translation, where the target
length is determined by the generation of the special end of sentence
token $\left\lbrack \text{EOS} \right\rbrack$. However, for CMLMs to
predict the entire sequence in parallel, they must know its length in
advance. Here, they added a special
$\left\lbrack \text{LENGTH} \right\rbrack$ token to the encoder and then
train the model to predict the length of the target sequence. Its loss
is added to the cross-entropy loss.</p>

<h2 id="mask-predict-decoding">Mask-Predict Decoding</h2>

<p>Mask-predict is a new highly-parallel decoding algorithm that predicts
any subset of the target words conditioned on the input text $X$ and a
partially masked target translation $Y_{\text{mask}}$. Decoding starts
with a completely masked target text, then the model predicts all of the
words in parallel, then repeatedly masks out and regenerates the subset
of words that the model is least confident about depending on the other
high-confidence predictions. After a few cycles, the model starts to
produce high-quality translations.</p>

<p>More formally; given the target sequence’s length N, we define two
variables: the target sequence $\left( y_{1},\ …y_{N} \right)$ and the
probability of each token $\left( p_{1},\ …p_{N} \right)$. The
algorithm runs for a predetermined number of iterations $T$, which is
either a constant or a simple function of $N$. At each iteration, we
perform a mask operation, followed by predict.</p>

<ul>
  <li><u><strong>Mask:</strong></u><br />
For the first iteration ($t = 0$), we mask all the tokens. For later
iterations, we mask the $n$ tokens with the lowest probability
scores. The number of masked tokens $n$ is a function of the
iteration $t$; usually a linear decay $n = N.\frac{T - t}{T}$. For
example, if $T = 10$, $90\%$ of the tokens will be masked at
$t = 1$. $80\%$ at $t = 2$, and so forth.</li>
</ul>

\[Y_{\text{mask}}^{\left( t \right)} = \underset{i}{\text{arg }\min}\left( p_{i},\ n \right)\]

\[Y_{\text{obs}}^{\left( t \right)} = Y\backslash Y_{\text{mask}}^{\left( t \right)}\]

<ul>
  <li><u><strong>Predict:</strong></u><br />
After masking, the CMLM predicts the masked tokens
$Y_{\text{mask}}^{\left( t \right)}$ mask, conditioned on the source
text $X$ and the unmasked target tokens
$Y_{\text{obs}}^{\left( t \right)}$. We select the prediction with
the highest probability for each masked token $y_i ∈ Y(t)$ mask and
unmask them by update its probability score:</li>
</ul>

\[y_{i}^{\left( t \right)} = \underset{w}{\text{arg }\text{max}}{P\left( y_{i} = w \middle| X,\ Y_{\text{obs}}^{\left( t \right)} \right)}\]

\[p_{i}^{\left( t \right)} = \underset{w}{\text{max}}{P\left( y_{i} = w \middle| X,\ Y_{\text{obs}}^{\left( t \right)} \right)}\]

<p>The following example is from the WMT’14 German→English validation set
that illustrates how mask-predict generates text. At each iteration, the
highlighted tokens are masked and re-predicted, conditioned on the other
tokens in the sequence.</p>

<div align="center">
    <img src="media/CMLM_Transformer/image1.png" width="750" />
</div>

<p>In the first iteration ($t = 0$), the entire target sequence is masked
($Y_{\text{mask}}^{\left( 0 \right)} = Y,\ Y_{\text{obs}}^{\left( 0 \right)} = \varnothing$),
and is thus generated by the CMLM in a purely non-autoregressive
process. This produces an ungrammatical translation with repetitions
(“completed completed”).</p>

<p>In the second iteration ($t = 1$), 8 of the 12 tokens generated in the
previous step were predicted with the lowest probabilities. That’s why
they got masked with the $\left\lbrack \text{MASK} \right\rbrack$ token
and re-predicted while conditioning on the input sequence $X$ and the
four unmasked tokens
$Y_{\text{obs}}^{\left( 0 \right)} = {<code class="language-plaintext highlighter-rouge">The",\</code>20”,\ <code class="language-plaintext highlighter-rouge">November",\</code>.”}$
which results in a more grammatical and accurate translation.</p>

<p>In the third iteration ($t = 2$), 4 of the 8 tokens generated in the
previous step were predicted with the lowest probabilities. Now that the
model is conditioning on 8 tokens, it is able to produce an more fluent
translation; “withdrawal” is a better fit for describing troop movement,
and “November 20th” is a more common date format in English</p>

<blockquote>
  <p><strong>Notes:</strong></p>

  <ul>
    <li>
      <p>In the third iteration ($t = 2$), two of the four masked tokens were
  predicted at the first step ($t = 0$), and not re-predicted at the
  second step ($t = 1$). This is quite common for earlier predictions
  to be masked at later iterations because they were predicted with
  less information and thus tend to have lower probabilities.</p>
    </li>
    <li>
      <p>When decoding, they unmask the highest $l$ tokens. This is a
  hyper-parameter that can be seen as the beam size for beam search of
  non-autoregressive decoders. The following table shows an experiment
  of base CMLM with $T = 10$. Surprisingly, more $l$ tokens can
  degrade performance.</p>

      <div align="center">
<img src="media/CMLM_Transformer/image2.png" width="450" />
</div>
    </li>
    <li>
      <p>The following figure shows the decoding speed of CLML Transformer,
  compared to the standard base transformer on the WMT’14 EN-DE test
  set, with beam sizes $b = 1$ (orange triangle) and $b = 5$ (red
  triangle). Each blue circle represents a mask-predict decoding run
  with a different number of iterations $T = \left\{ 4,\ …,\ 10 \right\}$
  and length candidates $l = \left\{ 1,\ 2,\ 3 \right\}$:</p>

      <div align="center">
<img src="media/CMLM_Transformer/image3.png" width="750" />
</div>
    </li>
  </ul>
</blockquote>

<h2 id="experiments">Experiments</h2>

<p>In this paper, they followed the standard hyper-parameters for
transformers in both small and base configuration:</p>

<div align="center" class="inline-table">
<table>
    <thead>
        <tr>
            <th></th>
            <th>Layers</th>
            <th>Attention Heads</th>
            <th>Model Dimension</th>
            <th>Hidden Dimension</th>
        </tr>
    </thead>
    <tr>
        <td><strong>Small</strong></td>
        <td>6</td>
        <td>8</td>
        <td>512</td>
        <td>512</td>
    </tr>
    <tr>
        <td><strong>Base</strong></td>
        <td>6</td>
        <td>8</td>
        <td>512</td>
        <td>2048</td>
    </tr>
</table>
</div>

<p>They followed the weight initialization scheme from
<a href="https://phanxuanphucnd.github.io/language-modeling/BERT">BERT</a>, which samples
weights from $\mathcal{N}\left( 0,\ 0.02 \right)$, initializes biases to
zero, and sets layer normalization parameters to
$\beta = 0,\ \gamma = 1$. For regularization, they used $0.3$ dropout,
$0.01$ L2 weight decay, and smoothed cross validation loss with
$\varepsilon = 0.1$.</p>

<p>They used batches of $128k$ tokens using Adam
optimizer with $\beta = (0.9,\ 0.999)$ and $\varepsilon = 10^{- 6}$. The
learning rate warms up to a peak of $5*10^{- 4}$ within $10,000$ steps,
and then decays with the inverse square-root schedule. All models were
trained for 300k steps, measured the validation loss at the end of each
epoch, and averaged the 5 best checkpoints. During decoding, they used a
beam size of $beam\ size = 5$ for autoregressive decoding, and similarly
use $l = 5$ length candidates for mask-predict decoding.</p>

<p>The following table shows that among the parallel decoding methods, CMLM
Transformer yields the state-of-the-art BLEU scores on WMT’14 EN-DE, in
both directions. Another striking result is that a CMLM with only 4
mask-predict iterations yields higher scores than 10 iterations of the
iterative refinement model:</p>

<div align="center">
    <img src="media/CMLM_Transformer/image4.png" width="750" />
</div>

<p>The translations produced by CMLM Transformer also score competitively
when compared to standard transformers. In all 4 benchmarks, CMLM-base
reaches within 0.5-1.2 BLEU points from a well-tuned base transformer, a
relative decrease of less than 4% in translation quality. In many
scenarios, this is an acceptable price to pay for a significant speedup
from parallel decoding.</p>

<p>The following table shows that these trends also hold for
English-Chinese translation, in both directions, despite major
linguistic differences between the two languages:</p>

<div align="center">
    <img src="media/CMLM_Transformer/image5.png" width="750" />
</div>

<p>Based on experiments run on EN-DE and EN-RO datasets, they found out
that multiple iterations $T$ is a must to solve the “multi-modality
problem” which means that the model often predict the same word $w$ with
high confidence, but at different positions. As we can see from the
following table, the proportion of repetitive tokens drops drastically
during the first 2-3 iterations:</p>

<div align="center">
    <img src="media/CMLM_Transformer/image6.png" width="750" />
</div>

<p>During experiments they noticed that <u><strong>longer sequences need more
iterations</strong></u>; the following table shows that increasing the number
of decoding iterations ($T$) appears to mainly improve the performance
on longer sequences:</p>

<div align="center">
    <img src="media/CMLM_Transformer/image7.png" width="750" />
</div>

<p>Having said that, the performance differences across length buckets are
not very large, and it seems that even 4 mask-predict iterations are
enough to produce decent translations for long sequences ($40 \leq N$).</p>

<h2 id="model-distillation">Model Distillation</h2>

<p>Following previous convention on non-autoregressive machine translation
models, they trained CMLM Transformer on translations produced by a
standard transformer model (large for EN-DE and EN-ZH, base for EN-RO).
For a fair comparison, they also trained standard left-to-right base
transformers on translations produced by large transformer models for
EN-DE and EN-ZH, in addition to the standard baselines.</p>

<p>To determine CMLM’s dependence on knowledge distillation, they train
models on both raw and distilled data, and compared their performance.
The following table shows that in every case, training with model
distillation substantially outperforms training on raw data:</p>

<div align="center">
    <img src="media/CMLM_Transformer/image8.png" width="750" />
</div>

<p><u><strong>It appears as though CMLMs are heavily dependent on model
distillation.</strong></u></p>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/machine-translation/CMLM_Transformer';
      this.page.identifier = '/machine-translation/CMLM_Transformer';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>