<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>GPT-2</title>
  <meta name="title" content="GPT-2">
  <meta name="description" content="GPT-2 stands for “Generative Pre-trained Transformer” which is a
language model published in this paper: “Language Models are
Unsupervised Multitask
Learners”
by OpenAI in 2019. In the paper, they tried to demonstrate that language
models can perform down-stream tasks such as (question answering,
machine translation, reading comprehension, and summarization) in a
zero-shot setting – without any parameter or architecture modification.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="GPT-2">
  <meta itemprop="description" content="GPT-2 stands for “Generative Pre-trained Transformer” which is a
language model published in this paper: “Language Models are
Unsupervised Multitask
Learners”
by OpenAI in 2019. In the paper, they tried to demonstrate that language
models can perform down-stream tasks such as (question answering,
machine translation, reading comprehension, and summarization) in a
zero-shot setting – without any parameter or architecture modification.

">
  <meta itemprop="image" content="/language-modeling/media/GPT-2/image0.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="GPT-2">
  <meta property="og:description" content="GPT-2 stands for “Generative Pre-trained Transformer” which is a
language model published in this paper: “Language Models are
Unsupervised Multitask
Learners”
by OpenAI in 2019. In the paper, they tried to demonstrate that language
models can perform down-stream tasks such as (question answering,
machine translation, reading comprehension, and summarization) in a
zero-shot setting – without any parameter or architecture modification.

">
  <meta property="og:image" content="/language-modeling/media/GPT-2/image0.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="GPT-2">
  <meta name="twitter:description" content="GPT-2 stands for “Generative Pre-trained Transformer” which is a
language model published in this paper: “Language Models are
Unsupervised Multitask
Learners”
by OpenAI in 2019. In the paper, they tried to demonstrate that language
models can perform down-stream tasks such as (question answering,
machine translation, reading comprehension, and summarization) in a
zero-shot setting – without any parameter or architecture modification.

">
  
  <meta name="twitter:image" content="/language-modeling/media/GPT-2/image0.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/language-modeling/GPT-2">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          3 mins read
        </span>
      </p>
      <time datetime="2019-02-14 00:00" class="post-meta__body date">Published on arXiv on: 14 Feb 2019</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#Open AI">Open AI</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=GPT-2> GPT-2</h1>
    <p>GPT-2 stands for “Generative Pre-trained Transformer” which is a
language model published in this paper: “<a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are
Unsupervised Multitask
Learners</a>”
by OpenAI in 2019. In the paper, they tried to demonstrate that language
models can perform down-stream tasks such as (question answering,
machine translation, reading comprehension, and summarization) in a
zero-shot setting – without any parameter or architecture modification.</p>

<p>One great way to experiment with GPT-2 is using the <a href="https://demo.allennlp.org/next-token-lm">AllenAI GPT-2
Explorer</a>. It uses GPT-2 to
display ten possible predictions for the next word (alongside their
probability score). You can select a word then see the next list of
predictions to continue writing the passage.</p>

<h2 id="webtext">WebText</h2>

<p>Most prior work trained language models on a single domain of text, such
as news articles, Wikipedia, or fiction books. Their approach is to
build as large and diverse a dataset as possible in order to collect
natural language demonstrations of tasks in as varied of domains and
contexts as possible.</p>

<p>A promising source of that kind of data is web scrapes such as <a href="https://commoncrawl.org/">Common
Crawl</a>, but they have significant data quality
issues which is that a large amount of documents content are mostly
unintelligible. That’s why in this paper, they created a new dataset
called WebText. This data contains text subset of around 45 million
links from Reddit where each link received at least 3 karma which
indicates whether other users found the link interesting or not. Also,
all Wikipedia links were removed since it’s a common data source.</p>

<h2 id="model">Model</h2>

<p>The model in this paper is the same as the one in GPT with a few
modifications:</p>

<ul>
  <li>
    <p>Layer normalization was moved to the input of each sub-block. And an
additional layer normalization was added after the final
self-attention block.</p>
  </li>
  <li>
    <p>A modified initialization, which accounts for the accumulation on
the residual path with model depth, is used. They scaled the
weights of residual layers at initialization by a factor of
$\frac{1}{\sqrt{N}}$ where $N$ is the number of residual layers.</p>
  </li>
  <li>
    <p>The vocabulary is expanded to 50,257 instead of 40,000.</p>
  </li>
  <li>
    <p>The context size is increased to 1024 instead of 512.</p>
  </li>
  <li>
    <p>The batch size is increased to 512 instead of 64.</p>

    <p>Also, they trained different versions of GPT-2 models: The smallest
model is equivalent to the original GPT, and the second smallest
equivalent to the largest model from BERT.The learning rate of each
model was manually tuned for the best perplexity on a 5% held-out
sample of WebText.</p>
  </li>
</ul>

<div align="center">
    <img src="media/GPT-2/image1.png" width="750" />
</div>

<h2 id="fine-tuning">Fine-Tuning</h2>

<p>As we said earlier, the purpose of this paper is to demonstrate that
language models can perform down-stream tasks such as (question
answering, machine translation, reading comprehension, and
summarization) in a zero-shot setting – without any parameter or
architecture modification.</p>

<p>Now, let’s see how they did that with different tasks:</p>

<ul>
  <li>Machine Translation:</li>
</ul>

<div align="center">
    <img src="media/GPT-2/image2.png" width="750" />
</div>

<ul>
  <li>Summarization:</li>
</ul>

<div align="center">
    <img src="media/GPT-2/image2.png" width="750" />
</div>

<h2 id="result-search">Result Search</h2>

<p>In order to produce good results when using our model, there are
multiple ways that we can search for the best result and they are:</p>

<ul>
  <li>
    <p>Exhaustive Search: Considering the whole vocabulary</p>
  </li>
  <li>
    <p>Greedy Search: Considering the top option at each time-step.</p>
  </li>
  <li>
    <p>Beam Serach: Considering the top N options at each time-step.</p>
  </li>
</ul>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/language-modeling/GPT-2';
      this.page.identifier = '/language-modeling/GPT-2';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>