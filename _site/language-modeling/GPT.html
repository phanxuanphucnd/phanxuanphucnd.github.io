<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>GPT</title>
  <meta name="title" content="GPT">
  <meta name="description" content="Transform is a state-of-the-art architecture for machine translation.
OpenAI tried to use this architecture for the language modeling task in
this paper “Improving Language Understanding by Generative
Pre-Training”
under the name “Improving Language Understanding by Generative
Pre-Training” which was published in 2018. Pre-training is the process
of training a model with one task (language modeling in the paper) that
is able to help it form parameters that can be used to make other tasks
easier (four other tasks: natural language inference, question
answering, semantic similarity, and text classification).

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="GPT">
  <meta itemprop="description" content="Transform is a state-of-the-art architecture for machine translation.
OpenAI tried to use this architecture for the language modeling task in
this paper “Improving Language Understanding by Generative
Pre-Training”
under the name “Improving Language Understanding by Generative
Pre-Training” which was published in 2018. Pre-training is the process
of training a model with one task (language modeling in the paper) that
is able to help it form parameters that can be used to make other tasks
easier (four other tasks: natural language inference, question
answering, semantic similarity, and text classification).

">
  <meta itemprop="image" content="/language-modeling/media/GPT/image0.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="GPT">
  <meta property="og:description" content="Transform is a state-of-the-art architecture for machine translation.
OpenAI tried to use this architecture for the language modeling task in
this paper “Improving Language Understanding by Generative
Pre-Training”
under the name “Improving Language Understanding by Generative
Pre-Training” which was published in 2018. Pre-training is the process
of training a model with one task (language modeling in the paper) that
is able to help it form parameters that can be used to make other tasks
easier (four other tasks: natural language inference, question
answering, semantic similarity, and text classification).

">
  <meta property="og:image" content="/language-modeling/media/GPT/image0.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="GPT">
  <meta name="twitter:description" content="Transform is a state-of-the-art architecture for machine translation.
OpenAI tried to use this architecture for the language modeling task in
this paper “Improving Language Understanding by Generative
Pre-Training”
under the name “Improving Language Understanding by Generative
Pre-Training” which was published in 2018. Pre-training is the process
of training a model with one task (language modeling in the paper) that
is able to help it form parameters that can be used to make other tasks
easier (four other tasks: natural language inference, question
answering, semantic similarity, and text classification).

">
  
  <meta name="twitter:image" content="/language-modeling/media/GPT/image0.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/language-modeling/GPT">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          3 mins read
        </span>
      </p>
      <time datetime="2018-06-11 00:00" class="post-meta__body date">Published on arXiv on: 11 Jun 2018</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#Open AI">Open AI</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=GPT> GPT</h1>
    <p>Transform is a state-of-the-art architecture for machine translation.
OpenAI tried to use this architecture for the language modeling task in
this paper “<a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative
Pre-Training</a>”
under the name “Improving Language Understanding by Generative
Pre-Training” which was published in 2018. Pre-training is the process
of training a model with one task (language modeling in the paper) that
is able to help it form parameters that can be used to make other tasks
easier (four other tasks: natural language inference, question
answering, semantic similarity, and text classification).</p>

<p>The Encoder-Decoder structure of the transformer made it perfect for
machine translation. But how would you use it to pre-train a language
model that can be fine-tuned for other tasks like sentiment analysis or
text classification? The way openAI team did it was pretty smart. It
turns out that we don’t need the entire transformer architecture to
adopt a language model. We can do it with just the decoder of the
transformer.</p>

<div align="center">
    <img src="media/GPT/image1.png" width="350" />
</div>

<p>The decoder is a good choice because it’s
a natural choice for language modeling (predicting the next word) since
it’s built to mask future tokens. Since there is no encoder in this set
up, the decoder layer would not have the encoder-decoder attention
sub-layer that vanilla transformer decoder layers have. So, the decoder
architecture becomes as shown in the image on the right.</p>

<p>In the original paper, they stacked twelve decoder layers with a
feed-forward neural network at the end with Softmax loss function. With
this structure, we can proceed to train the model on the same language
modeling task: predict the next word using massive (unlabeled) datasets.</p>

<div align="center">
    <img src="media/GPT/image2.png" width="750" />
</div>

<h2 id="model-specification">Model Specification</h2>

<p>The language model was trained on the BooksCorpus dataset for training
the language model. This dataset contains over 7,000 unique unpublished
books from a variety of genres. Crucially, it contains long stretches of
contiguous text, which allows the generative model to learn to condition
on long-range information.</p>

<p>The model, itself, has the following characteristics:</p>

<ul>
  <li>
    <p>12-layer decoder-only transformer.</p>
  </li>
  <li>
    <p>Masked self-attention with multi-heads (768 dimensional states and
12 attention heads).</p>
  </li>
  <li>
    <p>For the position-wise feed-forward networks, they used 3072 neurons.</p>
  </li>
  <li>
    <p>They used the Adam optimization scheme.</p>
  </li>
  <li>
    <p>The learning rate was increased linearly from zero over the first
2000 updates and annealed to 0 using a cosine schedule with a max
learning rate of 2.5e-4.</p>
  </li>
  <li>
    <p>We train for 100 epochs on minibatches of 64 randomly sampled,
contiguous sequences of 512 tokens.</p>
  </li>
  <li>
    <p>Since layer normalization is used extensively throughout the model,
a simple weight initialization of $\mathcal{N}(0,\ 0.02)$ was
sufficient.</p>
  </li>
  <li>
    <p>They used a bytepair encoding (BPE) vocabulary with 40,000 merges.</p>
  </li>
  <li>
    <p>Residual, embedding, and attention dropouts with a rate of 0.1 for
regularization.</p>
  </li>
  <li>
    <p>They also employed a modified version of L2 regularization proposed
in this paper, with w = 0.01 on all non bias or gain weights.</p>
  </li>
  <li>
    <p>For the activation function, we used the Gaussian Error Linear Unit (GELU).</p>
  </li>
  <li>
    <p>They used learned position embeddings instead of the sinusoidal
version proposed in the original work.</p>
  </li>
</ul>

<h2 id="fine-tuning">Fine-Tuning</h2>

<p>The OpenAI paper outlines a number of input transformations to handle
the inputs for different types of tasks. Since our language model was
trained on contiguous sequences of text, we require some modifications
to apply it to the different NLP tasks.</p>

<p>Previous <a href="https://arxiv.org/pdf/1802.05365.pdf">work</a> proposed learning
task-specific architectures on top of transferred representations. We
use a traversal-style approach where we convert structured inputs into
an ordered sequence that our pre-trained model can process which allows
us to avoid making extensive changes to the architecture across
different tasks.</p>

<p>The following image shows the structures of the model and the input
transformations to carry out different tasks. In these transformations,
we are using the following special tokens;
$\left\langle s \right\rangle$ for start token,
$\left\langle e \right\rangle$ for extract token, and
<span>$\left\langle \$ \right\rangle$</span> for delimiter token:</p>

<div align="center">
    <img src="media/GPT/image3.png" width="750" />
</div>

<p>So, for example the classification model will look like this:</p>

<div align="center">
    <img src="media/GPT/image4.png" width="450" />
</div>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/language-modeling/GPT';
      this.page.identifier = '/language-modeling/GPT';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>