<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>StructBERT</title>
  <meta name="title" content="StructBERT">
  <meta name="description" content="StructBERT stands for “Structural BERT” which is an extension of
BERT created by
incorporating language structures into pre-training. StructBERT was
proposed in 2019 by Alibaba Group and published in their “StructBERT:
Incorporating Language Structures Into Pre-Training For Deep Language
Understanding” paper. The
official code for this paper can be found in the following GitHub
repository:
alibaba/StructBERT.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="StructBERT">
  <meta itemprop="description" content="StructBERT stands for “Structural BERT” which is an extension of
BERT created by
incorporating language structures into pre-training. StructBERT was
proposed in 2019 by Alibaba Group and published in their “StructBERT:
Incorporating Language Structures Into Pre-Training For Deep Language
Understanding” paper. The
official code for this paper can be found in the following GitHub
repository:
alibaba/StructBERT.

">
  <meta itemprop="image" content="/language-modeling/media/StructBERT/image4.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="StructBERT">
  <meta property="og:description" content="StructBERT stands for “Structural BERT” which is an extension of
BERT created by
incorporating language structures into pre-training. StructBERT was
proposed in 2019 by Alibaba Group and published in their “StructBERT:
Incorporating Language Structures Into Pre-Training For Deep Language
Understanding” paper. The
official code for this paper can be found in the following GitHub
repository:
alibaba/StructBERT.

">
  <meta property="og:image" content="/language-modeling/media/StructBERT/image4.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="StructBERT">
  <meta name="twitter:description" content="StructBERT stands for “Structural BERT” which is an extension of
BERT created by
incorporating language structures into pre-training. StructBERT was
proposed in 2019 by Alibaba Group and published in their “StructBERT:
Incorporating Language Structures Into Pre-Training For Deep Language
Understanding” paper. The
official code for this paper can be found in the following GitHub
repository:
alibaba/StructBERT.

">
  
  <meta name="twitter:image" content="/language-modeling/media/StructBERT/image4.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/language-modeling/StructBERT">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          5 mins read
        </span>
      </p>
      <time datetime="2019-08-13 00:00" class="post-meta__body date">Published on arXiv on: 13 Aug 2019</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#Alibaba Group Inc.">Alibaba Group Inc.</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=StructBERT> StructBERT</h1>
    <p>StructBERT stands for “Structural BERT” which is an extension of
<a href="https://phanxuanphucnd.github.io/language-modeling/BERT">BERT</a> created by
incorporating language structures into pre-training. StructBERT was
proposed in 2019 by Alibaba Group and published in their “<a href="https://arxiv.org/pdf/1908.04577.pdf">StructBERT:
Incorporating Language Structures Into Pre-Training For Deep Language
Understanding</a>” paper. The
official code for this paper can be found in the following GitHub
repository:
<a href="https://github.com/alibaba/AliceMind/tree/main/StructBERT">alibaba/StructBERT</a>.</p>

<p>StructBERT incorporates the language structures into BERT by
pre-training BERT with two tasks/objectives that make the most use of
the sequential order of words and sentences along with the Masked
Language Modeling (MLM) objective. These two objectives are:</p>

<ul>
  <li>
    <p>Word Structural Objective.</p>
  </li>
  <li>
    <p>Sentence Structural Objective.</p>
  </li>
</ul>

<h2 id="mlm-recap">MLM Recap</h2>

<p>Before getting into more details about StructBERT, let’s first recap how
MLM objective worked in
<a href="https://phanxuanphucnd.github.io/language-modeling/BERT">BERT</a>. Given an
input sequence, 15% of the tokens in that sequence are replaced with a
[MASK] token; and the type of mask will be different according to the
following distribution</p>

<ul>
  <li>
    <p>80% of the time: the mask will be [MASK].</p>
  </li>
  <li>
    <p>10% of the time: the mask will be a random word.</p>
  </li>
  <li>
    <p>10% of the time: The mask will be the original word.</p>
  </li>
</ul>

<p>Then, BERT will have to learn to predict these masked tokens correctly while
pre-training.</p>

<div align="center">
    <img src="media/StructBERT/image1.png" width="750" />
</div>

<h2 id="word-structural-objective">Word Structural Objective</h2>

<p>This new word objective is jointly trained together with the original
MLM objective; the new objective takes the word order into
consideration. The way this objective works is that they randomly choose
$5\%$ of the unmasked trigrams to be shuffled and then StructBERT is
pre-trained to predict the original order of the tokens. As shown in the
following figure, the trigram (t2, t3, and t4) was shuffled to (t3, t4,
and t2).</p>

<div align="center">
    <img src="media/StructBERT/image2.png" width="750" />
</div>

<p>Given a randomly shuffled span of $K$ tokens (trigram means $K = 3$),
the word structural objective is equivalent to maximizing the likelihood
of placing every shuffled token in its correct position. So, given a set
of trainable parameters $\theta$, this objective can be defined as:</p>

\[\underset{\theta}{\text{arg}\max}{\sum_{}^{}{\log\left( P\left( \text{pos}_{1} = t_{1},\ \text{... pos}_{K} = t_{K} \middle| t_{1},\ ...t_{K}; \theta \right) \right)}}\]

<p>They studied the effect of this objective in comparison with MLM in BERT
during self-supervised pre-training. The following figure illustrates
the loss (left) and accuracy (right) of word and sentence prediction
over the number of pre-training steps for StructBERT-Base and BERT-Base:</p>

<div align="center">
    <img src="media/StructBERT/image3.png" width="750" />
</div>

<p>We can see that the the shuffled token prediction objective (blue line)
led to lower loss and higher accuracy more than MLM (red line).</p>

<h2 id="sentence-structural-objective">Sentence Structural Objective</h2>

<p>The Next Sentence Prediction (NSP) task is
considered easy for the original BERT model (the prediction accuracy of
BERT can easily achieve 97%-98% in this task). Therefore, they extended
the NSP task by considering both the next sentence and the previous
sentence to make the pre-trained language model aware of the sequential
order of the sentences in a bidirectional manner.</p>

<p>As illustrated in the following figure, they sampled the data for this
task in pairs $\left( S_{1},\ S_{2} \right)$ where the two sentences are
concatenated together with the separator token [SEP] in between, as
done in BERT. Given a first sentence $S_{1}$, they sampled $S_{2}$ to
be:</p>

<ul>
  <li>
    <p>The next sentence $\frac{1}{3}$ of the time.</p>
  </li>
  <li>
    <p>The previous sentence $\frac{1}{3}$ of the time.</p>
  </li>
  <li>
    <p>A random sentence from the data $\frac{1}{3}$ of the time.</p>
  </li>
</ul>

<div align="center">
    <img src="media/StructBERT/image4.png" width="750" />
</div>

<p>And given the input sequence, the model has to predict whether $S_{2}$
is the next sentence that follows $S_{1}$ (class label=1), or the
previous sentence that precedes $S_{1}$ (class label=2), or a random
sentence from a different document (class label=0).</p>

<p>They studied the effect of this objective during self-supervised
pre-training. The following figure illustrates the loss (left) and
accuracy (right) of word and sentence prediction over the number of
pre-training steps for StructBERT-Base and BERT-Base:</p>

<div align="center">
    <img src="media/StructBERT/image5.png" width="750" />
</div>

<p>As we can see, the new sentence structural objective in StructBERT leads
to a more challenging prediction task than that in BERT enabling
StructBERT to exploit inter-sentence structures, which benefits
sentence-pair downstream tasks.</p>

<h2 id="experiments">Experiments</h2>

<p>In this paper, they pre-trained StructBERT on documents from English
Wikipedia (2,500M words) and BookCorpus using WordPiece models. The
maximum length of input sequence was set to 512. They used Adam
optimizer ($\beta_{1} = 0.9,\ \beta_{2} = 0.999$). They used L2 weight
decay regularization of $0.01$. The learning rate was set to $1e^{- 4}$
with warm-up over the first 10% of the total steps, and linear decay at
the rest. The dropout probability was set to $0.1$ for every layer. They
used GeLU activation. They pre-trained two model sizes:</p>

<div align="center" class="inline-table">
<table>
    <thead>
        <tr>
            <th></th>
            <th>$$N$$</th>
            <th>$$d_{\text{ff}}$$</th>
            <th>$$h$$</th>
            <th># parameters</th>
            <th>Hardware</th>
        </tr>
    </thead>
    <tr>
        <td><strong>Base</strong></td>
        <td>12</td>
        <td>768</td>
        <td>12</td>
        <td>110 M</td>
        <td>64 GPU V100 + 38 hours</td>
    </tr>
    <tr>
        <td><strong>Large</strong></td>
        <td>24</td>
        <td>1024</td>
        <td>16</td>
        <td>340 M</td>
        <td>64 GPU V100 + 7 days</td>
    </tr>
</table>
</div>

<p>Now, we are going to see the performance of StructBERT over multiple
downstream tasks:</p>

<ul>
  <li><u><strong>Natural Language Understanding:</strong></u><br />
The following table shows the results of StructBERT on the
GLUE test set, which are scored by the GLUE evaluation server. The
number below each task denotes the number of training examples. The
state-of-the-art results are in bold. All the results are obtained
from the <a href="https://gluebenchmark.com/leaderboard">leaderboard</a>
(StructBERT submitted under a different model name ALICE):</li>
</ul>

<div align="center">
    <img src="media/StructBERT/image6.png" width="750" />
</div>

<ul>
  <li><u><strong>Natural Language Inference:</strong></u><br />
The following table shows the accuracy of multiple models on SNLI
dataset. As seen from the table, StructBERT outperformed all
existing systems on SNLI, creating new state-of-the-art results
91.7%, which amounts to 0.4% absolute improvement over the previous
state-of-the-art model SJRC and 0.9% absolute improvement over BERT.</li>
</ul>

<div align="center">
    <img src="media/StructBERT/image7.png" width="750" />
</div>

<ul>
  <li><u><strong>Question Answering:</strong></u><br />
The following table shows the results of SQuAD dataset where
we can see that StructBERT model is superior to all other models
except
<a href="https://phanxuanphucnd.github.io/language-modeling/XLNet">XLNet</a>+DA. It
demonstrates the effectiveness of StructBERT in modeling the
question-paragraph relationship.</li>
</ul>

<div align="center">
    <img src="media/StructBERT/image8.png" width="750" />
</div>

<p>To study the effect of the new pre-training tasks over fine-tuning, they
performed ablation study using StructBERT-Base architecture on six
different downstream tasks as shown in the following table:</p>

<div align="center">
    <img src="media/StructBERT/image9.png" width="750" />
</div>

<p>Based on this study, they found out the following:</p>

<ul>
  <li>
    <p>The two structural objectives were both critical to most of the
downstream tasks, except for the word structural objective in the
SNLI task.</p>
  </li>
  <li>
    <p>The StructBERT model with structural pre-training consistently
outperformed the original BERT model, which shows the effectiveness
of the proposed structural objectives.</p>
  </li>
  <li>
    <p>For the sentence-pair tasks such as MNLI, SNLI, QQP and SQuAD,
incorporating the sentence structural objective significantly
improved the performance.</p>
  </li>
  <li>
    <p>For the single-sentence tasks such as CoLA and SST-2, the word
structural objective played the most important role. Especially in
the CoLA task, which is related to the grammatical error correction,
the improvement was over 5%. The ability of reconstructing the order
of words in pre-training helped the model better judge the
acceptability of a single sentence.</p>
  </li>
</ul>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/language-modeling/StructBERT';
      this.page.identifier = '/language-modeling/StructBERT';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>