<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>ALBERT</title>
  <meta name="title" content="ALBERT">
  <meta name="description" content="ALBERT, stands for “A Lite BERT”, reduced version of BERT which is a
smaller, faster, cheaper and easier to scale. ALBERT was created by
Google &amp; Toyota Technical Institute in February 2019 and published in
this paper: “ALBERT: A Lite Bert For Self-Supervised Learning Of
Language Representations” and you can
fine the official code for this paper in Google Research’s official GitHub
repository: google-research/ALBERT.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="ALBERT">
  <meta itemprop="description" content="ALBERT, stands for “A Lite BERT”, reduced version of BERT which is a
smaller, faster, cheaper and easier to scale. ALBERT was created by
Google &amp; Toyota Technical Institute in February 2019 and published in
this paper: “ALBERT: A Lite Bert For Self-Supervised Learning Of
Language Representations” and you can
fine the official code for this paper in Google Research’s official GitHub
repository: google-research/ALBERT.

">
  <meta itemprop="image" content="/language-modeling/media/ALBERT/image0.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="ALBERT">
  <meta property="og:description" content="ALBERT, stands for “A Lite BERT”, reduced version of BERT which is a
smaller, faster, cheaper and easier to scale. ALBERT was created by
Google &amp; Toyota Technical Institute in February 2019 and published in
this paper: “ALBERT: A Lite Bert For Self-Supervised Learning Of
Language Representations” and you can
fine the official code for this paper in Google Research’s official GitHub
repository: google-research/ALBERT.

">
  <meta property="og:image" content="/language-modeling/media/ALBERT/image0.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="ALBERT">
  <meta name="twitter:description" content="ALBERT, stands for “A Lite BERT”, reduced version of BERT which is a
smaller, faster, cheaper and easier to scale. ALBERT was created by
Google &amp; Toyota Technical Institute in February 2019 and published in
this paper: “ALBERT: A Lite Bert For Self-Supervised Learning Of
Language Representations” and you can
fine the official code for this paper in Google Research’s official GitHub
repository: google-research/ALBERT.

">
  
  <meta name="twitter:image" content="/language-modeling/media/ALBERT/image0.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/language-modeling/ALBERT">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          3 mins read
        </span>
      </p>
      <time datetime="2019-09-26 00:00" class="post-meta__body date">Published on arXiv on: 26 Sep 2019</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#Google Research">Google Research</a> & <a href="/labs/#Toyota Technological Institute at Chicago">Toyota Technological Institute at Chicago</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=ALBERT> ALBERT</h1>
    <p>ALBERT, stands for “A Lite BERT”, reduced version of BERT which is a
smaller, faster, cheaper and easier to scale. ALBERT was created by
Google &amp; Toyota Technical Institute in February 2019 and published in
this paper: “<a href="https://arxiv.org/pdf/1909.11942.pdf">ALBERT: A Lite Bert For Self-Supervised Learning Of
Language Representations</a>” and you can
fine the official code for this paper in Google Research’s official GitHub
repository: <a href="https://github.com/google-research/ALBERT">google-research/ALBERT</a>.</p>

<div align="center">
    <img src="media/ALBERT/image1.png" width="450" />
</div>

<p>ALBERT incorporates two parameter reduction techniques that act as a form of
regularization that stabilizes the training and helps with generalization; and
one new loss function:</p>

<ul>
  <li>
    <p>Factorized Embedding Parameterization</p>
  </li>
  <li>
    <p>Cross-layer Parameter Sharing</p>
  </li>
  <li>
    <p>Sentence-Order Prediction Loss</p>
  </li>
</ul>

<p>Also, The parameter reduction techniques also act as a form of
regularization that stabilizes the training and helps with
generalization.</p>

<p>These three modifications significantly reduce the number of
parameters for BERT without seriously hurting performance. An ALBERT
configuration similar to BERT-large has 18x fewer parameters and can
be trained about 1.7x faster as shown in the following table:</p>

<div align="center">
    <img src="media/ALBERT/image2.png" width="750" />
</div>

<p>And since longer training usually leads to better performance, the
following table shows a comparison between the performance BERT and
ALBERT after the same training time which shows that ALBERT still
performs better:</p>

<div align="center">
    <img src="media/ALBERT/image3.png" width="750" />
</div>

<h2 id="factorized-embedding-parameterization">Factorized Embedding Parameterization</h2>

<p>In BERT, the Word embedding size $E$ is tied with the hidden layer size
$H$, i.e., $E\  \equiv \ H$. This decision appears sub-optimal for the
following reasons:</p>

<ul>
  <li>
    <p>Word embeddings are meant to learn context-independent
representations, whereas hidden-layer embeddings are meant to
learn context-dependent representations. And that dictates that
$H \gg E$.</p>
  </li>
  <li>
    <p>NLP usually requires the vocabulary size $V$ to be large. If
$E \equiv H$, then increasing $H$ increases the size of the
embedding matrix, which has size $V \times E$. This can easily
result in a model with billions of parameters, most of which are
only updated sparsely during training.</p>
  </li>
</ul>

<p>So, instead of projecting the one-hot vectors directly into the hidden
space of size H, they first projected them into a lower dimensional
embedding space of size E, and then projected it to the hidden space. By
using this decomposition, we reduce the embedding parameters from
$O(V \times H)$ to
$O\left( \left( V \times E \right) + \left( E \times H \right) \right)$.
This parameter reduction is significant when $H \gg E$.</p>

<h2 id="cross-layer-parameter-sharing">Cross-layer Parameter Sharing</h2>

<p>The default decision for ALBERT is to share all parameters across
layers. The following figure shows the L2 distances and cosine
similarity of the input and output embeddings for each layer. We observe
that the transitions from layer to layer are much smoother for ALBERT
than for BERT.</p>

<p>These results show that weight-sharing has an effect on stabilizing
network parameters. Although there is a drop for both metrics compared
to BERT, they nevertheless do not converge to 0 even after 24 layers.</p>

<div align="center">
    <img src="media/ALBERT/image4.png" width="750" />
</div>

<h2 id="sentence-order-prediction-sop">Sentence-Order Prediction (SOP)</h2>

<p>In this paper, they proposed a new loss function called sentence-order
prediction (SOP) loss as a replacement for next sentence prediction
(NSP) used with BERT. The same as NSP, SOP loss uses two consecutive
segments from the same document as a positive example. And as negative
examples, it uses the same two consecutive segments but with their order
swapped.</p>

<p>This forces the model to learn finer-grained distinctions about
discourse-level coherence properties. As shown in the following table,
it turns out that NSP cannot solve the SOP task at all, while SOP can
solve the NSP task to a reasonable degree.</p>

<div align="center">
    <img src="media/ALBERT/image5.png" width="350" />
</div>

<h2 id="hyper-parameters">Hyper-parameters</h2>

<p>In this part, we are going to talk about the effect of some
hyper-parameters on ALBERT:</p>

<ul>
  <li><strong>Network Depth (number of layers):</strong><br />
If we compare a 3-layer ALBERT model with a 1-layer ALBERT model,
although they have the same number of parameters, the performance
increases significantly. However, there are diminishing returns
when continuing to increase the number of layers: the results of a
12-layer network are relatively close to the results of a 24-layer
network, and the performance of a 48-layer network appears to
decline.</li>
</ul>

<div align="center">
    <img src="media/ALBERT/image6.png" width="750" />
</div>

<ul>
  <li><strong>Network Width (hidden size):</strong><br />
Using a 3-layers ALBERT, we can see that as we increase the hidden
size, we get an increase in performance with diminishing returns.
At a hidden size of 6144, the performance appears to decline
significantly:</li>
</ul>

<div align="center">
    <img src="media/ALBERT/image7.png" width="750" />
</div>

<ul>
  <li><strong>Dropout:</strong><br />
Using ALBERT-xxlarge at around 1M training steps states that
removing dropout helps the downstream tasks which indicates that
ALBERT models didn’t overfit the data.</li>
</ul>

<div align="center">
    <img src="media/ALBERT/image8.png" width="750" />
</div>

<ul>
  <li><strong>Data:</strong><br />
Adding more data improvements on the downstream tasks, except
for the SQuAD benchmarks (which are Wikipedia-based, and therefore
are negatively affected by out-of-domain training material).</li>
</ul>

<div align="center">
    <img src="media/ALBERT/image9.png" width="750" />
</div>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/language-modeling/ALBERT';
      this.page.identifier = '/language-modeling/ALBERT';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>