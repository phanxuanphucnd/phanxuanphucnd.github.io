<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  
  

  
  
  
  
  

  <title>Longformer: Long Transformer</title>
  <meta name="title" content="Longformer: Long Transformer">
  <meta name="description" content="Transformer-based models are unable to process long sequences due to
their self-attention operation, which has a time complexity of
$O\left( n^{2} \right)$ where $n$ is the input length. Longformer stands
for “Long Transformer” which is a encoder-side transformer with a novel
attention mechanism that scales linearly with sequence length making it
easy to process documents of thousands of tokens or longer. Longformer
was proposed by Allen Institute in 2020 and published in their paper:
Longformer: The Long-Document
Transformer. The official code
for this paper can be found in the official GitHub page of Allen
Institute: allenai/longformer.

">
  <meta name="author" content="Phanxuan Phuc">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Google / Search Engine Tags -->
  <meta itemprop="name" content="Longformer: Long Transformer">
  <meta itemprop="description" content="Transformer-based models are unable to process long sequences due to
their self-attention operation, which has a time complexity of
$O\left( n^{2} \right)$ where $n$ is the input length. Longformer stands
for “Long Transformer” which is a encoder-side transformer with a novel
attention mechanism that scales linearly with sequence length making it
easy to process documents of thousands of tokens or longer. Longformer
was proposed by Allen Institute in 2020 and published in their paper:
Longformer: The Long-Document
Transformer. The official code
for this paper can be found in the official GitHub page of Allen
Institute: allenai/longformer.

">
  <meta itemprop="image" content="/language-modeling/media/Longformer/image1.png">

  <!-- OpenGraph Meta Tags -->
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Phuc Phan's Blog"/>
  <meta property="og:title" content="Longformer: Long Transformer">
  <meta property="og:description" content="Transformer-based models are unable to process long sequences due to
their self-attention operation, which has a time complexity of
$O\left( n^{2} \right)$ where $n$ is the input length. Longformer stands
for “Long Transformer” which is a encoder-side transformer with a novel
attention mechanism that scales linearly with sequence length making it
easy to process documents of thousands of tokens or longer. Longformer
was proposed by Allen Institute in 2020 and published in their paper:
Longformer: The Long-Document
Transformer. The official code
for this paper can be found in the official GitHub page of Allen
Institute: allenai/longformer.

">
  <meta property="og:image" content="/language-modeling/media/Longformer/image1.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Longformer: Long Transformer">
  <meta name="twitter:description" content="Transformer-based models are unable to process long sequences due to
their self-attention operation, which has a time complexity of
$O\left( n^{2} \right)$ where $n$ is the input length. Longformer stands
for “Long Transformer” which is a encoder-side transformer with a novel
attention mechanism that scales linearly with sequence length making it
easy to process documents of thousands of tokens or longer. Longformer
was proposed by Allen Institute in 2020 and published in their paper:
Longformer: The Long-Document
Transformer. The official code
for this paper can be found in the official GitHub page of Allen
Institute: allenai/longformer.

">
  
  <meta name="twitter:image" content="/language-modeling/media/Longformer/image1.png">

  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/images/favicons/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/images/favicons/android-chrome-512x512.png" sizes="512x512">
  <link rel="manifest" href="/images/favicons/manifest.json">
  <link rel="shortcut icon" href="/images/favicons/favicon.ico">
  <meta name="msapplication-TileColor" content="#ffc40d">
  <meta name="msapplication-TileImage" content="/images/favicons/mstile-144x144.png">
  <meta name="msapplication-square70x70logo" content="/images/favicons/mstile-70x70.png" />
  <meta name="msapplication-square150x150logo" content="/images/favicons/mstile-150x150.png" />
  <meta name="msapplication-wide310x150logo" content="/images/favicons/mstile-310x150.png" />
  <meta name="msapplication-square310x310logo" content="/images/favicons/mstile-310x310.png" />
  <meta name="theme-color" content="#ffffff">
  
  <link rel="stylesheet" href="/css/main.css?1680763441293376118">
  <link rel="canonical" href="http://localhost:4000/language-modeling/Longformer">
  <link rel="alternate" type="application/rss+xml" title="Phuc Phan's Blog" href="/feed.xml">
  <script type="text/javascript" src="/js/jquery.v3.3.1.min.js"></script>
  <!-- <script type="text/javascript" src="/js/simple-jekyll-search.min.js"></script> -->
  <script type="text/javascript" src="/js/simple-blog-search.min.js"></script>
</head>

  <body>
    <span class="mobile btn-mobile-menu">
  <i class="extra-big-icon iconify-inline icon-list btn-mobile-menu__icon" data-icon="gg:menu-round"></i>
  <i class="extra-big-icon iconify-inline icon-x-circle btn-mobile-close__icon hidden" data-icon="ion:close-circle-outline"></i>
</span>

<header class="panel-cover "
  style="background-image: url(/images/forest.jpg)">
  <div class="panel-main panel-cover--overlay">

    <div class="panel-main__inner panel-inverted">
      <div class="panel-main__content">
        <img src="/images/avatar.jpg" class="user-image"
        onmouseover="this.src='/images/avatar-light.jpg';"
        onmouseout="this.src='/images/avatar.jpg';">
        <h1 class="panel-cover__title panel-title">
          <a href="/">Phanxuan Phuc</a>
        </h1>
        </a>
        <h6 class="panel-cover__title panel-subtitle">fakerphan
          <i class="medium-icon iconify-inline" data-icon="foundation:male-symbol"></i>
        </h6>
        <nav class="cover-navigation navigation--social">
    <ul class="navigation">

      
      <!-- Email -->
      <li class="navigation__item">
        <a href="mailto:phanxuanphucnd@gmail.com" target="_blank" title="Email">
          <i class="big-icon iconify-inline" data-icon="fluent:mail-copy-24-filled"></i>
        </a>
      </li>
      

      
      <!-- LinkedIn -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/phanxuanphucnd" target="_blank" title="LinkedIn">
          <i class="big-icon iconify-inline" data-icon="akar-icons:linkedin-fill"></i>
        </a>
      </li>
      

      
      <!-- GitHub -->
      <li class="navigation__item">
        <a href="https://www.github.com/phanxuanphucnd" target="_blank" title="GitHub">
          <i class="big-icon iconify-inline" data-icon="akar-icons:github-fill"></i>
        </a>
      </li>
      

      <li class="navigation__item">
        <a href="https://scholar.google.com/citations?user=ipcDPCQAAAAJ&hl=vi" title="Google Scholar" target="_blank">
          <i class="big-icon iconify-inline" data-icon="mdi:google"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>

      <li class="navigation__item">
        <a href="https://www.facebook.com/phanxuanphucnd" title="Facebook" target="_blank">
          <i class="big-icon iconify-inline" data-icon="ic:outline-facebook"></i>
          <!-- <span class="label">RSS</span> -->
        </a>
      </li>
    </ul>
  </nav>
        
        <form class="search-container" onsubmit="return searchTheArchives()" >
          <input type="search" id="search-input" placeholder="Search.." autocomplete="off" style="padding-right: 40px;">
        </form>

        <hr class="panel-cover__divider panel-cover__divider--secondary">
        
        <div class="navigation-wrapper">
          <p class="panel-cover__description">I’m a AI Engineer in Vin Bigdata. And I summarize research papers in these topics:</p>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              
              
              <li class="navigation__item">
                <a href="/cross-lingual-lm" class="blog-button" id="cross-lingual-lm"
                onclick="activateButton(this.id)">Cross-lingual Langluage Model</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/language-modeling" class="blog-button" id="language-modeling"
                onclick="activateButton(this.id)">Language Modeling</a>
              </li>
              
              
              
              <li class="navigation__item">
                <a href="/machine-translation" class="blog-button" id="machine-translation"
                onclick="activateButton(this.id)">Machine Translation</a>
              </li>
              
              
              
              
              
              <li class="navigation__item">
                <a href="/multilingual-nmt" class="blog-button" id="multilingual-nmt"
                onclick="activateButton(this.id)">Multilingual NMT</a>
              </li>
              
              
              
              
              
              
            </ul>
          </nav>
        </div>
        
        <hr class="panel-cover__divider">
      </div>

    </div>

    <!-- <div class="panel-cover--overlay"></div> -->
  </div>
</header>

    <div class="content-wrapper">
      <div class="content-wrapper__inner">
        <article class="post-container post-container--single">
  <header class="post-header">
    <div class="post-meta">
      <p class="post-meta__body">
        <i class="medium-icon iconify-inline" data-icon="bi:clock-history"></i>
        <span id="reading-time">
          
          4 mins read
        </span>
      </p>
      <time datetime="2020-04-10 00:00" class="post-meta__body date">Published on arXiv on: 10 Apr 2020</time>
      
      <p class="post-meta__tags">Published by: <a href="/labs/#Allen Institute AI">Allen Institute AI</a></p>
      
    </div>
  </header>

  <section class="post">
    <h1 id=Longformer: Long Transformer> Longformer: Long Transformer</h1>
    <p>Transformer-based models are unable to process long sequences due to
their self-attention operation, which has a time complexity of
$O\left( n^{2} \right)$ where $n$ is the input length. Longformer stands
for “Long Transformer” which is a encoder-side transformer with a novel
attention mechanism that scales linearly with sequence length making it
easy to process documents of thousands of tokens or longer. Longformer
was proposed by Allen Institute in 2020 and published in their paper:
<a href="https://arxiv.org/pdf/2004.05150.pdf">Longformer: The Long-Document
Transformer</a>. The official code
for this paper can be found in the official GitHub page of Allen
Institute: <a href="https://github.com/allenai/longformer">allenai/longformer</a>.</p>

<h2 id="attention-patterns">Attention Patterns</h2>

<p>Longformer sparsifies the full-attention mechanism matrix according to
an “attention pattern” specifying pairs of input locations attending to
one another. According to the paper, there are four different patterns
considered as shown below:</p>

<ul>
  <li><u><strong>Sliding Window:</strong></u><br />
This is a fixed-window attention surrounding each token. Using
multiple stacked layers of such windowed attention results in a
large receptive field. Given a fixed window size $w$, each token
attends to $\frac{1}{2}w$ tokens on each side. The computation
complexity of this pattern is $O\left( n \times w \right)$. In a
transformer with $l$ layers, the receptive field size at the top
layer is $l \times w$ (assuming $w$ is fixed for all layers).</li>
</ul>

<div align="center">
    <img src="media/Longformer/image2.png" width="350" />
</div>

<ul>
  <li><u><strong>Dilated Sliding Window:</strong></u><br />
To further increase the receptive field without increasing
computation, the sliding window can be “dilated” where the window
has gaps of size dilation $d$. Assuming a fixed $d$ and $w$ for all
layers, the receptive field is $l \times d \times w$, which can
reach tens of thousands of tokens even for small values of $d$.</li>
</ul>

<div align="center">
    <img src="media/Longformer/image1.png" width="350" />
</div>

<ul>
  <li><u><strong>Global Attention:</strong></u><br />
The windowed and dilated attention attend to subset of the
sequence. That’s why they decided to add “global attention” on few
pre-selected input tokens. This figure shows an example of a sliding
window attention with global attention at a few tokens at custom
locations.</li>
</ul>

<div align="center">
    <img src="media/Longformer/image3.png" width="350" />
</div>

<blockquote>
  <p><strong>Note:</strong></p>

  <ul>
    <li>
      <p>In multi-headed setup of dialted window attention, using different
  dilation configurations per head improves performance by allowing
  some heads without dilation to focus on local context, while others
  with dilation focus on longer context.</p>
    </li>
    <li>
      <p>Implementing Longformer’s dilated sliding widow attention requires a
  form of banded matrix multiplication (matrix multiplication where
  the output is all zero except certain diagonals) that is not
  directly supported in existing deep learning libraries like
  PyTorch/Tensorflow. So, they tried to implement it in three
  different ways:</p>

      <ul>
        <li>
          <p><strong>Longformer-loop:</strong> Naive implementation using for-loops in PyTorch.</p>
        </li>
        <li>
          <p><strong>Longformer-chunks:</strong> Chunks $Q$ and $K$ into overlapping blocks of
  size $w$ and overlap of size $\frac{1}{2}w$, multiplies the blocks,
  then mask out the diagonals.</p>
        </li>
        <li>
          <p><strong>Longformer-cuda:</strong> is a custom CUDA kernel that they implemented
  using TVM (Tensor Virtual Machine).</p>
        </li>
      </ul>

      <p>They compared these different implementations according to time and memory
and found out that <strong>longformer-chunks</strong> is the fastest.</p>

      <div align="center">
    <img src="media/Longformer/image4.png" width="750" />
</div>
    </li>
    <li>
      <p>They used small window sizes for the lower layers and increase window
  sizes as they moved to higher layers.</p>
    </li>
    <li>
      <p>Also, they didn’t use dilated sliding windows for lower layers to
  maximize their capacity to learn and utilize the immediate local
  context. For the higher layers, they used a small amount of
  increasing dilation only on $2$ heads. This gives the model the
  ability to directly attend to distant tokens without sacrificing
  local context.</p>
    </li>
  </ul>
</blockquote>

<h2 id="character-level-lm">Character-level LM</h2>

<p>Training Longformer is done over 5 phases where they started with a
short sequence length of 2,048 and small window size, then the attention
window size and sequence length across multiple is doubled on each
subsequent phase while halving the learning rate till they ended with a
length of 23,040 on the last phase.</p>

<p>Longformer implementation is based on the
<a href="https://phanxuanphucnd.github.io/language-modeling/Transformer-XL">Transformer-XL</a>
found <a href="https://github.com/kimiyoung/transformer-xl">here</a> with the
memory mechanism disabled. They used relative position embeddings with
sinusoidal weights. They used two different model sizes; each with a
different set of hyper-parameters:</p>

<div align="center">
    <img src="media/Longformer/image5.png" width="750" />
</div>

<p>To compare longformer with previous character-level language modeling,
they trained it on text8 &amp; enwik8 benchmark, both contain 100M
characters from Wikipedia split into 90M, 5M, 5M for train, dev, test
respectively. Longformer outperforms all other models and achieves
state-of-the-art results on both datasets:</p>

<div align="center">
    <img src="media/Longformer/image6.png" width="750" />
</div>

<h2 id="pre-training--fine-tuning">Pre-training &amp; Fine-tuning</h2>

<p>Longformer was pre-trained using masked language modeling (MLM), where
the goal is to recover randomly masked tokens in a sequence. Since MLM
pre-training is expensive, they continued pre-training from the
<a href="https://phanxuanphucnd.github.io/language-modeling/RoBERTa">RoBERTa</a> released
checkpoint, and only making the minimal changes necessary to support
Longformer’s attention mechanism. And since RoBERTa’s input is limited
to $512$, they decided to copy them till it matches the input to
longformer. After that, longformer was pre-trained on the following
data:</p>

<div align="center">
    <img src="media/Longformer/image7.png" width="450" />
</div>

<p><strong>Very Important Note:</strong><br />
Longformer’s attention pattern can be plugged into any pre-trained
transformer model without the need to change the model architecture.</p>

<p>After pre-training, longformer was fine-tuned on on six tasks resulting
in a model that can process sequences up to 4,096 tokens long (8 times
longer than BERT). These six tasks are QA (WikiHop, TriviaQA, HotpotQA),
Coreference Resolution () and document classification (IMDB,
Hyperpartisan):</p>

<div align="center">
    <img src="media/Longformer/image8.png" width="750" />
</div>

<h2 id="led">LED</h2>

<p>LED stands for “Longformer Encoder Decoder” which is a variant of the
Longformer model that follows an encoder-decoder architecture similar to
the original
<a href="https://phanxuanphucnd.github.io/machine-translation/Transformer">Transformer</a>
model; instead of an encoder-only Transformer architecture as the
Longformer model. LED is intended for long sequence-to-sequence tasks
such as text summarization.</p>

<p>Since pre-training LED is expensive, they initialized LED parameters
from the <a href="https://phanxuanphucnd.github.io/language-modeling/BART">BART</a>
following BART’s exact architecture in terms of number of layers and
hidden sizes. The only difference is that they extend position embedding
to 16K tokens (BART has only 1K tokens). Also, they initialized the new
position embedding matrix by repeatedly copying BART’s 1K position
embeddings 16 times.</p>

<p>Following BART, they released two model sizes, LED-base and LED-large,
which respectively have 6 and 12 layers in both encoder and decoder
stacks. LED was evaluated on the summarization task using the arXiv
summarization dataset which focuses on long document summarization in
the scientific domain. The following table shows that LED-large achieves
state-of-the-art results, slightly outperforming
<a href="https://phanxuanphucnd.github.io/language-modeling/BigBird">BigBird</a>.</p>

<div align="center">
<img src="media/Longformer/image9.png" width="750" />
</div>

  </section>
</article>

  
  <br><br><br><br>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/language-modeling/Longformer';
      this.page.identifier = '/language-modeling/Longformer';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://Phuc Phan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  <!-- <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->




      </div>
      
<footer class="footer">
  <!-- <span class="footer__copyright">&copy; 2023 Phanxuan Phuc. All rights reserved.</span> -->
  
  <!-- Adding Iconify -->
  <script src="https://code.iconify.design/2/2.0.3/iconify.min.js"></script>
  
  <!-- Adding MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      "HTML-CSS": { linebreaks: { automatic: true } },
      "SVG": { linebreaks: { automatic: true } },
    });
  </script>
  
  <script type="text/javascript" async
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  
  
  <!-- Adding main.js -->
  <script type="text/javascript" src="/js/main.js?1680763441293376118"></script>
  
  <!-- Adding Google Analytics -->
  
</footer>
    </div>
  </body>
</html>