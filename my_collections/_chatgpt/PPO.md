---
title: "PPO: Giải thích thuật toán Proximal Policy Opitmization (PPO)"
date: 2023-04-10
cover: /ppo0.jpg
labs: ["VinBigdata", "Hanoi, Vietnam"]
---

**Proximal Policy Optimization (PPO)** hiện nay được xem như là thuật toán SoTA trong Reinforcement Learning (RL). Thuật toán 
được giới thiệu bởi OpenAI năm 2017, được xây dựng nằm cân bằng giữa performance và khả năng hiểu/ dễ dàng sử dụng. 
Chất lượng của thuật toán cạnh tranh với các thuật toán khác trong RL trên nhiều benchmark chất lượng, thâm chí còn vượt 
trội trên một số task. Đồng thời, nó đủ đơn giản để chúng ta dễ dàng áp dụng vào thực tế, điều này thường rất khó khăn 
cho mọi thuật toán trong RL.

Hiểu theo bề nổi thì sự khác nhau giữa các phương pháp policy gradient truyền thống với PPO không quá lớn. Dựa trên 
pseudo-code của 2 thuật toán có thể thấy tưởng chừng như hai thuật toán này giống nhau. Tuy nhiên, đằng sau tất cả là 
một lý thuyết phong phú cần thiết để đánh giá và hiểu đầy đủ về thuật toán. Chúng ta sẽ thảo luận ngắn gọn về các điểm 
chính của các phương pháp **Policy Gradient**, **Natural Policy Gradient** và **Trust Region Policy Optimization (TRPO)** để có thể làm 
bước đệm cho việc hiểu rõ hơn về PPO.

### Vanilla Policy Gradient




### Natural Policy Gradient


### Trust Region Policy Optimization (TRPO)


### Proximal Policy Optimization (PPO)

#### PPO - Variant with Adaptive KL Penalty

#### PPO - Variant with Clipped Objective

#### PPO2


### Kết luận


